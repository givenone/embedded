{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "Emb_SimCLR_Practice.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "be1b90ce7a8243d8b06031ed18fdf8c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_aee2c650836e4a7fb0e9f771813f6151",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d5a5f2a58aac418099c343ef74412cf4",
              "IPY_MODEL_92f91d4f52594e288217c2b4a2aad18b"
            ]
          }
        },
        "aee2c650836e4a7fb0e9f771813f6151": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d5a5f2a58aac418099c343ef74412cf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b4b872255da44d8f95778fe28e2e56e6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_19d75b43d27c489a8722112b18ee78c3"
          }
        },
        "92f91d4f52594e288217c2b4a2aad18b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_56263b9442954498959897e2363c2947",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [00:19&lt;00:00, 53922711.79it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0494970e7ba545e2bfa62e355f1a8d8e"
          }
        },
        "b4b872255da44d8f95778fe28e2e56e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "19d75b43d27c489a8722112b18ee78c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "56263b9442954498959897e2363c2947": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0494970e7ba545e2bfa62e355f1a8d8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/givenone/embedded/blob/master/Emb_SimCLR_Practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIbUUOphvNRR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "outputId": "ebae36f4-50e1-422c-a228-5ec7e8b0e0d1"
      },
      "source": [
        "  pip install git+https://github.com/ildoonet/pytorch-gradual-warmup-lr.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/ildoonet/pytorch-gradual-warmup-lr.git\n",
            "  Cloning https://github.com/ildoonet/pytorch-gradual-warmup-lr.git to /tmp/pip-req-build-vzn9mjbh\n",
            "  Running command git clone -q https://github.com/ildoonet/pytorch-gradual-warmup-lr.git /tmp/pip-req-build-vzn9mjbh\n",
            "Building wheels for collected packages: warmup-scheduler\n",
            "  Building wheel for warmup-scheduler (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for warmup-scheduler: filename=warmup_scheduler-0.3.2-cp36-none-any.whl size=3881 sha256=6079ffa16238eefa12079c83e946ae191a2b535ac9a6015307bb18772eace7a4\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-fy84b0_t/wheels/b7/24/83/d30234cc013cff538805b14df916e79091f7cf9ee2c5bf3a64\n",
            "Successfully built warmup-scheduler\n",
            "Installing collected packages: warmup-scheduler\n",
            "Successfully installed warmup-scheduler-0.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYtKgry8uqb3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "\n",
        "import time"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZ4WM8_Guqb6",
        "colab_type": "text"
      },
      "source": [
        "### Step 1. Construct a CNN model\n",
        "\n",
        "#### Implementation 1-1. Design SimCLRHead class\n",
        "\n",
        "#### Implementation 1-2. Design SimCLRNet class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvfdvwkQuqb6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "from torchvision.models.resnet import conv3x3\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, inplanes, planes, norm_layer, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "        \n",
        "        self.bn1 = norm_layer(inplanes)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        \n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x \n",
        "        residual = self.bn1(residual)\n",
        "        residual = self.relu1(residual)\n",
        "        residual = self.conv1(residual)\n",
        "\n",
        "        residual = self.bn2(residual)\n",
        "        residual = self.relu2(residual)\n",
        "        residual = self.conv2(residual)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            x = self.downsample(x)\n",
        "        return x + residual\n",
        "\n",
        "class Downsample(nn.Module):\n",
        "    def __init__(self, nIn, nOut, stride):\n",
        "        super(Downsample, self).__init__()\n",
        "        self.avg = nn.AvgPool2d(stride)\n",
        "        assert nOut % nIn == 0\n",
        "        self.expand_ratio = nOut // nIn\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.avg(x)\n",
        "        return torch.cat([x] + [x.mul(0)] * (self.expand_ratio - 1), 1)\n",
        "\n",
        "class ResNetCifar(nn.Module):\n",
        "    def __init__(self, depth, width=1, classes=10, channels=3, norm_layer=nn.BatchNorm2d):\n",
        "        assert (depth - 2) % 6 == 0         # depth is 6N+2\n",
        "        self.N = (depth - 2) // 6\n",
        "        super(ResNetCifar, self).__init__()\n",
        "\n",
        "        # Following the Wide ResNet convention, we fix the very first convolution\n",
        "        self.conv1 = nn.Conv2d(channels, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.inplanes = 16\n",
        "        self.layer1 = self._make_layer(norm_layer, 16 * width)\n",
        "        self.layer2 = self._make_layer(norm_layer, 32 * width, stride=2)\n",
        "        self.layer3 = self._make_layer(norm_layer, 64 * width, stride=2)\n",
        "        self.bn = norm_layer(64 * width)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.avgpool = nn.AvgPool2d(8)\n",
        "\n",
        "        # Initialization\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "                \n",
        "    def _make_layer(self, norm_layer, planes, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes:\n",
        "            downsample = Downsample(self.inplanes, planes, stride)\n",
        "        layers = [BasicBlock(self.inplanes, planes, norm_layer, stride, downsample)]\n",
        "        self.inplanes = planes\n",
        "        for i in range(self.N - 1):\n",
        "            layers.append(BasicBlock(self.inplanes, planes, norm_layer))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x\n",
        "\n",
        "    \n",
        "class Normalize(nn.Module):\n",
        "\n",
        "    def __init__(self, power=2):\n",
        "        super(Normalize, self).__init__()\n",
        "        self.power = power\n",
        "\n",
        "    def forward(self, x):\n",
        "        norm = x.pow(self.power).sum(1, keepdim=True).pow(1. / self.power)\n",
        "        out = x.div(norm)\n",
        "        return out\n",
        "    \n",
        "\n",
        "class SimCLRHead(nn.Module):\n",
        "    def __init__(self, width, emb_dim):\n",
        "        super(SimCLRHead, self).__init__()\n",
        "        \n",
        "        ### IMPLEMENTATION 1-1 ###\n",
        "        ### 1. Linear layer (64 * width -> 64 * width)\n",
        "        ### 2. ReLU\n",
        "        ### 3. Linear layer (64 * width -> emb_dim)\n",
        "        ### 4. Normalization layer (Normalize module above)\n",
        "        self.fc1 = nn.Linear(64 * width, 64 * width)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.fc2 = nn.Linear(64 * width, emb_dim)\n",
        "        self.norm = Normalize()\n",
        "        ### IMPLEMENTATION ENDS HERE ###\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        ### IMPLEMENTATION 1-1 ###\n",
        "        ### Design a proper forward function\n",
        "        \n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.norm(x)\n",
        "\n",
        "        ### IMPLEMENTATION ENDS HERE ###\n",
        "        return x\n",
        "    \n",
        "\n",
        "class SimCLRNet(nn.Module):\n",
        "    def __init__(self, depth, width=1, num_classes=10, emb_dim=32):\n",
        "        super(SimCLRNet, self).__init__()\n",
        "        \n",
        "        self.num_classes = num_classes\n",
        "        \n",
        "        self.feat = ResNetCifar(depth=depth, width=width, classes=num_classes)\n",
        "        \n",
        "        ### IMPLEMENTATION 1-2 ###\n",
        "        ### 1. A projection head (SimCLRHead module above)\n",
        "        self.head = SimCLRHead(width, emb_dim)\n",
        "        \n",
        "        ### 2. A linear classifier (64 * width -> num_classes)\n",
        "        self.classifier = nn.Linear(64 * width, num_classes)\n",
        "        \n",
        "        ### 3. Normalization layer for conv feature normalization (Normalize module above)\n",
        "        self.norm = Normalize()\n",
        "        \n",
        "        ### IMPLEMENTATION ENDS HERE ###\n",
        "    \n",
        "    def forward(self, x, norm_feat=False):\n",
        "        \n",
        "        ### IMPLEMENTATION 1-2 ###\n",
        "        ### Your module must return\n",
        "        ### 1. Conv feature (feat) - when norm_feat is true, apply L2 normalization\n",
        "        ### 2. Projected embedding (emb)\n",
        "        ### 3. Logit vector by the linear classifier (logit)\n",
        "        \n",
        "        feat = self.feat(x)\n",
        "        if norm_feat :\n",
        "          feat = self.norm(feat)\n",
        "        \n",
        "        emb = self.head(feat)\n",
        "        logit = self.classifier(feat)\n",
        "\n",
        "        ### IMPLEMENTATION ENDS HERE ###\n",
        "        return feat, emb, logit"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvxUSFm-uqb9",
        "colab_type": "text"
      },
      "source": [
        "### Step 2. Prepare datasets & data augmentations\n",
        "\n",
        "For contrastaive learning, a set of random augmentation functions is first defined.\n",
        "\n",
        "Then, the set is applied twice to each image, which is implemented as provided DuplicatedCompose module\n",
        "\n",
        "#### Implementation 2-1. Design a train transform (train_transform)\n",
        "\n",
        "Follow the instruction inside the train_transform\n",
        "\n",
        "https://pytorch.org/docs/stable/torchvision/transforms.html\n",
        "\n",
        "Refer to the torchvision.transforms documentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S53vsB4Iuqb9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DuplicatedCompose(object):\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, img):\n",
        "        img1 = img.copy()\n",
        "        img2 = img.copy()\n",
        "        for t in self.transforms:\n",
        "            img1 = t(img1)\n",
        "            img2 = t(img2)\n",
        "        return img1, img2"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ex83YAnMuqcA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "cv2.setNumThreads(0)\n",
        "\n",
        "class GaussianBlur(object):\n",
        "    # Implements Gaussian blur as described in the SimCLR paper\n",
        "    def __init__(self, kernel_size, min=0.1, max=2.0):\n",
        "        self.min = min\n",
        "        self.max = max\n",
        "        # kernel size is set to be 10% of the image height/width\n",
        "        self.kernel_size = kernel_size\n",
        "        \n",
        "        if self.kernel_size % 2 == 0:\n",
        "            self.kernel_size += 1\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        sample = np.array(sample)\n",
        "\n",
        "        # blur the image with a 50% chance\n",
        "        prob = np.random.random_sample()\n",
        "\n",
        "        if prob < 0.5:\n",
        "            sigma = (self.max - self.min) * np.random.random_sample() + self.min\n",
        "            sample = cv2.GaussianBlur(sample, (self.kernel_size, self.kernel_size), sigma)\n",
        "\n",
        "        return sample"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xHmdyuguqcC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision.transforms as transforms\n",
        "\n",
        "img_size = (32, 32)\n",
        "\n",
        "color_jitter = transforms.ColorJitter(0.8, 0.8, 0.8, 0.2)\n",
        "\n",
        "train_transform = DuplicatedCompose([\n",
        "    ### IMPLEMENTATION 2-1 ###\n",
        "    ### 1. Random resized crop w/ final size of (32, 32)\n",
        "    ### 2. Random horizontal flip w/ p=0.5\n",
        "    ### 3. Randomly apply the pre-defined color jittering w/ p=0.8\n",
        "    ### 4. Random gray scale w/ p=0.2\n",
        "    ### 5. Gaussian blur w/ kernel size of 1/10 of the image width or height (32)\n",
        "    transforms.RandomResizedCrop(img_size),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    color_jitter,\n",
        "    transforms.RandomGrayscale(p=0.2),\n",
        "    GaussianBlur(img_size[0]//10),\n",
        "    \n",
        "    ### IMPLEMENTATION ENDS HERE ###\n",
        "    transforms.ToTensor(),\n",
        "])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zo9Xj3KruqcE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "be1b90ce7a8243d8b06031ed18fdf8c8",
            "aee2c650836e4a7fb0e9f771813f6151",
            "d5a5f2a58aac418099c343ef74412cf4",
            "92f91d4f52594e288217c2b4a2aad18b",
            "b4b872255da44d8f95778fe28e2e56e6",
            "19d75b43d27c489a8722112b18ee78c3",
            "56263b9442954498959897e2363c2947",
            "0494970e7ba545e2bfa62e355f1a8d8e"
          ]
        },
        "outputId": "25fd4108-62f3-479a-ae44-4446054fad31"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='.',\n",
        "                                 train=True,\n",
        "                                 download=True,\n",
        "                                 transform=train_transform\n",
        "                                )\n",
        "\n",
        "train_loader = DataLoader(train_dataset,\n",
        "                          batch_size=256,\n",
        "                          num_workers=4,\n",
        "                          shuffle=True,\n",
        "                          drop_last=True\n",
        "                         )"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "be1b90ce7a8243d8b06031ed18fdf8c8",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./cifar-10-python.tar.gz to .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xwkINzBuqcH",
        "colab_type": "text"
      },
      "source": [
        "### Step 3. Implement InfoNCE loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUjxzqf6uqcI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NTXentLoss(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, batch_size, temperature, use_cosine_similarity):\n",
        "        super(NTXentLoss, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.temperature = temperature\n",
        "        self.softmax = torch.nn.Softmax(dim=-1)\n",
        "        self.mask_samples_from_same_repr = self._get_correlated_mask().type(torch.bool)\n",
        "        self.similarity_function = self._get_similarity_function(use_cosine_similarity)\n",
        "        self.criterion = torch.nn.CrossEntropyLoss(reduction=\"sum\")\n",
        "\n",
        "    def _get_similarity_function(self, use_cosine_similarity):\n",
        "        if use_cosine_similarity:\n",
        "            self._cosine_similarity = torch.nn.CosineSimilarity(dim=-1)\n",
        "            return self._cosine_simililarity\n",
        "        else:\n",
        "            return self._dot_simililarity\n",
        "\n",
        "    def _get_correlated_mask(self):\n",
        "        diag = np.eye(2 * self.batch_size)\n",
        "        l1 = np.eye((2 * self.batch_size), 2 * self.batch_size, k=-self.batch_size)\n",
        "        l2 = np.eye((2 * self.batch_size), 2 * self.batch_size, k=self.batch_size)\n",
        "        mask = torch.from_numpy((diag + l1 + l2))\n",
        "        mask = (1 - mask).type(torch.bool)\n",
        "        return mask.cuda()\n",
        "\n",
        "    @staticmethod\n",
        "    def _dot_simililarity(x, y):\n",
        "        v = torch.tensordot(x.unsqueeze(1), y.T.unsqueeze(0), dims=2)\n",
        "        # x shape: (N, 1, C)\n",
        "        # y shape: (1, C, 2N)\n",
        "        # v shape: (N, 2N)\n",
        "        return v\n",
        "\n",
        "    def _cosine_simililarity(self, x, y):\n",
        "        # x shape: (N, 1, C)\n",
        "        # y shape: (1, 2N, C)\n",
        "        # v shape: (N, 2N)\n",
        "        v = self._cosine_similarity(x.unsqueeze(1), y.unsqueeze(0))\n",
        "        return v\n",
        "\n",
        "    def forward(self, zis, zjs):\n",
        "        representations = torch.cat([zjs, zis], dim=0)\n",
        "\n",
        "        similarity_matrix = self.similarity_function(representations, representations)\n",
        "\n",
        "        # filter out the scores from the positive samples\n",
        "        l_pos = torch.diag(similarity_matrix, self.batch_size)\n",
        "        r_pos = torch.diag(similarity_matrix, -self.batch_size)\n",
        "        positives = torch.cat([l_pos, r_pos]).view(2 * self.batch_size, 1)\n",
        "\n",
        "        negatives = similarity_matrix[self.mask_samples_from_same_repr].view(2 * self.batch_size, -1)\n",
        "\n",
        "        logits = torch.cat((positives, negatives), dim=1)\n",
        "        logits = logits / self.temperature\n",
        "\n",
        "        labels = torch.zeros(2 * self.batch_size).cuda().long()\n",
        "        loss = self.criterion(logits, labels)\n",
        "\n",
        "        return loss / (2 * self.batch_size)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-SGunpIuqcK",
        "colab_type": "text"
      },
      "source": [
        "### Step 4. Run pre-training step\n",
        "\n",
        "#### Implementation 4-1. Complete a basic SimCLR training loop\n",
        "\n",
        "https://github.com/ildoonet/pytorch-gradual-warmup-lr\n",
        "\n",
        "The linear warmup scheduler implementation is from a github in the link above\n",
        "\n",
        "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
        "\n",
        "Refer to this documentation to use lr schedulers integrated in PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlL2Yl5AuqcL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.optim.optimizer import Optimizer, required\n",
        "\n",
        "class SGD_with_lars(Optimizer):\n",
        "    r\"\"\"Implements stochastic gradient descent (optionally with momentum).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, lr=required, momentum=0, weight_decay=0, trust_coef=1.): # need to add trust coef\n",
        "        if lr is not required and lr < 0.0:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if momentum < 0.0:\n",
        "            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n",
        "        if weight_decay < 0.0:\n",
        "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
        "        if trust_coef < 0.0:\n",
        "            raise ValueError(\"Invalid trust_coef value: {}\".format(trust_coef))\n",
        "\n",
        "        defaults = dict(lr=lr, momentum=momentum, weight_decay=weight_decay, trust_coef=trust_coef)\n",
        "\n",
        "        super(SGD_with_lars, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(SGD_with_lars, self).__setstate__(state)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "        Arguments:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            weight_decay = group['weight_decay']\n",
        "            momentum = group['momentum']\n",
        "            trust_coef = group['trust_coef']\n",
        "            global_lr = group['lr']\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                d_p = p.grad.data\n",
        "\n",
        "                p_norm = torch.norm(p.data, p=2)\n",
        "                d_p_norm = torch.norm(d_p, p=2).add_(momentum, p_norm)\n",
        "                lr = torch.div(p_norm, d_p_norm).mul_(trust_coef)\n",
        "\n",
        "                lr.mul_(global_lr)\n",
        "\n",
        "                if weight_decay != 0:\n",
        "                    d_p.add_(weight_decay, p.data)\n",
        "\n",
        "                d_p.mul_(lr)\n",
        "\n",
        "                if momentum != 0:\n",
        "                    param_state = self.state[p]\n",
        "                    if 'momentum_buffer' not in param_state:\n",
        "                        buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
        "                    else:\n",
        "                        buf = param_state['momentum_buffer']\n",
        "                        buf.mul_(momentum).add_(d_p)\n",
        "                    d_p = buf\n",
        "\n",
        "                p.data.add_(-1, d_p)\n",
        "\n",
        "        return loss"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fjc3wkcuqcN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(net, loader):\n",
        "    \n",
        "    batch_size=256\n",
        "    temperature=0.07\n",
        "\n",
        "    loss_fn = NTXentLoss(batch_size=batch_size, temperature=temperature, use_cosine_similarity=True)\n",
        "    \n",
        "    ### IMPLEMENTATION 4-2 ###\n",
        "    ### 1. Use SGD_with_lars with\n",
        "    ### lr = 0.1 * batch_size / 256\n",
        "    ### momentum = 0.9\n",
        "    ### weight_decay = 1e-6\n",
        "    optimizer = SGD_with_lars(net.parameters(), lr = 0.1 * batch_size / 256, momentum=0.9, weight_decay=1e-6)\n",
        "    \n",
        "    from warmup_scheduler import GradualWarmupScheduler\n",
        "    ### 2. Use GradualWarmupScheduler with\n",
        "    ### multiplier = 1\n",
        "    ### total_epoch = 1/10 of total epochs\n",
        "    ### after_scheduler = optim.lr_scheduler.CosineAnnealingLR\n",
        "    scheduler = GradualWarmupScheduler(optimizer, multiplier=1, total_epoch=20, \n",
        "                                       after_scheduler=optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=180))\n",
        "    \n",
        "    train_start = time.time()\n",
        "    \n",
        "    loss_hist = []\n",
        "\n",
        "    for epoch in range(1, 200 + 1):\n",
        "        \n",
        "        train_loss = 0\n",
        "        net.train()\n",
        "        \n",
        "        epoch_start = time.time()\n",
        "        for idx, (data, target) in enumerate(loader):\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            ### 3. data variable contains two augmented images\n",
        "            ### -1. send them to your GPU by calling .cuda()\n",
        "            ### -2. forward each of them to net\n",
        "            ### -3. compute the InfoNCE loss\n",
        "            \n",
        "            # target : labels.\n",
        "\n",
        "            zi, zj = data\n",
        "            \n",
        "            feat_i, emb_i, logit_i = net(zi.cuda())\n",
        "            feat_j, emb_j, logit_j = net(zj.cuda())\n",
        "            loss = loss_fn(emb_i, emb_j)\n",
        "\n",
        "            ### IMPLEMENTATION ENDS HERE ###\n",
        "            \n",
        "            train_loss += loss.item()\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "        train_loss /= (idx + 1)\n",
        "        loss_hist.append(train_loss) # added by junwon\n",
        "        scheduler.step()\n",
        "        \n",
        "        epoch_time = time.time() - epoch_start\n",
        "        print(\"Epoch\\t\", epoch, \n",
        "              \"\\tLoss\\t\", train_loss, \n",
        "              \"\\tTime\\t\", epoch_time,\n",
        "             )\n",
        "        \n",
        "    elapsed_train_time = time.time() - train_start\n",
        "    print('Finished training. Train time was:', elapsed_train_time)\n",
        "\n",
        "    return loss_hist\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2Bqj--ZuqcR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "94af207e-1547-49f8-81e7-23777ce83e2b"
      },
      "source": [
        "GPU_NUM = '0'\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = GPU_NUM\n",
        "\n",
        "net = SimCLRNet(26, 1, 10, 32)\n",
        "\n",
        "net.cuda()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SimCLRNet(\n",
              "  (feat): ResNetCifar(\n",
              "    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (layer1): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (downsample): Downsample(\n",
              "          (avg): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "        )\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (downsample): Downsample(\n",
              "          (avg): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "        )\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
              "  )\n",
              "  (head): SimCLRHead(\n",
              "    (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
              "    (norm): Normalize()\n",
              "  )\n",
              "  (classifier): Linear(in_features=64, out_features=10, bias=True)\n",
              "  (norm): Normalize()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43yf5jCluqcT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "70b4b6cc-5c5e-4723-fe3f-fb35082e96c9"
      },
      "source": [
        "#net.zero_grad()\n",
        "loss_list = train(net, train_loader)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:46: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch\t 1 \tLoss\t 6.267516270661965 \tTime\t 68.65935754776001\n",
            "Epoch\t 2 \tLoss\t 5.997548533708621 \tTime\t 69.4774341583252\n",
            "Epoch\t 3 \tLoss\t 5.726462474236121 \tTime\t 69.28733396530151\n",
            "Epoch\t 4 \tLoss\t 5.385201908991887 \tTime\t 69.1840991973877\n",
            "Epoch\t 5 \tLoss\t 5.067203159821339 \tTime\t 69.04848051071167\n",
            "Epoch\t 6 \tLoss\t 4.6063751465235 \tTime\t 69.27956533432007\n",
            "Epoch\t 7 \tLoss\t 4.163073110580444 \tTime\t 69.10657048225403\n",
            "Epoch\t 8 \tLoss\t 3.8278773857997015 \tTime\t 69.57406044006348\n",
            "Epoch\t 9 \tLoss\t 3.618078660964966 \tTime\t 69.37399625778198\n",
            "Epoch\t 10 \tLoss\t 3.4667221289414627 \tTime\t 69.22460794448853\n",
            "Epoch\t 11 \tLoss\t 3.326366952749399 \tTime\t 69.36655616760254\n",
            "Epoch\t 12 \tLoss\t 3.212576044522799 \tTime\t 68.8926649093628\n",
            "Epoch\t 13 \tLoss\t 3.1059215570107486 \tTime\t 69.45085096359253\n",
            "Epoch\t 14 \tLoss\t 3.0056425815973524 \tTime\t 69.58503246307373\n",
            "Epoch\t 15 \tLoss\t 2.9502402220016872 \tTime\t 69.60094261169434\n",
            "Epoch\t 16 \tLoss\t 2.8980539407485573 \tTime\t 69.69247913360596\n",
            "Epoch\t 17 \tLoss\t 2.8292446454366047 \tTime\t 69.91272926330566\n",
            "Epoch\t 18 \tLoss\t 2.796525651980669 \tTime\t 69.46855235099792\n",
            "Epoch\t 19 \tLoss\t 2.751286535996657 \tTime\t 69.60183310508728\n",
            "Epoch\t 20 \tLoss\t 2.725570794863579 \tTime\t 69.23428440093994\n",
            "Epoch\t 21 \tLoss\t 2.68677745843545 \tTime\t 69.87360668182373\n",
            "Epoch\t 22 \tLoss\t 2.6289502498431085 \tTime\t 69.46336817741394\n",
            "Epoch\t 23 \tLoss\t 2.565778276247856 \tTime\t 69.51942682266235\n",
            "Epoch\t 24 \tLoss\t 2.5523773254492346 \tTime\t 68.56583619117737\n",
            "Epoch\t 25 \tLoss\t 2.5162836417173726 \tTime\t 68.56498456001282\n",
            "Epoch\t 26 \tLoss\t 2.4719427059858274 \tTime\t 68.42393064498901\n",
            "Epoch\t 27 \tLoss\t 2.440567861459194 \tTime\t 68.57524490356445\n",
            "Epoch\t 28 \tLoss\t 2.4243976103953826 \tTime\t 67.85034894943237\n",
            "Epoch\t 29 \tLoss\t 2.413613294943785 \tTime\t 68.07151699066162\n",
            "Epoch\t 30 \tLoss\t 2.3794657731667543 \tTime\t 67.70706510543823\n",
            "Epoch\t 31 \tLoss\t 2.3304465990800125 \tTime\t 67.68454360961914\n",
            "Epoch\t 32 \tLoss\t 2.328510089409657 \tTime\t 67.68454313278198\n",
            "Epoch\t 33 \tLoss\t 2.3045998671115973 \tTime\t 67.70062136650085\n",
            "Epoch\t 34 \tLoss\t 2.3009905790671326 \tTime\t 65.9900324344635\n",
            "Epoch\t 35 \tLoss\t 2.2688993539565647 \tTime\t 66.22203993797302\n",
            "Epoch\t 36 \tLoss\t 2.2497884664780052 \tTime\t 65.22188997268677\n",
            "Epoch\t 37 \tLoss\t 2.2232157413776106 \tTime\t 64.85269021987915\n",
            "Epoch\t 38 \tLoss\t 2.226406777822054 \tTime\t 64.0560052394867\n",
            "Epoch\t 39 \tLoss\t 2.2295812050501507 \tTime\t 64.37019276618958\n",
            "Epoch\t 40 \tLoss\t 2.21929702881055 \tTime\t 63.95195126533508\n",
            "Epoch\t 41 \tLoss\t 2.180020068242 \tTime\t 64.59342861175537\n",
            "Epoch\t 42 \tLoss\t 2.160928560525943 \tTime\t 63.999021768569946\n",
            "Epoch\t 43 \tLoss\t 2.1956578480891693 \tTime\t 64.270911693573\n",
            "Epoch\t 44 \tLoss\t 2.1445052813260985 \tTime\t 64.21836233139038\n",
            "Epoch\t 45 \tLoss\t 2.132634694759662 \tTime\t 64.14633202552795\n",
            "Epoch\t 46 \tLoss\t 2.135455678670834 \tTime\t 64.37652349472046\n",
            "Epoch\t 47 \tLoss\t 2.1240479744397676 \tTime\t 63.687742710113525\n",
            "Epoch\t 48 \tLoss\t 2.0787297719564193 \tTime\t 63.95740842819214\n",
            "Epoch\t 49 \tLoss\t 2.1051991059229924 \tTime\t 64.48335433006287\n",
            "Epoch\t 50 \tLoss\t 2.0988356437438576 \tTime\t 63.944589138031006\n",
            "Epoch\t 51 \tLoss\t 2.0975300550460814 \tTime\t 63.56045651435852\n",
            "Epoch\t 52 \tLoss\t 2.0864494030292215 \tTime\t 63.19982552528381\n",
            "Epoch\t 53 \tLoss\t 2.0579177122849686 \tTime\t 63.13495445251465\n",
            "Epoch\t 54 \tLoss\t 2.0632026782402626 \tTime\t 63.20989727973938\n",
            "Epoch\t 55 \tLoss\t 2.0569946533594377 \tTime\t 63.129993200302124\n",
            "Epoch\t 56 \tLoss\t 2.040649994214376 \tTime\t 63.33056855201721\n",
            "Epoch\t 57 \tLoss\t 2.041300230148511 \tTime\t 63.07570481300354\n",
            "Epoch\t 58 \tLoss\t 2.033439413095132 \tTime\t 63.639230251312256\n",
            "Epoch\t 59 \tLoss\t 2.013191468287737 \tTime\t 63.89214277267456\n",
            "Epoch\t 60 \tLoss\t 2.020325320194929 \tTime\t 63.60252404212952\n",
            "Epoch\t 61 \tLoss\t 2.0273418237001466 \tTime\t 63.32847881317139\n",
            "Epoch\t 62 \tLoss\t 1.9968146959940591 \tTime\t 63.407280921936035\n",
            "Epoch\t 63 \tLoss\t 2.018937667210897 \tTime\t 63.60285401344299\n",
            "Epoch\t 64 \tLoss\t 2.00002297376975 \tTime\t 63.400835037231445\n",
            "Epoch\t 65 \tLoss\t 1.9655543339558137 \tTime\t 63.673710107803345\n",
            "Epoch\t 66 \tLoss\t 2.0207853048275677 \tTime\t 64.17776226997375\n",
            "Epoch\t 67 \tLoss\t 1.978113309542338 \tTime\t 63.82818102836609\n",
            "Epoch\t 68 \tLoss\t 1.9487998033181215 \tTime\t 63.906532764434814\n",
            "Epoch\t 69 \tLoss\t 1.945285551975935 \tTime\t 63.87592911720276\n",
            "Epoch\t 70 \tLoss\t 1.9744266436650202 \tTime\t 63.786917209625244\n",
            "Epoch\t 71 \tLoss\t 1.9542911425614968 \tTime\t 63.49198579788208\n",
            "Epoch\t 72 \tLoss\t 1.9325912976876283 \tTime\t 63.555681228637695\n",
            "Epoch\t 73 \tLoss\t 1.9276562910813553 \tTime\t 63.57647895812988\n",
            "Epoch\t 74 \tLoss\t 1.9361189304253994 \tTime\t 63.422266721725464\n",
            "Epoch\t 75 \tLoss\t 1.9350018507395035 \tTime\t 63.535187005996704\n",
            "Epoch\t 76 \tLoss\t 1.926993179321289 \tTime\t 62.564146995544434\n",
            "Epoch\t 77 \tLoss\t 1.9250775862962772 \tTime\t 63.04930877685547\n",
            "Epoch\t 78 \tLoss\t 1.9121704645645925 \tTime\t 63.07352638244629\n",
            "Epoch\t 79 \tLoss\t 1.8979068737763625 \tTime\t 63.042041301727295\n",
            "Epoch\t 80 \tLoss\t 1.8915438193541307 \tTime\t 62.6722686290741\n",
            "Epoch\t 81 \tLoss\t 1.8916537180925026 \tTime\t 63.21482062339783\n",
            "Epoch\t 82 \tLoss\t 1.9038298612985856 \tTime\t 63.78955006599426\n",
            "Epoch\t 83 \tLoss\t 1.8805216654753074 \tTime\t 63.44261360168457\n",
            "Epoch\t 84 \tLoss\t 1.8818345858500554 \tTime\t 63.472145318984985\n",
            "Epoch\t 85 \tLoss\t 1.893834277911064 \tTime\t 62.656704902648926\n",
            "Epoch\t 86 \tLoss\t 1.8679261170900785 \tTime\t 62.974589586257935\n",
            "Epoch\t 87 \tLoss\t 1.8642448761524297 \tTime\t 62.813273191452026\n",
            "Epoch\t 88 \tLoss\t 1.8449247977672478 \tTime\t 63.055755615234375\n",
            "Epoch\t 89 \tLoss\t 1.8696678986916175 \tTime\t 63.302762031555176\n",
            "Epoch\t 90 \tLoss\t 1.8722039497815646 \tTime\t 62.85462260246277\n",
            "Epoch\t 91 \tLoss\t 1.870667578623845 \tTime\t 62.75846743583679\n",
            "Epoch\t 92 \tLoss\t 1.8474222091528085 \tTime\t 63.194554805755615\n",
            "Epoch\t 93 \tLoss\t 1.8648451676735511 \tTime\t 62.94294333457947\n",
            "Epoch\t 94 \tLoss\t 1.8446526399025551 \tTime\t 62.87138080596924\n",
            "Epoch\t 95 \tLoss\t 1.8276663988064497 \tTime\t 63.0919828414917\n",
            "Epoch\t 96 \tLoss\t 1.8224965651830038 \tTime\t 63.08124852180481\n",
            "Epoch\t 97 \tLoss\t 1.8225131157117012 \tTime\t 63.06569290161133\n",
            "Epoch\t 98 \tLoss\t 1.8259998278740124 \tTime\t 63.35394859313965\n",
            "Epoch\t 99 \tLoss\t 1.831270324266874 \tTime\t 63.41454291343689\n",
            "Epoch\t 100 \tLoss\t 1.833235372029818 \tTime\t 62.99313187599182\n",
            "Epoch\t 101 \tLoss\t 1.817083610021151 \tTime\t 62.80047941207886\n",
            "Epoch\t 102 \tLoss\t 1.8051621987269475 \tTime\t 62.8501501083374\n",
            "Epoch\t 103 \tLoss\t 1.8123325830850845 \tTime\t 63.097904920578\n",
            "Epoch\t 104 \tLoss\t 1.7852651388217242 \tTime\t 63.0553183555603\n",
            "Epoch\t 105 \tLoss\t 1.8084005435307822 \tTime\t 62.94760346412659\n",
            "Epoch\t 106 \tLoss\t 1.8041324670498187 \tTime\t 63.25816869735718\n",
            "Epoch\t 107 \tLoss\t 1.7820193205124293 \tTime\t 62.923250675201416\n",
            "Epoch\t 108 \tLoss\t 1.7962806524374546 \tTime\t 62.94171738624573\n",
            "Epoch\t 109 \tLoss\t 1.7930817304513393 \tTime\t 62.812955379486084\n",
            "Epoch\t 110 \tLoss\t 1.789792967453981 \tTime\t 62.79203796386719\n",
            "Epoch\t 111 \tLoss\t 1.7812223055423835 \tTime\t 62.60554313659668\n",
            "Epoch\t 112 \tLoss\t 1.785346525754684 \tTime\t 63.17959928512573\n",
            "Epoch\t 113 \tLoss\t 1.7885493657527827 \tTime\t 63.33392405509949\n",
            "Epoch\t 114 \tLoss\t 1.7671677020879892 \tTime\t 62.74914860725403\n",
            "Epoch\t 115 \tLoss\t 1.7685645977656046 \tTime\t 63.25654315948486\n",
            "Epoch\t 116 \tLoss\t 1.7716081185218615 \tTime\t 62.876837730407715\n",
            "Epoch\t 117 \tLoss\t 1.7829843594477728 \tTime\t 63.14761471748352\n",
            "Epoch\t 118 \tLoss\t 1.772922008465498 \tTime\t 62.828460693359375\n",
            "Epoch\t 119 \tLoss\t 1.7659885754952065 \tTime\t 63.11888647079468\n",
            "Epoch\t 120 \tLoss\t 1.7448960218674097 \tTime\t 63.22929286956787\n",
            "Epoch\t 121 \tLoss\t 1.7520486501547006 \tTime\t 63.6432888507843\n",
            "Epoch\t 122 \tLoss\t 1.7521573433509239 \tTime\t 62.95422625541687\n",
            "Epoch\t 123 \tLoss\t 1.7678827206293741 \tTime\t 63.3798565864563\n",
            "Epoch\t 124 \tLoss\t 1.7502376458583735 \tTime\t 62.8237099647522\n",
            "Epoch\t 125 \tLoss\t 1.769895480840634 \tTime\t 62.9195613861084\n",
            "Epoch\t 126 \tLoss\t 1.756000240643819 \tTime\t 62.95954418182373\n",
            "Epoch\t 127 \tLoss\t 1.7506872965739324 \tTime\t 62.67074799537659\n",
            "Epoch\t 128 \tLoss\t 1.745745114179758 \tTime\t 63.118263483047485\n",
            "Epoch\t 129 \tLoss\t 1.7377324495560085 \tTime\t 63.108617067337036\n",
            "Epoch\t 130 \tLoss\t 1.7158134686641204 \tTime\t 63.02924919128418\n",
            "Epoch\t 131 \tLoss\t 1.7412236030285175 \tTime\t 62.89516854286194\n",
            "Epoch\t 132 \tLoss\t 1.7306649568753365 \tTime\t 62.679909229278564\n",
            "Epoch\t 133 \tLoss\t 1.7245748746089447 \tTime\t 62.66066098213196\n",
            "Epoch\t 134 \tLoss\t 1.7289566480196439 \tTime\t 63.32841229438782\n",
            "Epoch\t 135 \tLoss\t 1.7108234833448361 \tTime\t 62.491352558135986\n",
            "Epoch\t 136 \tLoss\t 1.6991298583837655 \tTime\t 62.3332724571228\n",
            "Epoch\t 137 \tLoss\t 1.7115736839098807 \tTime\t 62.74306297302246\n",
            "Epoch\t 138 \tLoss\t 1.7007082712955963 \tTime\t 62.762959718704224\n",
            "Epoch\t 139 \tLoss\t 1.7187429745992024 \tTime\t 62.61979103088379\n",
            "Epoch\t 140 \tLoss\t 1.6926693610655956 \tTime\t 62.504223346710205\n",
            "Epoch\t 141 \tLoss\t 1.7033146136846298 \tTime\t 62.608073472976685\n",
            "Epoch\t 142 \tLoss\t 1.7205791528408343 \tTime\t 62.628968715667725\n",
            "Epoch\t 143 \tLoss\t 1.6938049933849237 \tTime\t 62.75688982009888\n",
            "Epoch\t 144 \tLoss\t 1.7151904136706622 \tTime\t 63.039963483810425\n",
            "Epoch\t 145 \tLoss\t 1.6962251809927134 \tTime\t 63.067909717559814\n",
            "Epoch\t 146 \tLoss\t 1.6823487367385472 \tTime\t 62.384145736694336\n",
            "Epoch\t 147 \tLoss\t 1.6853399460132306 \tTime\t 62.321884632110596\n",
            "Epoch\t 148 \tLoss\t 1.6768621634214351 \tTime\t 62.94698643684387\n",
            "Epoch\t 149 \tLoss\t 1.6715527577277942 \tTime\t 62.54809641838074\n",
            "Epoch\t 150 \tLoss\t 1.665389627065414 \tTime\t 62.6585054397583\n",
            "Epoch\t 151 \tLoss\t 1.6830049331371602 \tTime\t 62.95280051231384\n",
            "Epoch\t 152 \tLoss\t 1.6835978721960998 \tTime\t 62.745948791503906\n",
            "Epoch\t 153 \tLoss\t 1.67967787644802 \tTime\t 62.68786954879761\n",
            "Epoch\t 154 \tLoss\t 1.6763857138462557 \tTime\t 62.95465064048767\n",
            "Epoch\t 155 \tLoss\t 1.6782261481651892 \tTime\t 62.714362382888794\n",
            "Epoch\t 156 \tLoss\t 1.6617795449036818 \tTime\t 62.96763586997986\n",
            "Epoch\t 157 \tLoss\t 1.675019753896273 \tTime\t 62.9510543346405\n",
            "Epoch\t 158 \tLoss\t 1.675074587112818 \tTime\t 65.35823965072632\n",
            "Epoch\t 159 \tLoss\t 1.6546062561181876 \tTime\t 71.55635094642639\n",
            "Epoch\t 160 \tLoss\t 1.6746668840065981 \tTime\t 71.21927261352539\n",
            "Epoch\t 161 \tLoss\t 1.6435573663467016 \tTime\t 69.76940560340881\n",
            "Epoch\t 162 \tLoss\t 1.6557708746347672 \tTime\t 69.46242380142212\n",
            "Epoch\t 163 \tLoss\t 1.6678790911650045 \tTime\t 69.35124659538269\n",
            "Epoch\t 164 \tLoss\t 1.664384509355594 \tTime\t 71.1125271320343\n",
            "Epoch\t 165 \tLoss\t 1.6600723688419048 \tTime\t 71.94162893295288\n",
            "Epoch\t 166 \tLoss\t 1.661243896606641 \tTime\t 72.01282262802124\n",
            "Epoch\t 167 \tLoss\t 1.6629147285070174 \tTime\t 72.33958768844604\n",
            "Epoch\t 168 \tLoss\t 1.6620125477130596 \tTime\t 72.95786356925964\n",
            "Epoch\t 169 \tLoss\t 1.6602887862767928 \tTime\t 73.96707892417908\n",
            "Epoch\t 170 \tLoss\t 1.6492626385811047 \tTime\t 74.312002658844\n",
            "Epoch\t 171 \tLoss\t 1.6598077217737834 \tTime\t 74.04334545135498\n",
            "Epoch\t 172 \tLoss\t 1.6313616501979338 \tTime\t 73.81621289253235\n",
            "Epoch\t 173 \tLoss\t 1.6453075335575984 \tTime\t 73.28137230873108\n",
            "Epoch\t 174 \tLoss\t 1.6391288347733326 \tTime\t 71.95601725578308\n",
            "Epoch\t 175 \tLoss\t 1.6526925508792585 \tTime\t 70.92093181610107\n",
            "Epoch\t 176 \tLoss\t 1.6408355028201371 \tTime\t 70.17202544212341\n",
            "Epoch\t 177 \tLoss\t 1.6741687591259296 \tTime\t 69.64730167388916\n",
            "Epoch\t 178 \tLoss\t 1.6493007904443986 \tTime\t 69.17150163650513\n",
            "Epoch\t 179 \tLoss\t 1.6307186945890768 \tTime\t 67.66972494125366\n",
            "Epoch\t 180 \tLoss\t 1.6318966621007676 \tTime\t 67.56845283508301\n",
            "Epoch\t 181 \tLoss\t 1.6460150443590604 \tTime\t 66.0144100189209\n",
            "Epoch\t 182 \tLoss\t 1.6342561336664052 \tTime\t 65.29189372062683\n",
            "Epoch\t 183 \tLoss\t 1.6374236284158168 \tTime\t 65.38262867927551\n",
            "Epoch\t 184 \tLoss\t 1.6324674893648197 \tTime\t 65.18134713172913\n",
            "Epoch\t 185 \tLoss\t 1.6391727582002298 \tTime\t 65.1057243347168\n",
            "Epoch\t 186 \tLoss\t 1.6311474506671613 \tTime\t 64.95918345451355\n",
            "Epoch\t 187 \tLoss\t 1.6350793679555258 \tTime\t 64.83811283111572\n",
            "Epoch\t 188 \tLoss\t 1.6467056940763425 \tTime\t 64.92646360397339\n",
            "Epoch\t 189 \tLoss\t 1.6347985512171037 \tTime\t 65.87502717971802\n",
            "Epoch\t 190 \tLoss\t 1.6369034186387674 \tTime\t 65.06599950790405\n",
            "Epoch\t 191 \tLoss\t 1.6355789410762298 \tTime\t 64.79733896255493\n",
            "Epoch\t 192 \tLoss\t 1.6241045731764574 \tTime\t 64.38663864135742\n",
            "Epoch\t 193 \tLoss\t 1.6254989746289374 \tTime\t 63.669490575790405\n",
            "Epoch\t 194 \tLoss\t 1.6319795125570054 \tTime\t 63.83303713798523\n",
            "Epoch\t 195 \tLoss\t 1.6416837533315023 \tTime\t 64.1012270450592\n",
            "Epoch\t 196 \tLoss\t 1.625650544044299 \tTime\t 64.25086784362793\n",
            "Epoch\t 197 \tLoss\t 1.6344465396343133 \tTime\t 64.62059736251831\n",
            "Epoch\t 198 \tLoss\t 1.6315676242877275 \tTime\t 64.30861186981201\n",
            "Epoch\t 199 \tLoss\t 1.6486536013774382 \tTime\t 64.37908816337585\n",
            "Epoch\t 200 \tLoss\t 1.6273717764096383 \tTime\t 64.58815407752991\n",
            "Finished training. Train time was: 13051.887996673584\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8TTC-0U-Xkt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_loss(loss_hist, xlabel='Iteration number', ylabel='Loss value') :\n",
        "  plt.plot(loss_hist)\n",
        "  plt.xlabel(xlabel)\n",
        "  plt.ylabel(ylabel)\n",
        "  plt.show()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVo5Msnl--6B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "5998825a-5a5a-4f56-a8b5-632444f16936"
      },
      "source": [
        "plot_loss(loss_list)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ufE49N8BcxA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_model(model, model_save_name = 'SIMCLR.pt' ) :\n",
        "  # save ckpt in google drive.\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "  path = F\"/content/gdrive/My Drive/{model_save_name}\" \n",
        "  torch.save(model.state_dict(), path)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztekTngYVO20",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_model(model, path='SIMCLR.pt') :\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "  checkpoints_folder =  F\"/content/gdrive/My Drive/{path}\" \n",
        "  state_dict = torch.load(checkpoints_folder)\n",
        "  model.load_state_dict(state_dict)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CPurz9SuqcV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3f632672-f02a-461b-b490-ff3e7d9962f7"
      },
      "source": [
        "save_model(net, \"simclr_new.pt\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6q9K4_lgIJB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "52d52b92-8d1f-499d-dede-87be246cbea7"
      },
      "source": [
        "load_model(net, \"simclr_new.pt\")"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDATNL_YuUIP",
        "colab_type": "text"
      },
      "source": [
        "## Linear Evaluation Protocol\n",
        "\n",
        "- train Linear classifier with freezed extractor f"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocucN1fyuTZU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_classifier(net, loader):\n",
        "    \n",
        "    batch_size=256\n",
        "    temperature=0.07\n",
        "\n",
        "    loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    # Freezing\n",
        "    net.feat.requires_grad = False\n",
        "    net.head.requires_grad = False\n",
        "    net.norm.requires_grad = False\n",
        "\n",
        "   # net.feat.train(False)\n",
        "  \n",
        "    optimizer = torch.optim.Adam(net.classifier.parameters(), lr=1e-3)\n",
        "    \n",
        "    train_start = time.time()\n",
        "    \n",
        "    loss_hist = []\n",
        "    min_loss = 10000\n",
        "\n",
        "    for epoch in range(1, 30 + 1):\n",
        "        \n",
        "        train_loss = 0\n",
        "        #net.feat.train(False)\n",
        "        #net.classifier.train(True) \n",
        "        net.train()\n",
        "        epoch_start = time.time()\n",
        "\n",
        "        for idx, (data, target) in enumerate(loader):\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            feat, emb, logit = net(data.cuda())\n",
        "            \n",
        "            loss = loss_fn(logit, target.cuda())\n",
        "          \n",
        "            train_loss += loss.item()\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "        train_loss /= (idx + 1)\n",
        "        loss_hist.append(train_loss) # added by junwon\n",
        "        if train_loss < min_loss :\n",
        "          min_loss = train_loss\n",
        "          save_model(net, \"classifier\")\n",
        "       \n",
        "        epoch_time = time.time() - epoch_start\n",
        "        print(\"Epoch\\t\", epoch, \n",
        "              \"\\tLoss\\t\", train_loss, \n",
        "              \"\\tTime\\t\", epoch_time,\n",
        "             )\n",
        "        \n",
        "    elapsed_train_time = time.time() - train_start\n",
        "    print('Finished training. Train time was:', elapsed_train_time)\n",
        "\n",
        "    return loss_hist\n",
        "\n",
        "def test_classifier(net, loader) :\n",
        "  \n",
        "  net.eval()\n",
        "  \n",
        "  correct = 0\n",
        "  total = 0\n",
        "  \n",
        "  with torch.no_grad() :\n",
        "    for idx, (data, target) in enumerate(loader):\n",
        "      \n",
        "      feat, emb, logit = net(data.cuda())\n",
        "      #loss = loss_fn(logit, target.cuda())\n",
        "      \n",
        "      _, class_i = torch.max(logit.data, 1)\n",
        "\n",
        "      correct += (class_i == target.cuda()).sum().item()  \n",
        "      total += target.size(0)\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print('Accuracy : %d %%' % (100 * accuracy))\n",
        "  \n",
        "  return accuracy\n",
        "  \n"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTV1vdlDYlWt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "73b152d7-b730-4385-ad65-297f06119eb6"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "linear_dataset = datasets.CIFAR10(root='.',\n",
        "                                 train=True,\n",
        "                                 download=True,\n",
        "                                 transform=transforms.ToTensor()\n",
        "                                )\n",
        "\n",
        "linear_loader = DataLoader(linear_dataset,\n",
        "                          batch_size=256,\n",
        "                          num_workers=4,\n",
        "                          shuffle=True,\n",
        "                          drop_last=True\n",
        "                         )\n",
        "\n",
        "test_dataset = datasets.CIFAR10(root='.',\n",
        "                                 train=False,\n",
        "                                 download=True,\n",
        "                                 transform=transforms.ToTensor()\n",
        "                                )\n",
        "\n",
        "test_loader = DataLoader(test_dataset,\n",
        "                          batch_size=256,\n",
        "                          num_workers=4,\n",
        "                          shuffle=True,\n",
        "                          drop_last=True\n",
        "                         )"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gBtJya5UsO1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "70fde04b-12ee-466e-f63a-e8753a69243a"
      },
      "source": [
        "loss_list = train_classifier(net, linear_loader)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Epoch\t 1 \tLoss\t 2.03468404121888 \tTime\t 17.137782096862793\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Epoch\t 2 \tLoss\t 1.6301073435025337 \tTime\t 17.24441909790039\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Epoch\t 3 \tLoss\t 1.4283286290291028 \tTime\t 17.066750288009644\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Epoch\t 4 \tLoss\t 1.3150144711518899 \tTime\t 17.026099681854248\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Epoch\t 5 \tLoss\t 1.2440750293242626 \tTime\t 17.067901849746704\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Epoch\t 6 \tLoss\t 1.1951002854567307 \tTime\t 17.105433702468872\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Epoch\t 7 \tLoss\t 1.1595835169156392 \tTime\t 17.125438451766968\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Epoch\t 8 \tLoss\t 1.1338184274159946 \tTime\t 17.08957076072693\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Epoch\t 9 \tLoss\t 1.1112631134497815 \tTime\t 17.097830295562744\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Epoch\t 10 \tLoss\t 1.0948643947258974 \tTime\t 17.093303203582764\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Epoch\t 11 \tLoss\t 1.0812407942918631 \tTime\t 17.017643213272095\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Epoch\t 12 \tLoss\t 1.0695079476405414 \tTime\t 17.06408953666687\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Epoch\t 13 \tLoss\t 1.0593032726874718 \tTime\t 17.09551215171814\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Epoch\t 14 \tLoss\t 1.0512505369308667 \tTime\t 17.128983736038208\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Epoch\t 15 \tLoss\t 1.0428527550819593 \tTime\t 17.111952304840088\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Epoch\t 16 \tLoss\t 1.0376738615525074 \tTime\t 17.099048852920532\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Epoch\t 17 \tLoss\t 1.031489634513855 \tTime\t 17.15119194984436\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Epoch\t 18 \tLoss\t 1.027635129292806 \tTime\t 17.12689518928528\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Epoch\t 19 \tLoss\t 1.022793124577938 \tTime\t 17.13813066482544\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Epoch\t 20 \tLoss\t 1.0190817640377925 \tTime\t 17.150291204452515\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Epoch\t 21 \tLoss\t 1.014194057537959 \tTime\t 17.08976435661316\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Epoch\t 22 \tLoss\t 1.0108654868908418 \tTime\t 17.163275003433228\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Epoch\t 23 \tLoss\t 1.0085886481480721 \tTime\t 17.21928095817566\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Epoch\t 24 \tLoss\t 1.006259562113346 \tTime\t 17.208226680755615\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Epoch\t 25 \tLoss\t 1.002286654863602 \tTime\t 17.335190057754517\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Epoch\t 26 \tLoss\t 0.9998141579138927 \tTime\t 17.245723962783813\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Epoch\t 27 \tLoss\t 0.9975449063839057 \tTime\t 17.2394061088562\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Epoch\t 28 \tLoss\t 0.9960140689825401 \tTime\t 17.307114362716675\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Epoch\t 29 \tLoss\t 0.9927120361572657 \tTime\t 17.32283854484558\n",
            "Epoch\t 30 \tLoss\t 0.9929774443308512 \tTime\t 17.321990966796875\n",
            "Finished training. Train time was: 514.6455535888672\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNUX8YOYgnNW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "49721a9c-523b-4935-e7cb-9f3319b94b09"
      },
      "source": [
        "acc = test_classifier(net, test_loader)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy : 64 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yKmtkiya2FF",
        "colab_type": "text"
      },
      "source": [
        "## classifierë¥¼ Trainí•  ë•Œ Epoch\n",
        "ê³„ì† Loss ë‚®ì•„ì§€ì§€ë§Œ, ì´ë¯¸ 60%ë¥¼ ë„˜ìœ¼ë¯€ë¡œ. \n",
        "Pretrain ì„±ëŠ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•´ \n",
        "\"ì´ˆê¸°ì— ì–¼ë§ˆë‚˜ íŠ¸ë ˆì¸ì´ ìž˜ ë˜ëŠ”ì§€\" ë¥¼ í‰ê°€í•˜ê¸° ìœ„í•´ì„œ ì—í­ì„ ëŠ˜ë¦¬ì§€ ì•ŠìŒ. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pek2bM80tkes",
        "colab_type": "text"
      },
      "source": [
        "##Train Function with Parameters.\n",
        "\n",
        "- save the model with lowest loss\n",
        "- parameters : batch_size, temparture, pre-training epochs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ak1IDc6gtpqW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_param(net, loader, batch_size = 256, temparature = 0.07, n_epoch = 200):\n",
        "    \n",
        "\n",
        "    loss_fn = NTXentLoss(batch_size=batch_size, temperature=temperature, use_cosine_similarity=True)\n",
        "    \n",
        "\n",
        "    net.feat.weight.requries_grad = True\n",
        "    net.feat.bias.requries_grad = True\n",
        "\n",
        "    ### IMPLEMENTATION 4-2 ###\n",
        "    ### 1. Use SGD_with_lars with\n",
        "    ### lr = 0.1 * batch_size / 256\n",
        "    ### momentum = 0.9\n",
        "    ### weight_decay = 1e-6\n",
        "    optimizer = SGD_with_lars(net.parameters(), lr = 0.1 * batch_size / 256, momentum=0.9, weight_decay=1e-6)\n",
        "    \n",
        "    from warmup_scheduler import GradualWarmupScheduler\n",
        "    ### 2. Use GradualWarmupScheduler with\n",
        "    ### multiplier = 1\n",
        "    ### total_epoch = 1/10 of total epochs\n",
        "    ### after_scheduler = optim.lr_scheduler.CosineAnnealingLR\n",
        "    scheduler = GradualWarmupScheduler(optimizer, multiplier=1, total_epoch= n_epoch // 10, \n",
        "                                       after_scheduler=optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max= n_epoch))\n",
        "    \n",
        "    train_start = time.time()\n",
        "    \n",
        "    loss_hist = []\n",
        "\n",
        "    for epoch in range(1, n_epoch + 1):\n",
        "        \n",
        "        train_loss = 0\n",
        "        net.train()\n",
        "        \n",
        "        epoch_start = time.time()\n",
        "        for idx, (data, target) in enumerate(loader):\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            ### 3. data variable contains two augmented images\n",
        "            ### -1. send them to your GPU by calling .cuda()\n",
        "            ### -2. forward each of them to net\n",
        "            ### -3. compute the InfoNCE loss\n",
        "            \n",
        "            # target : labels.\n",
        "\n",
        "            zi, zj = data\n",
        "            feat_i, emb_i, logit_i = net(zi.cuda())\n",
        "            feat_j, emb_j, logit_j = net(zj.cuda())\n",
        "            \n",
        "            loss = loss_fn(emb_i, emb_j)\n",
        "\n",
        "            ### IMPLEMENTATION ENDS HERE ###\n",
        "            \n",
        "            train_loss += loss.item()\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "        train_loss /= (idx + 1)\n",
        "        loss_hist.append(train_loss) # added by junwon\n",
        "        scheduler.step()\n",
        "        \n",
        "        epoch_time = time.time() - epoch_start\n",
        "        print(\"Epoch\\t\", epoch, \n",
        "              \"\\tLoss\\t\", train_loss, \n",
        "              \"\\tTime\\t\", epoch_time,\n",
        "             )\n",
        "        \n",
        "    elapsed_train_time = time.time() - train_start\n",
        "    print('Finished training. Train time was:', elapsed_train_time)\n",
        "\n",
        "    return loss_hist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVW2dg4wcplr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = [64, 128, 256, 512, 1024]\n",
        "temperature = [0.01, 0.05, 0.1, 0.5, 1.0]\n",
        "n_epoch = [100, 200, 300, 400]\n",
        "\n",
        "batch_loss =[]\n",
        "temparature_loss =[]\n",
        "epoch_loss =[]\n",
        "\n",
        "batch_acc =[]\n",
        "temparature_acc =[]\n",
        "epoch_acc =[]\n",
        "\n",
        "# batch_size test\n",
        "\n",
        "for bs in batch_size :\n",
        "  net = SimCLRNet(26, 1, 10, 32)\n",
        "  net.cuda()\n",
        "  net.zero_grad()\n",
        "  train_param(net, train_loader, batch_size=bs)\n",
        "  batch_loss.append(loss)\n",
        "  train_classifier(net, linear_loader)\n",
        "  acc = test_classifer(net, test_loader)\n",
        "  batch_acc.append((bs, acc))\n",
        "  save_model(net, \"simclr_batch_{}.pt\".format(bs))\n",
        "\n",
        "for t in temparautre :\n",
        "  net = SimCLRNet(26, 1, 10, 32)\n",
        "  net.cuda()\n",
        "  net.zero_grad()\n",
        "  train_param(net, train_loader, temparature=t)\n",
        "  temparature_loss.append(loss)\n",
        "  train_classifier(net, train_loader)\n",
        "  acc = test_classifer(net, test_loader)\n",
        "  temparature_acc.append((t, acc))\n",
        "  save_model(net, \"simclr_tmp_{}.pt\".format(t))\n",
        "\n",
        "for n in n_epoch :\n",
        "  net = SimCLRNet(26, 1, 10, 32)\n",
        "  net.cuda()\n",
        "  net.zero_grad()\n",
        "  train_param(net, train_loader, n_epoch=n)\n",
        "  epoch_loss.append(loss)\n",
        "  train_classifier(net, linear_loader)\n",
        "  acc = test_classifer(net, test_loader)\n",
        "  epoch_acc.append((n, acc))\n",
        "  save_model(net, \"simclr_epoch_{}.pt\".format(n))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}