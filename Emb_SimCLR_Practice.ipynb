{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "Emb_SimCLR_Practice.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/givenone/embedded/blob/master/Emb_SimCLR_Practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIbUUOphvNRR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "6b0b5bd3-0052-4e32-c5c9-81986a0d2d30"
      },
      "source": [
        "  pip install git+https://github.com/ildoonet/pytorch-gradual-warmup-lr.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/ildoonet/pytorch-gradual-warmup-lr.git\n",
            "  Cloning https://github.com/ildoonet/pytorch-gradual-warmup-lr.git to /tmp/pip-req-build-gksnu771\n",
            "  Running command git clone -q https://github.com/ildoonet/pytorch-gradual-warmup-lr.git /tmp/pip-req-build-gksnu771\n",
            "Building wheels for collected packages: warmup-scheduler\n",
            "  Building wheel for warmup-scheduler (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for warmup-scheduler: filename=warmup_scheduler-0.3.2-cp36-none-any.whl size=3881 sha256=ae56a3a3db89b9396a78f25aa467969e004f23b89306a0ad72909d7caa7f5b7c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-jsc2rbb3/wheels/b7/24/83/d30234cc013cff538805b14df916e79091f7cf9ee2c5bf3a64\n",
            "Successfully built warmup-scheduler\n",
            "Installing collected packages: warmup-scheduler\n",
            "Successfully installed warmup-scheduler-0.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYtKgry8uqb3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "\n",
        "import time"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZ4WM8_Guqb6",
        "colab_type": "text"
      },
      "source": [
        "### Step 1. Construct a CNN model\n",
        "\n",
        "#### Implementation 1-1. Design SimCLRHead class\n",
        "\n",
        "#### Implementation 1-2. Design SimCLRNet class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvfdvwkQuqb6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "from torchvision.models.resnet import conv3x3\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, inplanes, planes, norm_layer, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "        \n",
        "        self.bn1 = norm_layer(inplanes)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        \n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x \n",
        "        residual = self.bn1(residual)\n",
        "        residual = self.relu1(residual)\n",
        "        residual = self.conv1(residual)\n",
        "\n",
        "        residual = self.bn2(residual)\n",
        "        residual = self.relu2(residual)\n",
        "        residual = self.conv2(residual)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            x = self.downsample(x)\n",
        "        return x + residual\n",
        "\n",
        "class Downsample(nn.Module):\n",
        "    def __init__(self, nIn, nOut, stride):\n",
        "        super(Downsample, self).__init__()\n",
        "        self.avg = nn.AvgPool2d(stride)\n",
        "        assert nOut % nIn == 0\n",
        "        self.expand_ratio = nOut // nIn\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.avg(x)\n",
        "        return torch.cat([x] + [x.mul(0)] * (self.expand_ratio - 1), 1)\n",
        "\n",
        "class ResNetCifar(nn.Module):\n",
        "    def __init__(self, depth, width=1, classes=10, channels=3, norm_layer=nn.BatchNorm2d):\n",
        "        assert (depth - 2) % 6 == 0         # depth is 6N+2\n",
        "        self.N = (depth - 2) // 6\n",
        "        super(ResNetCifar, self).__init__()\n",
        "\n",
        "        # Following the Wide ResNet convention, we fix the very first convolution\n",
        "        self.conv1 = nn.Conv2d(channels, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.inplanes = 16\n",
        "        self.layer1 = self._make_layer(norm_layer, 16 * width)\n",
        "        self.layer2 = self._make_layer(norm_layer, 32 * width, stride=2)\n",
        "        self.layer3 = self._make_layer(norm_layer, 64 * width, stride=2)\n",
        "        self.bn = norm_layer(64 * width)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.avgpool = nn.AvgPool2d(8)\n",
        "\n",
        "        # Initialization\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "                \n",
        "    def _make_layer(self, norm_layer, planes, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes:\n",
        "            downsample = Downsample(self.inplanes, planes, stride)\n",
        "        layers = [BasicBlock(self.inplanes, planes, norm_layer, stride, downsample)]\n",
        "        self.inplanes = planes\n",
        "        for i in range(self.N - 1):\n",
        "            layers.append(BasicBlock(self.inplanes, planes, norm_layer))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x\n",
        "\n",
        "    \n",
        "class Normalize(nn.Module):\n",
        "\n",
        "    def __init__(self, power=2):\n",
        "        super(Normalize, self).__init__()\n",
        "        self.power = power\n",
        "\n",
        "    def forward(self, x):\n",
        "        norm = x.pow(self.power).sum(1, keepdim=True).pow(1. / self.power)\n",
        "        out = x.div(norm)\n",
        "        return out\n",
        "    \n",
        "\n",
        "class SimCLRHead(nn.Module):\n",
        "    def __init__(self, width, emb_dim):\n",
        "        super(SimCLRHead, self).__init__()\n",
        "        \n",
        "        ### IMPLEMENTATION 1-1 ###\n",
        "        ### 1. Linear layer (64 * width -> 64 * width)\n",
        "        ### 2. ReLU\n",
        "        ### 3. Linear layer (64 * width -> emb_dim)\n",
        "        ### 4. Normalization layer (Normalize module above)\n",
        "        self.fc1 = nn.Linear(64 * width, 64 * width)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.fc2 = nn.Linear(64 * width, 64 * width)\n",
        "        self.norm = Normalize()\n",
        "        ### IMPLEMENTATION ENDS HERE ###\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        ### IMPLEMENTATION 1-1 ###\n",
        "        ### Design a proper forward function\n",
        "        \n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.norm(x)\n",
        "\n",
        "        ### IMPLEMENTATION ENDS HERE ###\n",
        "        return x\n",
        "    \n",
        "\n",
        "class SimCLRNet(nn.Module):\n",
        "    def __init__(self, depth, width=1, num_classes=10, emb_dim=32):\n",
        "        super(SimCLRNet, self).__init__()\n",
        "        \n",
        "        self.num_classes = num_classes\n",
        "        \n",
        "        self.feat = ResNetCifar(depth=depth, width=width, classes=num_classes)\n",
        "        \n",
        "        ### IMPLEMENTATION 1-2 ###\n",
        "        ### 1. A projection head (SimCLRHead module above)\n",
        "        self.head = SimCLRHead(width, emb_dim)\n",
        "        \n",
        "        ### 2. A linear classifier (64 * width -> num_classes)\n",
        "        self.classifier = nn.Linear(64 * width, num_classes)\n",
        "        \n",
        "        ### 3. Normalization layer for conv feature normalization (Normalize module above)\n",
        "        self.norm = Normalize()\n",
        "        \n",
        "        ### IMPLEMENTATION ENDS HERE ###\n",
        "    \n",
        "    def forward(self, x, norm_feat=False):\n",
        "        \n",
        "        ### IMPLEMENTATION 1-2 ###\n",
        "        ### Your module must return\n",
        "        ### 1. Conv feature (feat) - when norm_feat is true, apply L2 normalization\n",
        "        ### 2. Projected embedding (emb)\n",
        "        ### 3. Logit vector by the linear classifier (logit)\n",
        "        \n",
        "        feat = self.feat(x)\n",
        "        if norm_feat :\n",
        "          feat = self.norm(x)\n",
        "        \n",
        "        emb = self.head(feat)\n",
        "        logit = self.classifier(feat)\n",
        "\n",
        "        ### IMPLEMENTATION ENDS HERE ###\n",
        "        return feat, emb, logit"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvxUSFm-uqb9",
        "colab_type": "text"
      },
      "source": [
        "### Step 2. Prepare datasets & data augmentations\n",
        "\n",
        "For contrastaive learning, a set of random augmentation functions is first defined.\n",
        "\n",
        "Then, the set is applied twice to each image, which is implemented as provided DuplicatedCompose module\n",
        "\n",
        "#### Implementation 2-1. Design a train transform (train_transform)\n",
        "\n",
        "Follow the instruction inside the train_transform\n",
        "\n",
        "https://pytorch.org/docs/stable/torchvision/transforms.html\n",
        "\n",
        "Refer to the torchvision.transforms documentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S53vsB4Iuqb9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DuplicatedCompose(object):\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, img):\n",
        "        img1 = img.copy()\n",
        "        img2 = img.copy()\n",
        "        for t in self.transforms:\n",
        "            img1 = t(img1)\n",
        "            img2 = t(img2)\n",
        "        return img1, img2"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ex83YAnMuqcA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "cv2.setNumThreads(0)\n",
        "\n",
        "class GaussianBlur(object):\n",
        "    # Implements Gaussian blur as described in the SimCLR paper\n",
        "    def __init__(self, kernel_size, min=0.1, max=2.0):\n",
        "        self.min = min\n",
        "        self.max = max\n",
        "        # kernel size is set to be 10% of the image height/width\n",
        "        self.kernel_size = kernel_size\n",
        "        \n",
        "        if self.kernel_size % 2 == 0:\n",
        "            self.kernel_size += 1\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        sample = np.array(sample)\n",
        "\n",
        "        # blur the image with a 50% chance\n",
        "        prob = np.random.random_sample()\n",
        "\n",
        "        if prob < 0.5:\n",
        "            sigma = (self.max - self.min) * np.random.random_sample() + self.min\n",
        "            sample = cv2.GaussianBlur(sample, (self.kernel_size, self.kernel_size), sigma)\n",
        "\n",
        "        return sample"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xHmdyuguqcC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision.transforms as transforms\n",
        "\n",
        "img_size = (32, 32)\n",
        "\n",
        "color_jitter = transforms.ColorJitter(0.8, 0.8, 0.8, 0.2)\n",
        "\n",
        "train_transform = DuplicatedCompose([\n",
        "    ### IMPLEMENTATION 2-1 ###\n",
        "    ### 1. Random resized crop w/ final size of (32, 32)\n",
        "    ### 2. Random horizontal flip w/ p=0.5\n",
        "    ### 3. Randomly apply the pre-defined color jittering w/ p=0.8\n",
        "    ### 4. Random gray scale w/ p=0.2\n",
        "    ### 5. Gaussian blur w/ kernel size of 1/10 of the image width or height (32)\n",
        "    transforms.RandomCrop(img_size),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    color_jitter,\n",
        "    transforms.RandomGrayscale(p=0.2),\n",
        "    GaussianBlur(img_size[0]//10),\n",
        "    \n",
        "    ### IMPLEMENTATION ENDS HERE ###\n",
        "    transforms.ToTensor(),\n",
        "])"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zo9Xj3KruqcE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "62eedddb-ba5d-472b-d17c-1764d9636efd"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='.',\n",
        "                                 train=True,\n",
        "                                 download=True,\n",
        "                                 transform=train_transform\n",
        "                                )\n",
        "\n",
        "train_loader = DataLoader(train_dataset,\n",
        "                          batch_size=256,\n",
        "                          num_workers=4,\n",
        "                          shuffle=True,\n",
        "                          drop_last=True\n",
        "                         )"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xwkINzBuqcH",
        "colab_type": "text"
      },
      "source": [
        "### Step 3. Implement InfoNCE loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUjxzqf6uqcI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NTXentLoss(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, batch_size, temperature, use_cosine_similarity):\n",
        "        super(NTXentLoss, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.temperature = temperature\n",
        "        self.softmax = torch.nn.Softmax(dim=-1)\n",
        "        self.mask_samples_from_same_repr = self._get_correlated_mask().type(torch.bool)\n",
        "        self.similarity_function = self._get_similarity_function(use_cosine_similarity)\n",
        "        self.criterion = torch.nn.CrossEntropyLoss(reduction=\"sum\")\n",
        "\n",
        "    def _get_similarity_function(self, use_cosine_similarity):\n",
        "        if use_cosine_similarity:\n",
        "            self._cosine_similarity = torch.nn.CosineSimilarity(dim=-1)\n",
        "            return self._cosine_simililarity\n",
        "        else:\n",
        "            return self._dot_simililarity\n",
        "\n",
        "    def _get_correlated_mask(self):\n",
        "        diag = np.eye(2 * self.batch_size)\n",
        "        l1 = np.eye((2 * self.batch_size), 2 * self.batch_size, k=-self.batch_size)\n",
        "        l2 = np.eye((2 * self.batch_size), 2 * self.batch_size, k=self.batch_size)\n",
        "        mask = torch.from_numpy((diag + l1 + l2))\n",
        "        mask = (1 - mask).type(torch.bool)\n",
        "        return mask.cuda()\n",
        "\n",
        "    @staticmethod\n",
        "    def _dot_simililarity(x, y):\n",
        "        v = torch.tensordot(x.unsqueeze(1), y.T.unsqueeze(0), dims=2)\n",
        "        # x shape: (N, 1, C)\n",
        "        # y shape: (1, C, 2N)\n",
        "        # v shape: (N, 2N)\n",
        "        return v\n",
        "\n",
        "    def _cosine_simililarity(self, x, y):\n",
        "        # x shape: (N, 1, C)\n",
        "        # y shape: (1, 2N, C)\n",
        "        # v shape: (N, 2N)\n",
        "        v = self._cosine_similarity(x.unsqueeze(1), y.unsqueeze(0))\n",
        "        return v\n",
        "\n",
        "    def forward(self, zis, zjs):\n",
        "        representations = torch.cat([zjs, zis], dim=0)\n",
        "\n",
        "        similarity_matrix = self.similarity_function(representations, representations)\n",
        "\n",
        "        # filter out the scores from the positive samples\n",
        "        l_pos = torch.diag(similarity_matrix, self.batch_size)\n",
        "        r_pos = torch.diag(similarity_matrix, -self.batch_size)\n",
        "        positives = torch.cat([l_pos, r_pos]).view(2 * self.batch_size, 1)\n",
        "\n",
        "        negatives = similarity_matrix[self.mask_samples_from_same_repr].view(2 * self.batch_size, -1)\n",
        "\n",
        "        logits = torch.cat((positives, negatives), dim=1)\n",
        "        logits = logits / self.temperature\n",
        "\n",
        "        labels = torch.zeros(2 * self.batch_size).cuda().long()\n",
        "        loss = self.criterion(logits, labels)\n",
        "\n",
        "        return loss / (2 * self.batch_size)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-SGunpIuqcK",
        "colab_type": "text"
      },
      "source": [
        "### Step 4. Run pre-training step\n",
        "\n",
        "#### Implementation 4-1. Complete a basic SimCLR training loop\n",
        "\n",
        "https://github.com/ildoonet/pytorch-gradual-warmup-lr\n",
        "\n",
        "The linear warmup scheduler implementation is from a github in the link above\n",
        "\n",
        "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
        "\n",
        "Refer to this documentation to use lr schedulers integrated in PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlL2Yl5AuqcL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.optim.optimizer import Optimizer, required\n",
        "\n",
        "class SGD_with_lars(Optimizer):\n",
        "    r\"\"\"Implements stochastic gradient descent (optionally with momentum).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, lr=required, momentum=0, weight_decay=0, trust_coef=1.): # need to add trust coef\n",
        "        if lr is not required and lr < 0.0:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if momentum < 0.0:\n",
        "            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n",
        "        if weight_decay < 0.0:\n",
        "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
        "        if trust_coef < 0.0:\n",
        "            raise ValueError(\"Invalid trust_coef value: {}\".format(trust_coef))\n",
        "\n",
        "        defaults = dict(lr=lr, momentum=momentum, weight_decay=weight_decay, trust_coef=trust_coef)\n",
        "\n",
        "        super(SGD_with_lars, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(SGD_with_lars, self).__setstate__(state)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "        Arguments:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            weight_decay = group['weight_decay']\n",
        "            momentum = group['momentum']\n",
        "            trust_coef = group['trust_coef']\n",
        "            global_lr = group['lr']\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                d_p = p.grad.data\n",
        "\n",
        "                p_norm = torch.norm(p.data, p=2)\n",
        "                d_p_norm = torch.norm(d_p, p=2).add_(momentum, p_norm)\n",
        "                lr = torch.div(p_norm, d_p_norm).mul_(trust_coef)\n",
        "\n",
        "                lr.mul_(global_lr)\n",
        "\n",
        "                if weight_decay != 0:\n",
        "                    d_p.add_(weight_decay, p.data)\n",
        "\n",
        "                d_p.mul_(lr)\n",
        "\n",
        "                if momentum != 0:\n",
        "                    param_state = self.state[p]\n",
        "                    if 'momentum_buffer' not in param_state:\n",
        "                        buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
        "                    else:\n",
        "                        buf = param_state['momentum_buffer']\n",
        "                        buf.mul_(momentum).add_(d_p)\n",
        "                    d_p = buf\n",
        "\n",
        "                p.data.add_(-1, d_p)\n",
        "\n",
        "        return loss"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fjc3wkcuqcN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(net, loader):\n",
        "    \n",
        "    batch_size=256\n",
        "    temperature=0.07\n",
        "\n",
        "    loss_fn = NTXentLoss(batch_size=batch_size, temperature=temperature, use_cosine_similarity=True)\n",
        "    \n",
        "    ### IMPLEMENTATION 4-2 ###\n",
        "    ### 1. Use SGD_with_lars with\n",
        "    ### lr = 0.1 * batch_size / 256\n",
        "    ### momentum = 0.9\n",
        "    ### weight_decay = 1e-6\n",
        "    optimizer = SGD_with_lars(net.parameters(), lr = 0.1 * batch_size / 256, momentum=0.9, weight_decay=1e-6)\n",
        "    \n",
        "    from warmup_scheduler import GradualWarmupScheduler\n",
        "    ### 2. Use GradualWarmupScheduler with\n",
        "    ### multiplier = 1\n",
        "    ### total_epoch = 1/10 of total epochs\n",
        "    ### after_scheduler = optim.lr_scheduler.CosineAnnealingLR\n",
        "    scheduler = GradualWarmupScheduler(optimizer, multiplier=1, total_epoch=20, \n",
        "                                       after_scheduler=optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=200))\n",
        "    \n",
        "    train_start = time.time()\n",
        "    \n",
        "    loss_hist = []\n",
        "\n",
        "    for epoch in range(1, 200 + 1):\n",
        "        \n",
        "        train_loss = 0\n",
        "        net.train()\n",
        "        \n",
        "        epoch_start = time.time()\n",
        "        for idx, (data, target) in enumerate(loader):\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            ### 3. data variable contains two augmented images\n",
        "            ### -1. send them to your GPU by calling .cuda()\n",
        "            ### -2. forward each of them to net\n",
        "            ### -3. compute the InfoNCE loss\n",
        "            \n",
        "            # target : labels.\n",
        "\n",
        "            zi, zj = data\n",
        "            feat_i, emb_i, logit_i = net(zi.cuda())\n",
        "            feat_j, emb_j, logit_j = net(zj.cuda())\n",
        "            \n",
        "            loss = loss_fn(emb_i, emb_j)\n",
        "\n",
        "            ### IMPLEMENTATION ENDS HERE ###\n",
        "            \n",
        "            train_loss += loss.item()\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "        train_loss /= (idx + 1)\n",
        "        loss_hist.append(train_loss) # added by junwon\n",
        "        scheduler.step()\n",
        "        \n",
        "        epoch_time = time.time() - epoch_start\n",
        "        print(\"Epoch\\t\", epoch, \n",
        "              \"\\tLoss\\t\", train_loss, \n",
        "              \"\\tTime\\t\", epoch_time,\n",
        "             )\n",
        "        \n",
        "    elapsed_train_time = time.time() - train_start\n",
        "    print('Finished training. Train time was:', elapsed_train_time)\n",
        "\n",
        "    return loss_hist\n",
        "\n"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2Bqj--ZuqcR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8899020e-1ebd-4c58-9cf3-22adf1baccb4"
      },
      "source": [
        "GPU_NUM = '0'\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = GPU_NUM\n",
        "\n",
        "net = SimCLRNet(26, 1, 10, 32)\n",
        "\n",
        "net.cuda()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SimCLRNet(\n",
              "  (feat): ResNetCifar(\n",
              "    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (layer1): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (downsample): Downsample(\n",
              "          (avg): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "        )\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (downsample): Downsample(\n",
              "          (avg): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "        )\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
              "  )\n",
              "  (head): SimCLRHead(\n",
              "    (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
              "    (norm): Normalize()\n",
              "  )\n",
              "  (classifier): Linear(in_features=64, out_features=10, bias=True)\n",
              "  (norm): Normalize()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43yf5jCluqcT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8eb1ce45-29db-4873-deab-a5ea07d807da"
      },
      "source": [
        "net.zero_grad()\n",
        "loss_list = train(net, train_loader)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch\t 1 \tLoss\t 6.387218812795786 \tTime\t 59.926681995391846\n",
            "Epoch\t 2 \tLoss\t 4.00650057945496 \tTime\t 59.734951972961426\n",
            "Epoch\t 3 \tLoss\t 0.43090210939065005 \tTime\t 60.6079318523407\n",
            "Epoch\t 4 \tLoss\t 0.21036440176077378 \tTime\t 59.232654333114624\n",
            "Epoch\t 5 \tLoss\t 0.15465733645818172 \tTime\t 59.43998885154724\n",
            "Epoch\t 6 \tLoss\t 0.11985162607370278 \tTime\t 58.77029728889465\n",
            "Epoch\t 7 \tLoss\t 0.10051411462899966 \tTime\t 59.83280301094055\n",
            "Epoch\t 8 \tLoss\t 0.09566674647040856 \tTime\t 59.20227026939392\n",
            "Epoch\t 9 \tLoss\t 0.07896991160053474 \tTime\t 59.02400994300842\n",
            "Epoch\t 10 \tLoss\t 0.07341968061832281 \tTime\t 59.22950315475464\n",
            "Epoch\t 11 \tLoss\t 0.06937564372634276 \tTime\t 59.082884550094604\n",
            "Epoch\t 12 \tLoss\t 0.0660804576598681 \tTime\t 59.275195598602295\n",
            "Epoch\t 13 \tLoss\t 0.06699300347230373 \tTime\t 60.16993498802185\n",
            "Epoch\t 14 \tLoss\t 0.060417894703837544 \tTime\t 58.88829302787781\n",
            "Epoch\t 15 \tLoss\t 0.055829949103868924 \tTime\t 59.31361794471741\n",
            "Epoch\t 16 \tLoss\t 0.05496092559053348 \tTime\t 59.06645941734314\n",
            "Epoch\t 17 \tLoss\t 0.05313144327165224 \tTime\t 59.93861174583435\n",
            "Epoch\t 18 \tLoss\t 0.05164009169317209 \tTime\t 60.12755799293518\n",
            "Epoch\t 19 \tLoss\t 0.046341971174264564 \tTime\t 59.40132737159729\n",
            "Epoch\t 20 \tLoss\t 0.047587190253230244 \tTime\t 59.38704824447632\n",
            "Epoch\t 21 \tLoss\t 0.045669567661407666 \tTime\t 59.118754386901855\n",
            "Epoch\t 22 \tLoss\t 0.04429687515665323 \tTime\t 59.72051811218262\n",
            "Epoch\t 23 \tLoss\t 0.043707511726862346 \tTime\t 58.63462018966675\n",
            "Epoch\t 24 \tLoss\t 0.03715419270671331 \tTime\t 59.130205392837524\n",
            "Epoch\t 25 \tLoss\t 0.03734037114832646 \tTime\t 58.94318604469299\n",
            "Epoch\t 26 \tLoss\t 0.03609532680457983 \tTime\t 59.36958599090576\n",
            "Epoch\t 27 \tLoss\t 0.03971527837789976 \tTime\t 58.80853629112244\n",
            "Epoch\t 28 \tLoss\t 0.03454782021924471 \tTime\t 59.18925619125366\n",
            "Epoch\t 29 \tLoss\t 0.03600833905048859 \tTime\t 59.23969554901123\n",
            "Epoch\t 30 \tLoss\t 0.032539794431665006 \tTime\t 59.69344997406006\n",
            "Epoch\t 31 \tLoss\t 0.03376680135917969 \tTime\t 60.1886785030365\n",
            "Epoch\t 32 \tLoss\t 0.03432702102149144 \tTime\t 59.204556941986084\n",
            "Epoch\t 33 \tLoss\t 0.035576450519072704 \tTime\t 59.05180644989014\n",
            "Epoch\t 34 \tLoss\t 0.03178694187066494 \tTime\t 58.971264123916626\n",
            "Epoch\t 35 \tLoss\t 0.032143458561637465 \tTime\t 58.8121075630188\n",
            "Epoch\t 36 \tLoss\t 0.030064503953624994 \tTime\t 59.278637409210205\n",
            "Epoch\t 37 \tLoss\t 0.0323021148928465 \tTime\t 59.308330059051514\n",
            "Epoch\t 38 \tLoss\t 0.030534725684003953 \tTime\t 59.4169762134552\n",
            "Epoch\t 39 \tLoss\t 0.02928430825853959 \tTime\t 59.086103677749634\n",
            "Epoch\t 40 \tLoss\t 0.029752012657431454 \tTime\t 59.5794038772583\n",
            "Epoch\t 41 \tLoss\t 0.02786165799181431 \tTime\t 59.236072063446045\n",
            "Epoch\t 42 \tLoss\t 0.0271893293907245 \tTime\t 59.65101981163025\n",
            "Epoch\t 43 \tLoss\t 0.02769371934521657 \tTime\t 59.13779354095459\n",
            "Epoch\t 44 \tLoss\t 0.028309450355859904 \tTime\t 59.388805627822876\n",
            "Epoch\t 45 \tLoss\t 0.027507061864703128 \tTime\t 58.8238685131073\n",
            "Epoch\t 46 \tLoss\t 0.026815009074142346 \tTime\t 60.16486120223999\n",
            "Epoch\t 47 \tLoss\t 0.026907663725507566 \tTime\t 59.88854503631592\n",
            "Epoch\t 48 \tLoss\t 0.027650214798557452 \tTime\t 59.24545478820801\n",
            "Epoch\t 49 \tLoss\t 0.02459160757657045 \tTime\t 60.275110483169556\n",
            "Epoch\t 50 \tLoss\t 0.028011810588531007 \tTime\t 60.00285530090332\n",
            "Epoch\t 51 \tLoss\t 0.025600202429371003 \tTime\t 59.74456453323364\n",
            "Epoch\t 52 \tLoss\t 0.024965673068968148 \tTime\t 59.794682264328\n",
            "Epoch\t 53 \tLoss\t 0.025379484987411743 \tTime\t 58.86680746078491\n",
            "Epoch\t 54 \tLoss\t 0.023844268679236755 \tTime\t 58.54529333114624\n",
            "Epoch\t 55 \tLoss\t 0.023833818456683403 \tTime\t 59.882758140563965\n",
            "Epoch\t 56 \tLoss\t 0.02518282585228101 \tTime\t 59.32868027687073\n",
            "Epoch\t 57 \tLoss\t 0.02463909154996658 \tTime\t 59.90359878540039\n",
            "Epoch\t 58 \tLoss\t 0.02423344308940264 \tTime\t 59.187559366226196\n",
            "Epoch\t 59 \tLoss\t 0.02273343780483955 \tTime\t 59.38354682922363\n",
            "Epoch\t 60 \tLoss\t 0.02410773793951823 \tTime\t 59.55736494064331\n",
            "Epoch\t 61 \tLoss\t 0.02437969683550107 \tTime\t 59.75032877922058\n",
            "Epoch\t 62 \tLoss\t 0.022046295338525222 \tTime\t 58.94056558609009\n",
            "Epoch\t 63 \tLoss\t 0.022966653753358584 \tTime\t 59.75651550292969\n",
            "Epoch\t 64 \tLoss\t 0.02581517224988112 \tTime\t 59.89482641220093\n",
            "Epoch\t 65 \tLoss\t 0.02428281950071836 \tTime\t 59.973313331604004\n",
            "Epoch\t 66 \tLoss\t 0.022860662510188725 \tTime\t 59.11924767494202\n",
            "Epoch\t 67 \tLoss\t 0.023553975308552768 \tTime\t 58.86299753189087\n",
            "Epoch\t 68 \tLoss\t 0.024255780632106157 \tTime\t 59.31137442588806\n",
            "Epoch\t 69 \tLoss\t 0.022270634359656236 \tTime\t 59.18827772140503\n",
            "Epoch\t 70 \tLoss\t 0.024197866141987153 \tTime\t 59.412060022354126\n",
            "Epoch\t 71 \tLoss\t 0.02042298481728022 \tTime\t 59.488184690475464\n",
            "Epoch\t 72 \tLoss\t 0.02416608983125442 \tTime\t 59.32232451438904\n",
            "Epoch\t 73 \tLoss\t 0.022474980597885757 \tTime\t 58.12462258338928\n",
            "Epoch\t 74 \tLoss\t 0.02008513332081911 \tTime\t 58.28841519355774\n",
            "Epoch\t 75 \tLoss\t 0.02188588537944433 \tTime\t 58.576674461364746\n",
            "Epoch\t 76 \tLoss\t 0.02115576101992375 \tTime\t 58.7072696685791\n",
            "Epoch\t 77 \tLoss\t 0.02242897388835748 \tTime\t 58.02679371833801\n",
            "Epoch\t 78 \tLoss\t 0.021543879787891337 \tTime\t 58.17719125747681\n",
            "Epoch\t 79 \tLoss\t 0.023315846561812438 \tTime\t 57.79122519493103\n",
            "Epoch\t 80 \tLoss\t 0.020475521564292602 \tTime\t 57.977357625961304\n",
            "Epoch\t 81 \tLoss\t 0.02146212877944494 \tTime\t 58.60243773460388\n",
            "Epoch\t 82 \tLoss\t 0.021148583598625967 \tTime\t 58.47247123718262\n",
            "Epoch\t 83 \tLoss\t 0.021433515875385358 \tTime\t 57.89562940597534\n",
            "Epoch\t 84 \tLoss\t 0.020236072655862723 \tTime\t 57.9386670589447\n",
            "Epoch\t 85 \tLoss\t 0.019835216962756256 \tTime\t 58.679255962371826\n",
            "Epoch\t 86 \tLoss\t 0.019924783329359996 \tTime\t 58.81764316558838\n",
            "Epoch\t 87 \tLoss\t 0.018335430252437408 \tTime\t 58.721471071243286\n",
            "Epoch\t 88 \tLoss\t 0.02100465687421652 \tTime\t 58.27299904823303\n",
            "Epoch\t 89 \tLoss\t 0.02007595444910037 \tTime\t 58.91067409515381\n",
            "Epoch\t 90 \tLoss\t 0.02111293331075173 \tTime\t 58.518746852874756\n",
            "Epoch\t 91 \tLoss\t 0.02094174850350007 \tTime\t 58.747156381607056\n",
            "Epoch\t 92 \tLoss\t 0.019452007305927766 \tTime\t 59.67953562736511\n",
            "Epoch\t 93 \tLoss\t 0.019686221445982272 \tTime\t 59.92636704444885\n",
            "Epoch\t 94 \tLoss\t 0.02026103599808919 \tTime\t 60.14932060241699\n",
            "Epoch\t 95 \tLoss\t 0.018740769618978866 \tTime\t 59.492358446121216\n",
            "Epoch\t 96 \tLoss\t 0.020416677127090783 \tTime\t 59.146751403808594\n",
            "Epoch\t 97 \tLoss\t 0.020069363837440807 \tTime\t 59.774075508117676\n",
            "Epoch\t 98 \tLoss\t 0.018722078543251905 \tTime\t 59.91093397140503\n",
            "Epoch\t 99 \tLoss\t 0.020350151021893206 \tTime\t 58.967289447784424\n",
            "Epoch\t 100 \tLoss\t 0.019706593525524323 \tTime\t 59.313878536224365\n",
            "Epoch\t 101 \tLoss\t 0.01947982962219379 \tTime\t 60.35904264450073\n",
            "Epoch\t 102 \tLoss\t 0.018940333487131658 \tTime\t 59.13351583480835\n",
            "Epoch\t 103 \tLoss\t 0.01916653478088287 \tTime\t 60.7879843711853\n",
            "Epoch\t 104 \tLoss\t 0.020612069429495394 \tTime\t 60.790849685668945\n",
            "Epoch\t 105 \tLoss\t 0.019966161174651904 \tTime\t 58.54816555976868\n",
            "Epoch\t 106 \tLoss\t 0.019695633912506776 \tTime\t 59.44976854324341\n",
            "Epoch\t 107 \tLoss\t 0.02016334667419776 \tTime\t 60.2799928188324\n",
            "Epoch\t 108 \tLoss\t 0.018612414077879526 \tTime\t 59.841195583343506\n",
            "Epoch\t 109 \tLoss\t 0.01896234175715691 \tTime\t 58.71305823326111\n",
            "Epoch\t 110 \tLoss\t 0.020097542525484013 \tTime\t 59.232237577438354\n",
            "Epoch\t 111 \tLoss\t 0.019934855473156158 \tTime\t 59.524982929229736\n",
            "Epoch\t 112 \tLoss\t 0.019170417192463693 \tTime\t 60.5257613658905\n",
            "Epoch\t 113 \tLoss\t 0.0186444219488364 \tTime\t 58.67906379699707\n",
            "Epoch\t 114 \tLoss\t 0.01796732619404793 \tTime\t 59.84099102020264\n",
            "Epoch\t 115 \tLoss\t 0.01770737714205797 \tTime\t 59.87950325012207\n",
            "Epoch\t 116 \tLoss\t 0.019497801382572223 \tTime\t 59.00599026679993\n",
            "Epoch\t 117 \tLoss\t 0.019844370225492198 \tTime\t 57.90461993217468\n",
            "Epoch\t 118 \tLoss\t 0.01831561555274022 \tTime\t 58.9094352722168\n",
            "Epoch\t 119 \tLoss\t 0.01881537542033654 \tTime\t 58.44181180000305\n",
            "Epoch\t 120 \tLoss\t 0.018991136097182067 \tTime\t 59.559677839279175\n",
            "Epoch\t 121 \tLoss\t 0.017683029910310722 \tTime\t 58.506657123565674\n",
            "Epoch\t 122 \tLoss\t 0.018423103631880038 \tTime\t 60.36368775367737\n",
            "Epoch\t 123 \tLoss\t 0.017372707401712737 \tTime\t 60.49052810668945\n",
            "Epoch\t 124 \tLoss\t 0.01754352503384535 \tTime\t 60.3541796207428\n",
            "Epoch\t 125 \tLoss\t 0.01771533450541588 \tTime\t 59.71996188163757\n",
            "Epoch\t 126 \tLoss\t 0.01727353683553445 \tTime\t 61.341580390930176\n",
            "Epoch\t 127 \tLoss\t 0.01742310450436213 \tTime\t 60.2730278968811\n",
            "Epoch\t 128 \tLoss\t 0.01686843507564985 \tTime\t 60.46474504470825\n",
            "Epoch\t 129 \tLoss\t 0.01709046297921584 \tTime\t 61.7332546710968\n",
            "Epoch\t 130 \tLoss\t 0.017698118859567704 \tTime\t 61.06983304023743\n",
            "Epoch\t 131 \tLoss\t 0.017633916351657647 \tTime\t 60.62458157539368\n",
            "Epoch\t 132 \tLoss\t 0.017822456469711586 \tTime\t 61.418290853500366\n",
            "Epoch\t 133 \tLoss\t 0.018388688368483997 \tTime\t 61.38297510147095\n",
            "Epoch\t 134 \tLoss\t 0.017535505615747893 \tTime\t 62.43956923484802\n",
            "Epoch\t 135 \tLoss\t 0.01750108177940815 \tTime\t 62.04556608200073\n",
            "Epoch\t 136 \tLoss\t 0.016729869707845724 \tTime\t 62.230379581451416\n",
            "Epoch\t 137 \tLoss\t 0.017604281910910055 \tTime\t 61.230093479156494\n",
            "Epoch\t 138 \tLoss\t 0.01746373100636097 \tTime\t 62.34316039085388\n",
            "Epoch\t 139 \tLoss\t 0.018616906080681544 \tTime\t 62.28102707862854\n",
            "Epoch\t 140 \tLoss\t 0.01776285563619473 \tTime\t 61.95289707183838\n",
            "Epoch\t 141 \tLoss\t 0.017075707228519976 \tTime\t 62.281121492385864\n",
            "Epoch\t 142 \tLoss\t 0.01810326237613574 \tTime\t 63.92170333862305\n",
            "Epoch\t 143 \tLoss\t 0.017489642387208265 \tTime\t 62.897522926330566\n",
            "Epoch\t 144 \tLoss\t 0.01599527688171619 \tTime\t 63.657938718795776\n",
            "Epoch\t 145 \tLoss\t 0.018157094635833534 \tTime\t 62.266053915023804\n",
            "Epoch\t 146 \tLoss\t 0.016109455434175638 \tTime\t 63.21452713012695\n",
            "Epoch\t 147 \tLoss\t 0.01651725987306772 \tTime\t 62.8464035987854\n",
            "Epoch\t 148 \tLoss\t 0.0161426164687444 \tTime\t 63.02459502220154\n",
            "Epoch\t 149 \tLoss\t 0.017153779765925345 \tTime\t 62.900848388671875\n",
            "Epoch\t 150 \tLoss\t 0.017133140583068897 \tTime\t 63.536789894104004\n",
            "Epoch\t 151 \tLoss\t 0.017818103563518095 \tTime\t 63.406025409698486\n",
            "Epoch\t 152 \tLoss\t 0.016242856732927836 \tTime\t 63.77348875999451\n",
            "Epoch\t 153 \tLoss\t 0.017646851863425513 \tTime\t 63.763036489486694\n",
            "Epoch\t 154 \tLoss\t 0.016680593124757975 \tTime\t 63.62480425834656\n",
            "Epoch\t 155 \tLoss\t 0.01698571001776518 \tTime\t 63.146167039871216\n",
            "Epoch\t 156 \tLoss\t 0.017516075385113558 \tTime\t 62.99050450325012\n",
            "Epoch\t 157 \tLoss\t 0.017058926548522254 \tTime\t 62.68651533126831\n",
            "Epoch\t 158 \tLoss\t 0.017546323605645925 \tTime\t 62.214258432388306\n",
            "Epoch\t 159 \tLoss\t 0.016602335263712284 \tTime\t 61.80123496055603\n",
            "Epoch\t 160 \tLoss\t 0.016624036899361854 \tTime\t 62.21568489074707\n",
            "Epoch\t 161 \tLoss\t 0.0159787638972585 \tTime\t 63.25231099128723\n",
            "Epoch\t 162 \tLoss\t 0.016781327223930602 \tTime\t 62.52939200401306\n",
            "Epoch\t 163 \tLoss\t 0.017175405520277146 \tTime\t 62.73543930053711\n",
            "Epoch\t 164 \tLoss\t 0.015360951834382155 \tTime\t 63.38215732574463\n",
            "Epoch\t 165 \tLoss\t 0.016890912565092245 \tTime\t 62.157614946365356\n",
            "Epoch\t 166 \tLoss\t 0.016158066011774233 \tTime\t 62.263081789016724\n",
            "Epoch\t 167 \tLoss\t 0.01642318523178498 \tTime\t 61.82653307914734\n",
            "Epoch\t 168 \tLoss\t 0.015957348215847444 \tTime\t 62.72541427612305\n",
            "Epoch\t 169 \tLoss\t 0.016419446162688426 \tTime\t 61.7754225730896\n",
            "Epoch\t 170 \tLoss\t 0.017072448836496244 \tTime\t 61.975966691970825\n",
            "Epoch\t 171 \tLoss\t 0.017525917802674647 \tTime\t 62.33259725570679\n",
            "Epoch\t 172 \tLoss\t 0.01574812339953123 \tTime\t 62.01090860366821\n",
            "Epoch\t 173 \tLoss\t 0.016404099370806644 \tTime\t 62.67270493507385\n",
            "Epoch\t 174 \tLoss\t 0.017547991031255476 \tTime\t 63.81123995780945\n",
            "Epoch\t 175 \tLoss\t 0.01620791638508821 \tTime\t 63.528780937194824\n",
            "Epoch\t 176 \tLoss\t 0.016908167474544964 \tTime\t 63.4912965297699\n",
            "Epoch\t 177 \tLoss\t 0.016607936242452034 \tTime\t 63.22781538963318\n",
            "Epoch\t 178 \tLoss\t 0.014969194093002723 \tTime\t 62.11222577095032\n",
            "Epoch\t 179 \tLoss\t 0.014893363545147273 \tTime\t 62.06160831451416\n",
            "Epoch\t 180 \tLoss\t 0.016014871335564517 \tTime\t 62.02453827857971\n",
            "Epoch\t 181 \tLoss\t 0.0161997770699553 \tTime\t 63.04101586341858\n",
            "Epoch\t 182 \tLoss\t 0.015222676045810565 \tTime\t 63.30871629714966\n",
            "Epoch\t 183 \tLoss\t 0.015274643687865673 \tTime\t 64.33305287361145\n",
            "Epoch\t 184 \tLoss\t 0.016316500272697362 \tTime\t 64.56066918373108\n",
            "Epoch\t 185 \tLoss\t 0.015869901907176544 \tTime\t 63.22943162918091\n",
            "Epoch\t 186 \tLoss\t 0.015420378300432976 \tTime\t 63.92475199699402\n",
            "Epoch\t 187 \tLoss\t 0.015383790920560176 \tTime\t 62.83879804611206\n",
            "Epoch\t 188 \tLoss\t 0.016150877853998772 \tTime\t 63.555813789367676\n",
            "Epoch\t 189 \tLoss\t 0.01578040503634092 \tTime\t 63.46297001838684\n",
            "Epoch\t 190 \tLoss\t 0.015165198302994936 \tTime\t 63.406895875930786\n",
            "Epoch\t 191 \tLoss\t 0.01610134360499871 \tTime\t 63.02466678619385\n",
            "Epoch\t 192 \tLoss\t 0.015475265657863555 \tTime\t 63.69407081604004\n",
            "Epoch\t 193 \tLoss\t 0.015077593736350536 \tTime\t 64.05852389335632\n",
            "Epoch\t 194 \tLoss\t 0.01584347183219133 \tTime\t 63.698046922683716\n",
            "Epoch\t 195 \tLoss\t 0.0156523814997994 \tTime\t 64.20742750167847\n",
            "Epoch\t 196 \tLoss\t 0.015381882024499086 \tTime\t 64.1802864074707\n",
            "Epoch\t 197 \tLoss\t 0.014469846199529294 \tTime\t 64.34305953979492\n",
            "Epoch\t 198 \tLoss\t 0.015912859304211078 \tTime\t 64.27108979225159\n",
            "Epoch\t 199 \tLoss\t 0.015838554640037892 \tTime\t 63.81822991371155\n",
            "Epoch\t 200 \tLoss\t 0.015615473090647123 \tTime\t 64.25711798667908\n",
            "Finished training. Train time was: 12122.183138608932\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8TTC-0U-Xkt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_loss(loss_hist, xlabel='Iteration number', ylabel='Loss value') :\n",
        "  plt.plot(loss_hist)\n",
        "  plt.xlabel(xlabel)\n",
        "  plt.ylabel(ylabel)\n",
        "  plt.show()"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVo5Msnl--6B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "033390d7-2cac-49ec-979a-6305d33df160"
      },
      "source": [
        "plot_loss(loss_list)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-d40d53ab0e66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'loss_list' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ufE49N8BcxA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save ckpt in google drive.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!ls /content/gdrive/My Drive\n",
        "model_save_name = 'simCLR.pt'\n",
        "path = F\"/content/gdrive/My Drive/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CPurz9SuqcV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " torch.save(model.state_dict(),\"path?\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDATNL_YuUIP",
        "colab_type": "text"
      },
      "source": [
        "## Linear Evaluation Protocol\n",
        "\n",
        "- train Linear classifier with freezed extractor f"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocucN1fyuTZU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pek2bM80tkes",
        "colab_type": "text"
      },
      "source": [
        "##Train Function with Parameters.\n",
        "\n",
        "- save the model with lowest loss\n",
        "- parameters : batch_size, temparture, pre-training epochs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ak1IDc6gtpqW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_param(net, loader):\n",
        "    \n",
        "    batch_size=256\n",
        "    temperature=0.07\n",
        "\n",
        "    loss_fn = NTXentLoss(batch_size=batch_size, temperature=temperature, use_cosine_similarity=True)\n",
        "    \n",
        "    ### IMPLEMENTATION 4-2 ###\n",
        "    ### 1. Use SGD_with_lars with\n",
        "    ### lr = 0.1 * batch_size / 256\n",
        "    ### momentum = 0.9\n",
        "    ### weight_decay = 1e-6\n",
        "    optimizer = SGD_with_lars(net.parameters(), lr = 0.1 * batch_size / 256, momentum=0.9, weight_decay=1e-6)\n",
        "    \n",
        "    from warmup_scheduler import GradualWarmupScheduler\n",
        "    ### 2. Use GradualWarmupScheduler with\n",
        "    ### multiplier = 1\n",
        "    ### total_epoch = 1/10 of total epochs\n",
        "    ### after_scheduler = optim.lr_scheduler.CosineAnnealingLR\n",
        "    scheduler = GradualWarmupScheduler(optimizer, multiplier=1, total_epoch=20, \n",
        "                                       after_scheduler=optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=200))\n",
        "    \n",
        "    train_start = time.time()\n",
        "    \n",
        "    loss_hist = []\n",
        "\n",
        "    for epoch in range(1, 200 + 1):\n",
        "        \n",
        "        train_loss = 0\n",
        "        net.train()\n",
        "        \n",
        "        epoch_start = time.time()\n",
        "        for idx, (data, target) in enumerate(loader):\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            ### 3. data variable contains two augmented images\n",
        "            ### -1. send them to your GPU by calling .cuda()\n",
        "            ### -2. forward each of them to net\n",
        "            ### -3. compute the InfoNCE loss\n",
        "            \n",
        "            # target : labels.\n",
        "\n",
        "            zi, zj = data\n",
        "            feat_i, emb_i, logit_i = net(zi.cuda())\n",
        "            feat_j, emb_j, logit_j = net(zj.cuda())\n",
        "            \n",
        "            loss = loss_fn(emb_i, emb_j)\n",
        "\n",
        "            ### IMPLEMENTATION ENDS HERE ###\n",
        "            \n",
        "            train_loss += loss.item()\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "        train_loss /= (idx + 1)\n",
        "        loss_hist.append(train_loss) # added by junwon\n",
        "        scheduler.step()\n",
        "        \n",
        "        epoch_time = time.time() - epoch_start\n",
        "        print(\"Epoch\\t\", epoch, \n",
        "              \"\\tLoss\\t\", train_loss, \n",
        "              \"\\tTime\\t\", epoch_time,\n",
        "             )\n",
        "        \n",
        "    elapsed_train_time = time.time() - train_start\n",
        "    print('Finished training. Train time was:', elapsed_train_time)\n",
        "\n",
        "    return loss_hist"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}