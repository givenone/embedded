{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"colab":{"name":"Emb_SimCLR_Practice.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"f7fea33aacc64e1dbdb86c6a6e0b0f78":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_651f158def4643d3b5ccc41af6cb003e","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_a7c05ad3822b444fa37bd4e7ce6c996d","IPY_MODEL_6d9c8e5940a543c8b9983a821de20ebb"]}},"651f158def4643d3b5ccc41af6cb003e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a7c05ad3822b444fa37bd4e7ce6c996d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_88c3403693ac4f9ba5ce775d3d0183d0","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"info","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f42ee3aa40a04723b1b56c616ccf45d4"}},"6d9c8e5940a543c8b9983a821de20ebb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_221a3f36ed154be4bc069fd1f6fdc243","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 170500096/? [00:20&lt;00:00, 99787425.05it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1fc76fe65e02418cb2716dc89fdf8cce"}},"88c3403693ac4f9ba5ce775d3d0183d0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"f42ee3aa40a04723b1b56c616ccf45d4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"221a3f36ed154be4bc069fd1f6fdc243":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"1fc76fe65e02418cb2716dc89fdf8cce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"pIbUUOphvNRR","executionInfo":{"status":"ok","timestamp":1601137080146,"user_tz":-540,"elapsed":4882,"user":{"displayName":"서준원","photoUrl":"","userId":"09602630791249428793"}},"outputId":"89cc23e7-727a-47f0-8ea9-c7e0d6e50a34","colab":{"base_uri":"https://localhost:8080/","height":195}},"source":["  pip install git+https://github.com/ildoonet/pytorch-gradual-warmup-lr.git"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting git+https://github.com/ildoonet/pytorch-gradual-warmup-lr.git\n","  Cloning https://github.com/ildoonet/pytorch-gradual-warmup-lr.git to /tmp/pip-req-build-st8no5xl\n","  Running command git clone -q https://github.com/ildoonet/pytorch-gradual-warmup-lr.git /tmp/pip-req-build-st8no5xl\n","Building wheels for collected packages: warmup-scheduler\n","  Building wheel for warmup-scheduler (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for warmup-scheduler: filename=warmup_scheduler-0.3.2-cp36-none-any.whl size=3881 sha256=7a31562d8968c9bf58b2b999e607c01a87d44fcae0ba4a0ab1b9ffdb4cd9c4db\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-j959nyms/wheels/b7/24/83/d30234cc013cff538805b14df916e79091f7cf9ee2c5bf3a64\n","Successfully built warmup-scheduler\n","Installing collected packages: warmup-scheduler\n","Successfully installed warmup-scheduler-0.3.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DYtKgry8uqb3","executionInfo":{"status":"ok","timestamp":1601137088117,"user_tz":-540,"elapsed":4183,"user":{"displayName":"서준원","photoUrl":"","userId":"09602630791249428793"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","import torchvision.datasets as datasets\n","\n","import numpy as np\n","\n","import os\n","\n","import time"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iZ4WM8_Guqb6"},"source":["### Step 1. Construct a CNN model\n","\n","#### Implementation 1-1. Design SimCLRHead class\n","\n","#### Implementation 1-2. Design SimCLRNet class"]},{"cell_type":"code","metadata":{"id":"DvfdvwkQuqb6","executionInfo":{"status":"ok","timestamp":1601137088956,"user_tz":-540,"elapsed":3323,"user":{"displayName":"서준원","photoUrl":"","userId":"09602630791249428793"}}},"source":["import math\n","from torchvision.models.resnet import conv3x3\n","\n","class BasicBlock(nn.Module):\n","    def __init__(self, inplanes, planes, norm_layer, stride=1, downsample=None):\n","        super(BasicBlock, self).__init__()\n","        self.downsample = downsample\n","        self.stride = stride\n","        \n","        self.bn1 = norm_layer(inplanes)\n","        self.relu1 = nn.ReLU(inplace=True)\n","        self.conv1 = conv3x3(inplanes, planes, stride)\n","        \n","        self.bn2 = norm_layer(planes)\n","        self.relu2 = nn.ReLU(inplace=True)\n","        self.conv2 = conv3x3(planes, planes)\n","\n","    def forward(self, x):\n","        residual = x \n","        residual = self.bn1(residual)\n","        residual = self.relu1(residual)\n","        residual = self.conv1(residual)\n","\n","        residual = self.bn2(residual)\n","        residual = self.relu2(residual)\n","        residual = self.conv2(residual)\n","\n","        if self.downsample is not None:\n","            x = self.downsample(x)\n","        return x + residual\n","\n","class Downsample(nn.Module):\n","    def __init__(self, nIn, nOut, stride):\n","        super(Downsample, self).__init__()\n","        self.avg = nn.AvgPool2d(stride)\n","        assert nOut % nIn == 0\n","        self.expand_ratio = nOut // nIn\n","\n","    def forward(self, x):\n","        x = self.avg(x)\n","        return torch.cat([x] + [x.mul(0)] * (self.expand_ratio - 1), 1)\n","\n","class ResNetCifar(nn.Module):\n","    def __init__(self, depth, width=1, classes=10, channels=3, norm_layer=nn.BatchNorm2d):\n","        assert (depth - 2) % 6 == 0         # depth is 6N+2\n","        self.N = (depth - 2) // 6\n","        super(ResNetCifar, self).__init__()\n","\n","        # Following the Wide ResNet convention, we fix the very first convolution\n","        self.conv1 = nn.Conv2d(channels, 16, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.inplanes = 16\n","        self.layer1 = self._make_layer(norm_layer, 16 * width)\n","        self.layer2 = self._make_layer(norm_layer, 32 * width, stride=2)\n","        self.layer3 = self._make_layer(norm_layer, 64 * width, stride=2)\n","        self.bn = norm_layer(64 * width)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.avgpool = nn.AvgPool2d(8)\n","\n","        # Initialization\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","                m.weight.data.normal_(0, math.sqrt(2. / n))\n","                \n","    def _make_layer(self, norm_layer, planes, stride=1):\n","        downsample = None\n","        if stride != 1 or self.inplanes != planes:\n","            downsample = Downsample(self.inplanes, planes, stride)\n","        layers = [BasicBlock(self.inplanes, planes, norm_layer, stride, downsample)]\n","        self.inplanes = planes\n","        for i in range(self.N - 1):\n","            layers.append(BasicBlock(self.inplanes, planes, norm_layer))\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.bn(x)\n","        x = self.relu(x)\n","        x = self.avgpool(x)\n","        x = x.view(x.size(0), -1)\n","        return x\n","\n","    \n","class Normalize(nn.Module):\n","\n","    def __init__(self, power=2):\n","        super(Normalize, self).__init__()\n","        self.power = power\n","\n","    def forward(self, x):\n","        norm = x.pow(self.power).sum(1, keepdim=True).pow(1. / self.power)\n","        out = x.div(norm)\n","        return out\n","    \n","\n","class SimCLRHead(nn.Module):\n","    def __init__(self, width, emb_dim):\n","        super(SimCLRHead, self).__init__()\n","        \n","        ### IMPLEMENTATION 1-1 ###\n","        ### 1. Linear layer (64 * width -> 64 * width)\n","        ### 2. ReLU\n","        ### 3. Linear layer (64 * width -> emb_dim)\n","        ### 4. Normalization layer (Normalize module above)\n","        self.fc1 = nn.Linear(64 * width, 64 * width)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.fc2 = nn.Linear(64 * width, emb_dim)\n","        self.norm = Normalize()\n","        ### IMPLEMENTATION ENDS HERE ###\n","        \n","    def forward(self, x):\n","        \n","        ### IMPLEMENTATION 1-1 ###\n","        ### Design a proper forward function\n","        \n","        x = self.fc1(x)\n","        x = self.relu(x)\n","        x = self.fc2(x)\n","        x = self.norm(x)\n","\n","        ### IMPLEMENTATION ENDS HERE ###\n","        return x\n","    \n","\n","class SimCLRNet(nn.Module):\n","    def __init__(self, depth, width=1, num_classes=10, emb_dim=32):\n","        super(SimCLRNet, self).__init__()\n","        \n","        self.num_classes = num_classes\n","        \n","        self.feat = ResNetCifar(depth=depth, width=width, classes=num_classes)\n","        \n","        ### IMPLEMENTATION 1-2 ###\n","        ### 1. A projection head (SimCLRHead module above)\n","        self.head = SimCLRHead(width, emb_dim)\n","        \n","        ### 2. A linear classifier (64 * width -> num_classes)\n","        self.classifier = nn.Linear(64 * width, num_classes)\n","        \n","        ### 3. Normalization layer for conv feature normalization (Normalize module above)\n","        self.norm = Normalize()\n","        \n","        ### IMPLEMENTATION ENDS HERE ###\n","    \n","    def forward(self, x, norm_feat=False):\n","        \n","        ### IMPLEMENTATION 1-2 ###\n","        ### Your module must return\n","        ### 1. Conv feature (feat) - when norm_feat is true, apply L2 normalization\n","        ### 2. Projected embedding (emb)\n","        ### 3. Logit vector by the linear classifier (logit)\n","        \n","        feat = self.feat(x)\n","        if norm_feat :\n","          feat = self.norm(feat)\n","        \n","        emb = self.head(feat)\n","        logit = self.classifier(feat)\n","\n","        ### IMPLEMENTATION ENDS HERE ###\n","        return feat, emb, logit"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OvxUSFm-uqb9"},"source":["### Step 2. Prepare datasets & data augmentations\n","\n","For contrastaive learning, a set of random augmentation functions is first defined.\n","\n","Then, the set is applied twice to each image, which is implemented as provided DuplicatedCompose module\n","\n","#### Implementation 2-1. Design a train transform (train_transform)\n","\n","Follow the instruction inside the train_transform\n","\n","https://pytorch.org/docs/stable/torchvision/transforms.html\n","\n","Refer to the torchvision.transforms documentation"]},{"cell_type":"code","metadata":{"id":"S53vsB4Iuqb9","executionInfo":{"status":"ok","timestamp":1601137092761,"user_tz":-540,"elapsed":749,"user":{"displayName":"서준원","photoUrl":"","userId":"09602630791249428793"}}},"source":["class DuplicatedCompose(object):\n","    def __init__(self, transforms):\n","        self.transforms = transforms\n","\n","    def __call__(self, img):\n","        img1 = img.copy()\n","        img2 = img.copy()\n","        for t in self.transforms:\n","            img1 = t(img1)\n","            img2 = t(img2)\n","        return img1, img2"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"ex83YAnMuqcA","executionInfo":{"status":"ok","timestamp":1601137094655,"user_tz":-540,"elapsed":1639,"user":{"displayName":"서준원","photoUrl":"","userId":"09602630791249428793"}}},"source":["import cv2\n","cv2.setNumThreads(0)\n","\n","class GaussianBlur(object):\n","    # Implements Gaussian blur as described in the SimCLR paper\n","    def __init__(self, kernel_size, min=0.1, max=2.0):\n","        self.min = min\n","        self.max = max\n","        # kernel size is set to be 10% of the image height/width\n","        self.kernel_size = kernel_size\n","        \n","        if self.kernel_size % 2 == 0:\n","            self.kernel_size += 1\n","\n","    def __call__(self, sample):\n","        sample = np.array(sample)\n","\n","        # blur the image with a 50% chance\n","        prob = np.random.random_sample()\n","\n","        if prob < 0.5:\n","            sigma = (self.max - self.min) * np.random.random_sample() + self.min\n","            sample = cv2.GaussianBlur(sample, (self.kernel_size, self.kernel_size), sigma)\n","\n","        return sample"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"8xHmdyuguqcC","executionInfo":{"status":"ok","timestamp":1601137095371,"user_tz":-540,"elapsed":434,"user":{"displayName":"서준원","photoUrl":"","userId":"09602630791249428793"}}},"source":["import torchvision.transforms as transforms\n","\n","img_size = (32, 32)\n","\n","color_jitter = transforms.ColorJitter(0.8, 0.8, 0.8, 0.2)\n","\n","train_transform = DuplicatedCompose([\n","    ### IMPLEMENTATION 2-1 ###\n","    ### 1. Random resized crop w/ final size of (32, 32)\n","    ### 2. Random horizontal flip w/ p=0.5\n","    ### 3. Randomly apply the pre-defined color jittering w/ p=0.8\n","    ### 4. Random gray scale w/ p=0.2\n","    ### 5. Gaussian blur w/ kernel size of 1/10 of the image width or height (32)\n","    transforms.RandomResizedCrop(img_size),\n","    transforms.RandomHorizontalFlip(p=0.5),\n","    color_jitter,\n","    transforms.RandomGrayscale(p=0.2),\n","    GaussianBlur(img_size[0]//10),\n","    \n","    ### IMPLEMENTATION ENDS HERE ###\n","    transforms.ToTensor(),\n","])"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zo9Xj3KruqcE","executionInfo":{"status":"ok","timestamp":1601137100989,"user_tz":-540,"elapsed":4826,"user":{"displayName":"서준원","photoUrl":"","userId":"09602630791249428793"}},"outputId":"6260236d-91e4-4946-da8e-0f8b266e63a5","colab":{"base_uri":"https://localhost:8080/","height":84,"referenced_widgets":["f7fea33aacc64e1dbdb86c6a6e0b0f78","651f158def4643d3b5ccc41af6cb003e","a7c05ad3822b444fa37bd4e7ce6c996d","6d9c8e5940a543c8b9983a821de20ebb","88c3403693ac4f9ba5ce775d3d0183d0","f42ee3aa40a04723b1b56c616ccf45d4","221a3f36ed154be4bc069fd1f6fdc243","1fc76fe65e02418cb2716dc89fdf8cce"]}},"source":["from torch.utils.data import DataLoader\n","\n","train_dataset = datasets.CIFAR10(root='.',\n","                                 train=True,\n","                                 download=True,\n","                                 transform=train_transform\n","                                )\n","\n","train_loader = DataLoader(train_dataset,\n","                          batch_size=256,\n","                          num_workers=4,\n","                          shuffle=True,\n","                          drop_last=True\n","                         )"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f7fea33aacc64e1dbdb86c6a6e0b0f78","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Extracting ./cifar-10-python.tar.gz to .\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6xwkINzBuqcH"},"source":["### Step 3. Implement InfoNCE loss"]},{"cell_type":"code","metadata":{"id":"MUjxzqf6uqcI","executionInfo":{"status":"ok","timestamp":1601137100993,"user_tz":-540,"elapsed":3032,"user":{"displayName":"서준원","photoUrl":"","userId":"09602630791249428793"}}},"source":["class NTXentLoss(torch.nn.Module):\n","\n","    def __init__(self, batch_size, temperature, use_cosine_similarity):\n","        super(NTXentLoss, self).__init__()\n","        self.batch_size = batch_size\n","        self.temperature = temperature\n","        self.softmax = torch.nn.Softmax(dim=-1)\n","        self.mask_samples_from_same_repr = self._get_correlated_mask().type(torch.bool)\n","        self.similarity_function = self._get_similarity_function(use_cosine_similarity)\n","        self.criterion = torch.nn.CrossEntropyLoss(reduction=\"sum\")\n","\n","    def _get_similarity_function(self, use_cosine_similarity):\n","        if use_cosine_similarity:\n","            self._cosine_similarity = torch.nn.CosineSimilarity(dim=-1)\n","            return self._cosine_simililarity\n","        else:\n","            return self._dot_simililarity\n","\n","    def _get_correlated_mask(self):\n","        diag = np.eye(2 * self.batch_size)\n","        l1 = np.eye((2 * self.batch_size), 2 * self.batch_size, k=-self.batch_size)\n","        l2 = np.eye((2 * self.batch_size), 2 * self.batch_size, k=self.batch_size)\n","        mask = torch.from_numpy((diag + l1 + l2))\n","        mask = (1 - mask).type(torch.bool)\n","        return mask.cuda()\n","\n","    @staticmethod\n","    def _dot_simililarity(x, y):\n","        v = torch.tensordot(x.unsqueeze(1), y.T.unsqueeze(0), dims=2)\n","        # x shape: (N, 1, C)\n","        # y shape: (1, C, 2N)\n","        # v shape: (N, 2N)\n","        return v\n","\n","    def _cosine_simililarity(self, x, y):\n","        # x shape: (N, 1, C)\n","        # y shape: (1, 2N, C)\n","        # v shape: (N, 2N)\n","        v = self._cosine_similarity(x.unsqueeze(1), y.unsqueeze(0))\n","        return v\n","\n","    def forward(self, zis, zjs):\n","        representations = torch.cat([zjs, zis], dim=0)\n","\n","        similarity_matrix = self.similarity_function(representations, representations)\n","\n","        # filter out the scores from the positive samples\n","        l_pos = torch.diag(similarity_matrix, self.batch_size)\n","        r_pos = torch.diag(similarity_matrix, -self.batch_size)\n","        positives = torch.cat([l_pos, r_pos]).view(2 * self.batch_size, 1)\n","\n","        negatives = similarity_matrix[self.mask_samples_from_same_repr].view(2 * self.batch_size, -1)\n","\n","        logits = torch.cat((positives, negatives), dim=1)\n","        logits = logits / self.temperature\n","\n","        labels = torch.zeros(2 * self.batch_size).cuda().long()\n","        loss = self.criterion(logits, labels)\n","\n","        return loss / (2 * self.batch_size)"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u-SGunpIuqcK"},"source":["### Step 4. Run pre-training step\n","\n","#### Implementation 4-1. Complete a basic SimCLR training loop\n","\n","https://github.com/ildoonet/pytorch-gradual-warmup-lr\n","\n","The linear warmup scheduler implementation is from a github in the link above\n","\n","https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","\n","Refer to this documentation to use lr schedulers integrated in PyTorch"]},{"cell_type":"code","metadata":{"id":"UlL2Yl5AuqcL","executionInfo":{"status":"ok","timestamp":1601137101874,"user_tz":-540,"elapsed":590,"user":{"displayName":"서준원","photoUrl":"","userId":"09602630791249428793"}}},"source":["from torch.optim.optimizer import Optimizer, required\n","\n","class SGD_with_lars(Optimizer):\n","    r\"\"\"Implements stochastic gradient descent (optionally with momentum).\n","    \"\"\"\n","\n","    def __init__(self, params, lr=required, momentum=0, weight_decay=0, trust_coef=1.): # need to add trust coef\n","        if lr is not required and lr < 0.0:\n","            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n","        if momentum < 0.0:\n","            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n","        if weight_decay < 0.0:\n","            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n","        if trust_coef < 0.0:\n","            raise ValueError(\"Invalid trust_coef value: {}\".format(trust_coef))\n","\n","        defaults = dict(lr=lr, momentum=momentum, weight_decay=weight_decay, trust_coef=trust_coef)\n","\n","        super(SGD_with_lars, self).__init__(params, defaults)\n","\n","    def __setstate__(self, state):\n","        super(SGD_with_lars, self).__setstate__(state)\n","\n","    def step(self, closure=None):\n","        \"\"\"Performs a single optimization step.\n","        Arguments:\n","            closure (callable, optional): A closure that reevaluates the model\n","                and returns the loss.\n","        \"\"\"\n","        loss = None\n","        if closure is not None:\n","            loss = closure()\n","\n","        for group in self.param_groups:\n","            weight_decay = group['weight_decay']\n","            momentum = group['momentum']\n","            trust_coef = group['trust_coef']\n","            global_lr = group['lr']\n","\n","            for p in group['params']:\n","                if p.grad is None:\n","                    continue\n","                d_p = p.grad.data\n","\n","                p_norm = torch.norm(p.data, p=2)\n","                d_p_norm = torch.norm(d_p, p=2).add_(momentum, p_norm)\n","                lr = torch.div(p_norm, d_p_norm).mul_(trust_coef)\n","\n","                lr.mul_(global_lr)\n","\n","                if weight_decay != 0:\n","                    d_p.add_(weight_decay, p.data)\n","\n","                d_p.mul_(lr)\n","\n","                if momentum != 0:\n","                    param_state = self.state[p]\n","                    if 'momentum_buffer' not in param_state:\n","                        buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n","                    else:\n","                        buf = param_state['momentum_buffer']\n","                        buf.mul_(momentum).add_(d_p)\n","                    d_p = buf\n","\n","                p.data.add_(-1, d_p)\n","\n","        return loss"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"5fjc3wkcuqcN","executionInfo":{"status":"ok","timestamp":1601137104556,"user_tz":-540,"elapsed":636,"user":{"displayName":"서준원","photoUrl":"","userId":"09602630791249428793"}}},"source":["def train(net, loader):\n","    \n","    batch_size=256\n","    temperature=0.07\n","\n","    loss_fn = NTXentLoss(batch_size=batch_size, temperature=temperature, use_cosine_similarity=True)\n","    \n","    ### IMPLEMENTATION 4-2 ###\n","    ### 1. Use SGD_with_lars with\n","    ### lr = 0.1 * batch_size / 256\n","    ### momentum = 0.9\n","    ### weight_decay = 1e-6\n","    optimizer = SGD_with_lars(net.parameters(), lr = 0.1 * batch_size / 256, momentum=0.9, weight_decay=1e-6)\n","    \n","    from warmup_scheduler import GradualWarmupScheduler\n","    ### 2. Use GradualWarmupScheduler with\n","    ### multiplier = 1\n","    ### total_epoch = 1/10 of total epochs\n","    ### after_scheduler = optim.lr_scheduler.CosineAnnealingLR\n","    scheduler = GradualWarmupScheduler(optimizer, multiplier=1, total_epoch=20, \n","                                       after_scheduler=optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=180))\n","    \n","    train_start = time.time()\n","    \n","    loss_hist = []\n","\n","    for epoch in range(1, 200 + 1):\n","        \n","        train_loss = 0\n","        net.train()\n","        \n","        epoch_start = time.time()\n","        for idx, (data, target) in enumerate(loader):\n","            optimizer.zero_grad()\n","            \n","            ### 3. data variable contains two augmented images\n","            ### -1. send them to your GPU by calling .cuda()\n","            ### -2. forward each of them to net\n","            ### -3. compute the InfoNCE loss\n","            \n","            # target : labels.\n","\n","            zi, zj = data\n","            \n","            feat_i, emb_i, logit_i = net(zi.cuda())\n","            feat_j, emb_j, logit_j = net(zj.cuda())\n","            loss = loss_fn(emb_i, emb_j)\n","\n","            ### IMPLEMENTATION ENDS HERE ###\n","            \n","            train_loss += loss.item()\n","            \n","            loss.backward()\n","            optimizer.step()\n","            \n","        train_loss /= (idx + 1)\n","        loss_hist.append(train_loss) # added by junwon\n","        scheduler.step()\n","        \n","        epoch_time = time.time() - epoch_start\n","        print(\"Epoch\\t\", epoch, \n","              \"\\tLoss\\t\", train_loss, \n","              \"\\tTime\\t\", epoch_time,\n","             )\n","        \n","    elapsed_train_time = time.time() - train_start\n","    print('Finished training. Train time was:', elapsed_train_time)\n","\n","    return loss_hist\n","\n"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"w2Bqj--ZuqcR","executionInfo":{"status":"ok","timestamp":1601137118342,"user_tz":-540,"elapsed":10002,"user":{"displayName":"서준원","photoUrl":"","userId":"09602630791249428793"}},"outputId":"016e0874-ae7b-4b6d-82f2-32c87fb7b144","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["GPU_NUM = '0'\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = GPU_NUM\n","\n","net = SimCLRNet(26, 1, 10, 32)\n","\n","net.cuda()"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["SimCLRNet(\n","  (feat): ResNetCifar(\n","    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","    (layer1): Sequential(\n","      (0): BasicBlock(\n","        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu1): ReLU(inplace=True)\n","        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu2): ReLU(inplace=True)\n","        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","      (1): BasicBlock(\n","        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu1): ReLU(inplace=True)\n","        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu2): ReLU(inplace=True)\n","        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","      (2): BasicBlock(\n","        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu1): ReLU(inplace=True)\n","        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu2): ReLU(inplace=True)\n","        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","      (3): BasicBlock(\n","        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu1): ReLU(inplace=True)\n","        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu2): ReLU(inplace=True)\n","        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","    (layer2): Sequential(\n","      (0): BasicBlock(\n","        (downsample): Downsample(\n","          (avg): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","        )\n","        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu1): ReLU(inplace=True)\n","        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu2): ReLU(inplace=True)\n","        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","      (1): BasicBlock(\n","        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu1): ReLU(inplace=True)\n","        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu2): ReLU(inplace=True)\n","        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","      (2): BasicBlock(\n","        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu1): ReLU(inplace=True)\n","        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu2): ReLU(inplace=True)\n","        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","      (3): BasicBlock(\n","        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu1): ReLU(inplace=True)\n","        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu2): ReLU(inplace=True)\n","        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","    (layer3): Sequential(\n","      (0): BasicBlock(\n","        (downsample): Downsample(\n","          (avg): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","        )\n","        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu1): ReLU(inplace=True)\n","        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu2): ReLU(inplace=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","      (1): BasicBlock(\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu1): ReLU(inplace=True)\n","        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu2): ReLU(inplace=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","      (2): BasicBlock(\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu1): ReLU(inplace=True)\n","        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu2): ReLU(inplace=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","      (3): BasicBlock(\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu1): ReLU(inplace=True)\n","        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu2): ReLU(inplace=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (relu): ReLU(inplace=True)\n","    (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n","  )\n","  (head): SimCLRHead(\n","    (fc1): Linear(in_features=64, out_features=64, bias=True)\n","    (relu): ReLU(inplace=True)\n","    (fc2): Linear(in_features=64, out_features=32, bias=True)\n","    (norm): Normalize()\n","  )\n","  (classifier): Linear(in_features=64, out_features=10, bias=True)\n","  (norm): Normalize()\n",")"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"43yf5jCluqcT","executionInfo":{"status":"ok","timestamp":1600953363220,"user_tz":-540,"elapsed":13052651,"user":{"displayName":"­서준원 / 학생 / 컴퓨터공학부","photoUrl":"","userId":"17446851116920853979"}},"outputId":"70b4b6cc-5c5e-4723-fe3f-fb35082e96c9","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["#net.zero_grad()\n","loss_list = train(net, train_loader)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:46: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch\t 1 \tLoss\t 6.267516270661965 \tTime\t 68.65935754776001\n","Epoch\t 2 \tLoss\t 5.997548533708621 \tTime\t 69.4774341583252\n","Epoch\t 3 \tLoss\t 5.726462474236121 \tTime\t 69.28733396530151\n","Epoch\t 4 \tLoss\t 5.385201908991887 \tTime\t 69.1840991973877\n","Epoch\t 5 \tLoss\t 5.067203159821339 \tTime\t 69.04848051071167\n","Epoch\t 6 \tLoss\t 4.6063751465235 \tTime\t 69.27956533432007\n","Epoch\t 7 \tLoss\t 4.163073110580444 \tTime\t 69.10657048225403\n","Epoch\t 8 \tLoss\t 3.8278773857997015 \tTime\t 69.57406044006348\n","Epoch\t 9 \tLoss\t 3.618078660964966 \tTime\t 69.37399625778198\n","Epoch\t 10 \tLoss\t 3.4667221289414627 \tTime\t 69.22460794448853\n","Epoch\t 11 \tLoss\t 3.326366952749399 \tTime\t 69.36655616760254\n","Epoch\t 12 \tLoss\t 3.212576044522799 \tTime\t 68.8926649093628\n","Epoch\t 13 \tLoss\t 3.1059215570107486 \tTime\t 69.45085096359253\n","Epoch\t 14 \tLoss\t 3.0056425815973524 \tTime\t 69.58503246307373\n","Epoch\t 15 \tLoss\t 2.9502402220016872 \tTime\t 69.60094261169434\n","Epoch\t 16 \tLoss\t 2.8980539407485573 \tTime\t 69.69247913360596\n","Epoch\t 17 \tLoss\t 2.8292446454366047 \tTime\t 69.91272926330566\n","Epoch\t 18 \tLoss\t 2.796525651980669 \tTime\t 69.46855235099792\n","Epoch\t 19 \tLoss\t 2.751286535996657 \tTime\t 69.60183310508728\n","Epoch\t 20 \tLoss\t 2.725570794863579 \tTime\t 69.23428440093994\n","Epoch\t 21 \tLoss\t 2.68677745843545 \tTime\t 69.87360668182373\n","Epoch\t 22 \tLoss\t 2.6289502498431085 \tTime\t 69.46336817741394\n","Epoch\t 23 \tLoss\t 2.565778276247856 \tTime\t 69.51942682266235\n","Epoch\t 24 \tLoss\t 2.5523773254492346 \tTime\t 68.56583619117737\n","Epoch\t 25 \tLoss\t 2.5162836417173726 \tTime\t 68.56498456001282\n","Epoch\t 26 \tLoss\t 2.4719427059858274 \tTime\t 68.42393064498901\n","Epoch\t 27 \tLoss\t 2.440567861459194 \tTime\t 68.57524490356445\n","Epoch\t 28 \tLoss\t 2.4243976103953826 \tTime\t 67.85034894943237\n","Epoch\t 29 \tLoss\t 2.413613294943785 \tTime\t 68.07151699066162\n","Epoch\t 30 \tLoss\t 2.3794657731667543 \tTime\t 67.70706510543823\n","Epoch\t 31 \tLoss\t 2.3304465990800125 \tTime\t 67.68454360961914\n","Epoch\t 32 \tLoss\t 2.328510089409657 \tTime\t 67.68454313278198\n","Epoch\t 33 \tLoss\t 2.3045998671115973 \tTime\t 67.70062136650085\n","Epoch\t 34 \tLoss\t 2.3009905790671326 \tTime\t 65.9900324344635\n","Epoch\t 35 \tLoss\t 2.2688993539565647 \tTime\t 66.22203993797302\n","Epoch\t 36 \tLoss\t 2.2497884664780052 \tTime\t 65.22188997268677\n","Epoch\t 37 \tLoss\t 2.2232157413776106 \tTime\t 64.85269021987915\n","Epoch\t 38 \tLoss\t 2.226406777822054 \tTime\t 64.0560052394867\n","Epoch\t 39 \tLoss\t 2.2295812050501507 \tTime\t 64.37019276618958\n","Epoch\t 40 \tLoss\t 2.21929702881055 \tTime\t 63.95195126533508\n","Epoch\t 41 \tLoss\t 2.180020068242 \tTime\t 64.59342861175537\n","Epoch\t 42 \tLoss\t 2.160928560525943 \tTime\t 63.999021768569946\n","Epoch\t 43 \tLoss\t 2.1956578480891693 \tTime\t 64.270911693573\n","Epoch\t 44 \tLoss\t 2.1445052813260985 \tTime\t 64.21836233139038\n","Epoch\t 45 \tLoss\t 2.132634694759662 \tTime\t 64.14633202552795\n","Epoch\t 46 \tLoss\t 2.135455678670834 \tTime\t 64.37652349472046\n","Epoch\t 47 \tLoss\t 2.1240479744397676 \tTime\t 63.687742710113525\n","Epoch\t 48 \tLoss\t 2.0787297719564193 \tTime\t 63.95740842819214\n","Epoch\t 49 \tLoss\t 2.1051991059229924 \tTime\t 64.48335433006287\n","Epoch\t 50 \tLoss\t 2.0988356437438576 \tTime\t 63.944589138031006\n","Epoch\t 51 \tLoss\t 2.0975300550460814 \tTime\t 63.56045651435852\n","Epoch\t 52 \tLoss\t 2.0864494030292215 \tTime\t 63.19982552528381\n","Epoch\t 53 \tLoss\t 2.0579177122849686 \tTime\t 63.13495445251465\n","Epoch\t 54 \tLoss\t 2.0632026782402626 \tTime\t 63.20989727973938\n","Epoch\t 55 \tLoss\t 2.0569946533594377 \tTime\t 63.129993200302124\n","Epoch\t 56 \tLoss\t 2.040649994214376 \tTime\t 63.33056855201721\n","Epoch\t 57 \tLoss\t 2.041300230148511 \tTime\t 63.07570481300354\n","Epoch\t 58 \tLoss\t 2.033439413095132 \tTime\t 63.639230251312256\n","Epoch\t 59 \tLoss\t 2.013191468287737 \tTime\t 63.89214277267456\n","Epoch\t 60 \tLoss\t 2.020325320194929 \tTime\t 63.60252404212952\n","Epoch\t 61 \tLoss\t 2.0273418237001466 \tTime\t 63.32847881317139\n","Epoch\t 62 \tLoss\t 1.9968146959940591 \tTime\t 63.407280921936035\n","Epoch\t 63 \tLoss\t 2.018937667210897 \tTime\t 63.60285401344299\n","Epoch\t 64 \tLoss\t 2.00002297376975 \tTime\t 63.400835037231445\n","Epoch\t 65 \tLoss\t 1.9655543339558137 \tTime\t 63.673710107803345\n","Epoch\t 66 \tLoss\t 2.0207853048275677 \tTime\t 64.17776226997375\n","Epoch\t 67 \tLoss\t 1.978113309542338 \tTime\t 63.82818102836609\n","Epoch\t 68 \tLoss\t 1.9487998033181215 \tTime\t 63.906532764434814\n","Epoch\t 69 \tLoss\t 1.945285551975935 \tTime\t 63.87592911720276\n","Epoch\t 70 \tLoss\t 1.9744266436650202 \tTime\t 63.786917209625244\n","Epoch\t 71 \tLoss\t 1.9542911425614968 \tTime\t 63.49198579788208\n","Epoch\t 72 \tLoss\t 1.9325912976876283 \tTime\t 63.555681228637695\n","Epoch\t 73 \tLoss\t 1.9276562910813553 \tTime\t 63.57647895812988\n","Epoch\t 74 \tLoss\t 1.9361189304253994 \tTime\t 63.422266721725464\n","Epoch\t 75 \tLoss\t 1.9350018507395035 \tTime\t 63.535187005996704\n","Epoch\t 76 \tLoss\t 1.926993179321289 \tTime\t 62.564146995544434\n","Epoch\t 77 \tLoss\t 1.9250775862962772 \tTime\t 63.04930877685547\n","Epoch\t 78 \tLoss\t 1.9121704645645925 \tTime\t 63.07352638244629\n","Epoch\t 79 \tLoss\t 1.8979068737763625 \tTime\t 63.042041301727295\n","Epoch\t 80 \tLoss\t 1.8915438193541307 \tTime\t 62.6722686290741\n","Epoch\t 81 \tLoss\t 1.8916537180925026 \tTime\t 63.21482062339783\n","Epoch\t 82 \tLoss\t 1.9038298612985856 \tTime\t 63.78955006599426\n","Epoch\t 83 \tLoss\t 1.8805216654753074 \tTime\t 63.44261360168457\n","Epoch\t 84 \tLoss\t 1.8818345858500554 \tTime\t 63.472145318984985\n","Epoch\t 85 \tLoss\t 1.893834277911064 \tTime\t 62.656704902648926\n","Epoch\t 86 \tLoss\t 1.8679261170900785 \tTime\t 62.974589586257935\n","Epoch\t 87 \tLoss\t 1.8642448761524297 \tTime\t 62.813273191452026\n","Epoch\t 88 \tLoss\t 1.8449247977672478 \tTime\t 63.055755615234375\n","Epoch\t 89 \tLoss\t 1.8696678986916175 \tTime\t 63.302762031555176\n","Epoch\t 90 \tLoss\t 1.8722039497815646 \tTime\t 62.85462260246277\n","Epoch\t 91 \tLoss\t 1.870667578623845 \tTime\t 62.75846743583679\n","Epoch\t 92 \tLoss\t 1.8474222091528085 \tTime\t 63.194554805755615\n","Epoch\t 93 \tLoss\t 1.8648451676735511 \tTime\t 62.94294333457947\n","Epoch\t 94 \tLoss\t 1.8446526399025551 \tTime\t 62.87138080596924\n","Epoch\t 95 \tLoss\t 1.8276663988064497 \tTime\t 63.0919828414917\n","Epoch\t 96 \tLoss\t 1.8224965651830038 \tTime\t 63.08124852180481\n","Epoch\t 97 \tLoss\t 1.8225131157117012 \tTime\t 63.06569290161133\n","Epoch\t 98 \tLoss\t 1.8259998278740124 \tTime\t 63.35394859313965\n","Epoch\t 99 \tLoss\t 1.831270324266874 \tTime\t 63.41454291343689\n","Epoch\t 100 \tLoss\t 1.833235372029818 \tTime\t 62.99313187599182\n","Epoch\t 101 \tLoss\t 1.817083610021151 \tTime\t 62.80047941207886\n","Epoch\t 102 \tLoss\t 1.8051621987269475 \tTime\t 62.8501501083374\n","Epoch\t 103 \tLoss\t 1.8123325830850845 \tTime\t 63.097904920578\n","Epoch\t 104 \tLoss\t 1.7852651388217242 \tTime\t 63.0553183555603\n","Epoch\t 105 \tLoss\t 1.8084005435307822 \tTime\t 62.94760346412659\n","Epoch\t 106 \tLoss\t 1.8041324670498187 \tTime\t 63.25816869735718\n","Epoch\t 107 \tLoss\t 1.7820193205124293 \tTime\t 62.923250675201416\n","Epoch\t 108 \tLoss\t 1.7962806524374546 \tTime\t 62.94171738624573\n","Epoch\t 109 \tLoss\t 1.7930817304513393 \tTime\t 62.812955379486084\n","Epoch\t 110 \tLoss\t 1.789792967453981 \tTime\t 62.79203796386719\n","Epoch\t 111 \tLoss\t 1.7812223055423835 \tTime\t 62.60554313659668\n","Epoch\t 112 \tLoss\t 1.785346525754684 \tTime\t 63.17959928512573\n","Epoch\t 113 \tLoss\t 1.7885493657527827 \tTime\t 63.33392405509949\n","Epoch\t 114 \tLoss\t 1.7671677020879892 \tTime\t 62.74914860725403\n","Epoch\t 115 \tLoss\t 1.7685645977656046 \tTime\t 63.25654315948486\n","Epoch\t 116 \tLoss\t 1.7716081185218615 \tTime\t 62.876837730407715\n","Epoch\t 117 \tLoss\t 1.7829843594477728 \tTime\t 63.14761471748352\n","Epoch\t 118 \tLoss\t 1.772922008465498 \tTime\t 62.828460693359375\n","Epoch\t 119 \tLoss\t 1.7659885754952065 \tTime\t 63.11888647079468\n","Epoch\t 120 \tLoss\t 1.7448960218674097 \tTime\t 63.22929286956787\n","Epoch\t 121 \tLoss\t 1.7520486501547006 \tTime\t 63.6432888507843\n","Epoch\t 122 \tLoss\t 1.7521573433509239 \tTime\t 62.95422625541687\n","Epoch\t 123 \tLoss\t 1.7678827206293741 \tTime\t 63.3798565864563\n","Epoch\t 124 \tLoss\t 1.7502376458583735 \tTime\t 62.8237099647522\n","Epoch\t 125 \tLoss\t 1.769895480840634 \tTime\t 62.9195613861084\n","Epoch\t 126 \tLoss\t 1.756000240643819 \tTime\t 62.95954418182373\n","Epoch\t 127 \tLoss\t 1.7506872965739324 \tTime\t 62.67074799537659\n","Epoch\t 128 \tLoss\t 1.745745114179758 \tTime\t 63.118263483047485\n","Epoch\t 129 \tLoss\t 1.7377324495560085 \tTime\t 63.108617067337036\n","Epoch\t 130 \tLoss\t 1.7158134686641204 \tTime\t 63.02924919128418\n","Epoch\t 131 \tLoss\t 1.7412236030285175 \tTime\t 62.89516854286194\n","Epoch\t 132 \tLoss\t 1.7306649568753365 \tTime\t 62.679909229278564\n","Epoch\t 133 \tLoss\t 1.7245748746089447 \tTime\t 62.66066098213196\n","Epoch\t 134 \tLoss\t 1.7289566480196439 \tTime\t 63.32841229438782\n","Epoch\t 135 \tLoss\t 1.7108234833448361 \tTime\t 62.491352558135986\n","Epoch\t 136 \tLoss\t 1.6991298583837655 \tTime\t 62.3332724571228\n","Epoch\t 137 \tLoss\t 1.7115736839098807 \tTime\t 62.74306297302246\n","Epoch\t 138 \tLoss\t 1.7007082712955963 \tTime\t 62.762959718704224\n","Epoch\t 139 \tLoss\t 1.7187429745992024 \tTime\t 62.61979103088379\n","Epoch\t 140 \tLoss\t 1.6926693610655956 \tTime\t 62.504223346710205\n","Epoch\t 141 \tLoss\t 1.7033146136846298 \tTime\t 62.608073472976685\n","Epoch\t 142 \tLoss\t 1.7205791528408343 \tTime\t 62.628968715667725\n","Epoch\t 143 \tLoss\t 1.6938049933849237 \tTime\t 62.75688982009888\n","Epoch\t 144 \tLoss\t 1.7151904136706622 \tTime\t 63.039963483810425\n","Epoch\t 145 \tLoss\t 1.6962251809927134 \tTime\t 63.067909717559814\n","Epoch\t 146 \tLoss\t 1.6823487367385472 \tTime\t 62.384145736694336\n","Epoch\t 147 \tLoss\t 1.6853399460132306 \tTime\t 62.321884632110596\n","Epoch\t 148 \tLoss\t 1.6768621634214351 \tTime\t 62.94698643684387\n","Epoch\t 149 \tLoss\t 1.6715527577277942 \tTime\t 62.54809641838074\n","Epoch\t 150 \tLoss\t 1.665389627065414 \tTime\t 62.6585054397583\n","Epoch\t 151 \tLoss\t 1.6830049331371602 \tTime\t 62.95280051231384\n","Epoch\t 152 \tLoss\t 1.6835978721960998 \tTime\t 62.745948791503906\n","Epoch\t 153 \tLoss\t 1.67967787644802 \tTime\t 62.68786954879761\n","Epoch\t 154 \tLoss\t 1.6763857138462557 \tTime\t 62.95465064048767\n","Epoch\t 155 \tLoss\t 1.6782261481651892 \tTime\t 62.714362382888794\n","Epoch\t 156 \tLoss\t 1.6617795449036818 \tTime\t 62.96763586997986\n","Epoch\t 157 \tLoss\t 1.675019753896273 \tTime\t 62.9510543346405\n","Epoch\t 158 \tLoss\t 1.675074587112818 \tTime\t 65.35823965072632\n","Epoch\t 159 \tLoss\t 1.6546062561181876 \tTime\t 71.55635094642639\n","Epoch\t 160 \tLoss\t 1.6746668840065981 \tTime\t 71.21927261352539\n","Epoch\t 161 \tLoss\t 1.6435573663467016 \tTime\t 69.76940560340881\n","Epoch\t 162 \tLoss\t 1.6557708746347672 \tTime\t 69.46242380142212\n","Epoch\t 163 \tLoss\t 1.6678790911650045 \tTime\t 69.35124659538269\n","Epoch\t 164 \tLoss\t 1.664384509355594 \tTime\t 71.1125271320343\n","Epoch\t 165 \tLoss\t 1.6600723688419048 \tTime\t 71.94162893295288\n","Epoch\t 166 \tLoss\t 1.661243896606641 \tTime\t 72.01282262802124\n","Epoch\t 167 \tLoss\t 1.6629147285070174 \tTime\t 72.33958768844604\n","Epoch\t 168 \tLoss\t 1.6620125477130596 \tTime\t 72.95786356925964\n","Epoch\t 169 \tLoss\t 1.6602887862767928 \tTime\t 73.96707892417908\n","Epoch\t 170 \tLoss\t 1.6492626385811047 \tTime\t 74.312002658844\n","Epoch\t 171 \tLoss\t 1.6598077217737834 \tTime\t 74.04334545135498\n","Epoch\t 172 \tLoss\t 1.6313616501979338 \tTime\t 73.81621289253235\n","Epoch\t 173 \tLoss\t 1.6453075335575984 \tTime\t 73.28137230873108\n","Epoch\t 174 \tLoss\t 1.6391288347733326 \tTime\t 71.95601725578308\n","Epoch\t 175 \tLoss\t 1.6526925508792585 \tTime\t 70.92093181610107\n","Epoch\t 176 \tLoss\t 1.6408355028201371 \tTime\t 70.17202544212341\n","Epoch\t 177 \tLoss\t 1.6741687591259296 \tTime\t 69.64730167388916\n","Epoch\t 178 \tLoss\t 1.6493007904443986 \tTime\t 69.17150163650513\n","Epoch\t 179 \tLoss\t 1.6307186945890768 \tTime\t 67.66972494125366\n","Epoch\t 180 \tLoss\t 1.6318966621007676 \tTime\t 67.56845283508301\n","Epoch\t 181 \tLoss\t 1.6460150443590604 \tTime\t 66.0144100189209\n","Epoch\t 182 \tLoss\t 1.6342561336664052 \tTime\t 65.29189372062683\n","Epoch\t 183 \tLoss\t 1.6374236284158168 \tTime\t 65.38262867927551\n","Epoch\t 184 \tLoss\t 1.6324674893648197 \tTime\t 65.18134713172913\n","Epoch\t 185 \tLoss\t 1.6391727582002298 \tTime\t 65.1057243347168\n","Epoch\t 186 \tLoss\t 1.6311474506671613 \tTime\t 64.95918345451355\n","Epoch\t 187 \tLoss\t 1.6350793679555258 \tTime\t 64.83811283111572\n","Epoch\t 188 \tLoss\t 1.6467056940763425 \tTime\t 64.92646360397339\n","Epoch\t 189 \tLoss\t 1.6347985512171037 \tTime\t 65.87502717971802\n","Epoch\t 190 \tLoss\t 1.6369034186387674 \tTime\t 65.06599950790405\n","Epoch\t 191 \tLoss\t 1.6355789410762298 \tTime\t 64.79733896255493\n","Epoch\t 192 \tLoss\t 1.6241045731764574 \tTime\t 64.38663864135742\n","Epoch\t 193 \tLoss\t 1.6254989746289374 \tTime\t 63.669490575790405\n","Epoch\t 194 \tLoss\t 1.6319795125570054 \tTime\t 63.83303713798523\n","Epoch\t 195 \tLoss\t 1.6416837533315023 \tTime\t 64.1012270450592\n","Epoch\t 196 \tLoss\t 1.625650544044299 \tTime\t 64.25086784362793\n","Epoch\t 197 \tLoss\t 1.6344465396343133 \tTime\t 64.62059736251831\n","Epoch\t 198 \tLoss\t 1.6315676242877275 \tTime\t 64.30861186981201\n","Epoch\t 199 \tLoss\t 1.6486536013774382 \tTime\t 64.37908816337585\n","Epoch\t 200 \tLoss\t 1.6273717764096383 \tTime\t 64.58815407752991\n","Finished training. Train time was: 13051.887996673584\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"j8TTC-0U-Xkt","executionInfo":{"status":"ok","timestamp":1601137118343,"user_tz":-540,"elapsed":5102,"user":{"displayName":"서준원","photoUrl":"","userId":"09602630791249428793"}}},"source":["import matplotlib.pyplot as plt\n","\n","def plot_loss(loss_hist, xlabel='Iteration number', ylabel='Loss value') :\n","  plt.plot(loss_hist)\n","  plt.xlabel(xlabel)\n","  plt.ylabel(ylabel)\n","  plt.show()"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"AVo5Msnl--6B","executionInfo":{"status":"ok","timestamp":1600953528378,"user_tz":-540,"elapsed":696,"user":{"displayName":"­서준원 / 학생 / 컴퓨터공학부","photoUrl":"","userId":"17446851116920853979"}},"outputId":"5998825a-5a5a-4f56-a8b5-632444f16936","colab":{"base_uri":"https://localhost:8080/","height":279}},"source":["plot_loss(loss_list)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"8ufE49N8BcxA","executionInfo":{"status":"ok","timestamp":1601137118344,"user_tz":-540,"elapsed":3599,"user":{"displayName":"서준원","photoUrl":"","userId":"09602630791249428793"}}},"source":["def save_model(model, model_save_name = 'SIMCLR.pt' ) :\n","  # save ckpt in google drive.\n","  from google.colab import drive\n","  drive.mount('/content/gdrive')\n","  path = F\"/content/gdrive/My Drive/{model_save_name}\" \n","  torch.save(model.state_dict(), path)"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"ztekTngYVO20","executionInfo":{"status":"ok","timestamp":1601137118344,"user_tz":-540,"elapsed":2752,"user":{"displayName":"서준원","photoUrl":"","userId":"09602630791249428793"}}},"source":["def load_model(model, path='SIMCLR.pt') :\n","  from google.colab import drive\n","  drive.mount('/content/gdrive')\n","  checkpoints_folder =  F\"/content/gdrive/My Drive/{path}\" \n","  state_dict = torch.load(checkpoints_folder)\n","  model.load_state_dict(state_dict)"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"5CPurz9SuqcV","executionInfo":{"status":"ok","timestamp":1601099389199,"user_tz":-540,"elapsed":21779,"user":{"displayName":"서준원","photoUrl":"","userId":"09602630791249428793"}},"outputId":"3d763810-4eac-4754-f7a8-26179841ec15","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["save_model(net, \"simclr_new.pt\")"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"u6q9K4_lgIJB","executionInfo":{"status":"ok","timestamp":1601137137302,"user_tz":-540,"elapsed":20521,"user":{"displayName":"서준원","photoUrl":"","userId":"09602630791249428793"}},"outputId":"8876ae24-29e4-4235-db97-4308c9825fbb","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["load_model(net, \"simclr_new.pt\")"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QDATNL_YuUIP"},"source":["## Linear Evaluation Protocol\n","\n","- train Linear classifier with freezed extractor f"]},{"cell_type":"code","metadata":{"id":"ocucN1fyuTZU","executionInfo":{"status":"ok","timestamp":1601137171833,"user_tz":-540,"elapsed":1087,"user":{"displayName":"서준원","photoUrl":"","userId":"09602630791249428793"}}},"source":["def train_classifier(net, loader):\n","    \n","    batch_size=256\n","    temperature=0.07\n","\n","    loss_fn = torch.nn.CrossEntropyLoss()\n","\n","    # Freezing\n","    net.feat.requires_grad = False\n","    net.head.requires_grad = False\n","    net.norm.requires_grad = False\n","\n","   # net.feat.train(False)\n","  \n","    optimizer = torch.optim.Adam(net.classifier.parameters(), lr=1e-3)\n","    \n","    train_start = time.time()\n","    \n","    loss_hist = []\n","    min_loss = 10000\n","\n","    for epoch in range(1, 10 + 1):\n","        \n","        train_loss = 0\n","        #net.feat.train(False)\n","        #net.classifier.train(True) \n","        net.train()\n","        epoch_start = time.time()\n","\n","        for idx, (data, target) in enumerate(loader):\n","            optimizer.zero_grad()\n","            \n","            feat, emb, logit = net(data.cuda())\n","            \n","            loss = loss_fn(logit, target.cuda())\n","          \n","            train_loss += loss.item()\n","            \n","            loss.backward()\n","            optimizer.step()\n","            \n","        train_loss /= (idx + 1)\n","        loss_hist.append(train_loss) # added by junwon\n","        if train_loss < min_loss :\n","          min_loss = train_loss\n","          save_model(net, \"classifier\")\n","       \n","        epoch_time = time.time() - epoch_start\n","        print(\"Epoch\\t\", epoch, \n","              \"\\tLoss\\t\", train_loss, \n","              \"\\tTime\\t\", epoch_time,\n","             )\n","        \n","    elapsed_train_time = time.time() - train_start\n","    print('Finished training. Train time was:', elapsed_train_time)\n","\n","    return loss_hist\n","\n","def test_classifier(net, loader) :\n","  \n","  net.eval()\n","  \n","  correct = 0\n","  total = 0\n","  \n","  with torch.no_grad() :\n","    for idx, (data, target) in enumerate(loader):\n","      \n","      feat, emb, logit = net(data.cuda())\n","      #loss = loss_fn(logit, target.cuda())\n","      \n","      _, class_i = torch.max(logit.data, 1)\n","\n","      correct += (class_i == target.cuda()).sum().item()  \n","      total += target.size(0)\n","\n","    accuracy = correct / total\n","    print('Accuracy : %d %%' % (100 * accuracy))\n","  \n","  return accuracy\n","  \n"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"wTV1vdlDYlWt","executionInfo":{"status":"ok","timestamp":1601137176144,"user_tz":-540,"elapsed":2653,"user":{"displayName":"서준원","photoUrl":"","userId":"09602630791249428793"}},"outputId":"f8e778c0-4567-4feb-8814-2a1282da1e7a","colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["from torch.utils.data import DataLoader\n","\n","linear_dataset = datasets.CIFAR10(root='.',\n","                                 train=True,\n","                                 download=True,\n","                                 transform=transforms.ToTensor()\n","                                )\n","\n","linear_loader = DataLoader(linear_dataset,\n","                          batch_size=256,\n","                          num_workers=4,\n","                          shuffle=True,\n","                          drop_last=True\n","                         )\n","\n","test_dataset = datasets.CIFAR10(root='.',\n","                                 train=False,\n","                                 download=True,\n","                                 transform=transforms.ToTensor()\n","                                )\n","\n","test_loader = DataLoader(test_dataset,\n","                          batch_size=256,\n","                          num_workers=4,\n","                          shuffle=True,\n","                          drop_last=True\n","                         )"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6gBtJya5UsO1","executionInfo":{"status":"ok","timestamp":1600956080948,"user_tz":-540,"elapsed":515277,"user":{"displayName":"­서준원 / 학생 / 컴퓨터공학부","photoUrl":"","userId":"17446851116920853979"}},"outputId":"70fde04b-12ee-466e-f63a-e8753a69243a","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["loss_list = train_classifier(net, linear_loader)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 1 \tLoss\t 2.03468404121888 \tTime\t 17.137782096862793\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 2 \tLoss\t 1.6301073435025337 \tTime\t 17.24441909790039\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 3 \tLoss\t 1.4283286290291028 \tTime\t 17.066750288009644\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 4 \tLoss\t 1.3150144711518899 \tTime\t 17.026099681854248\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 5 \tLoss\t 1.2440750293242626 \tTime\t 17.067901849746704\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 6 \tLoss\t 1.1951002854567307 \tTime\t 17.105433702468872\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 7 \tLoss\t 1.1595835169156392 \tTime\t 17.125438451766968\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 8 \tLoss\t 1.1338184274159946 \tTime\t 17.08957076072693\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 9 \tLoss\t 1.1112631134497815 \tTime\t 17.097830295562744\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 10 \tLoss\t 1.0948643947258974 \tTime\t 17.093303203582764\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 11 \tLoss\t 1.0812407942918631 \tTime\t 17.017643213272095\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 12 \tLoss\t 1.0695079476405414 \tTime\t 17.06408953666687\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 13 \tLoss\t 1.0593032726874718 \tTime\t 17.09551215171814\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 14 \tLoss\t 1.0512505369308667 \tTime\t 17.128983736038208\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 15 \tLoss\t 1.0428527550819593 \tTime\t 17.111952304840088\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 16 \tLoss\t 1.0376738615525074 \tTime\t 17.099048852920532\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 17 \tLoss\t 1.031489634513855 \tTime\t 17.15119194984436\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 18 \tLoss\t 1.027635129292806 \tTime\t 17.12689518928528\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 19 \tLoss\t 1.022793124577938 \tTime\t 17.13813066482544\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 20 \tLoss\t 1.0190817640377925 \tTime\t 17.150291204452515\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 21 \tLoss\t 1.014194057537959 \tTime\t 17.08976435661316\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 22 \tLoss\t 1.0108654868908418 \tTime\t 17.163275003433228\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 23 \tLoss\t 1.0085886481480721 \tTime\t 17.21928095817566\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 24 \tLoss\t 1.006259562113346 \tTime\t 17.208226680755615\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 25 \tLoss\t 1.002286654863602 \tTime\t 17.335190057754517\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 26 \tLoss\t 0.9998141579138927 \tTime\t 17.245723962783813\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 27 \tLoss\t 0.9975449063839057 \tTime\t 17.2394061088562\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 28 \tLoss\t 0.9960140689825401 \tTime\t 17.307114362716675\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 29 \tLoss\t 0.9927120361572657 \tTime\t 17.32283854484558\n","Epoch\t 30 \tLoss\t 0.9929774443308512 \tTime\t 17.321990966796875\n","Finished training. Train time was: 514.6455535888672\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SNUX8YOYgnNW","executionInfo":{"status":"ok","timestamp":1600956088698,"user_tz":-540,"elapsed":2633,"user":{"displayName":"­서준원 / 학생 / 컴퓨터공학부","photoUrl":"","userId":"17446851116920853979"}},"outputId":"49721a9c-523b-4935-e7cb-9f3319b94b09","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["acc = test_classifier(net, test_loader)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy : 64 %\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3yKmtkiya2FF"},"source":["## classifier를 Train할 때 Epoch\n","계속 Loss 낮아지지만, 이미 60%를 넘으므로. \n","Pretrain 성능을 평가하기 위해 \n","\"초기에 얼마나 트레인이 잘 되는지\" 를 평가하기 위해서 에폭을 늘리지 않음. "]},{"cell_type":"markdown","metadata":{"id":"Pek2bM80tkes"},"source":["##Train Function with Parameters.\n","\n","- save the model with lowest loss\n","- parameters : batch_size, temparture, pre-training epochs.\n","\n"]},{"cell_type":"code","metadata":{"id":"rDJ6q_OzGOxz","executionInfo":{"status":"ok","timestamp":1601137179801,"user_tz":-540,"elapsed":643,"user":{"displayName":"서준원","photoUrl":"","userId":"09602630791249428793"}}},"source":["from torch.utils.data import DataLoader\n","\n","def make_loader(batch_size) :\n","  \n","  train_dataset = datasets.CIFAR10(root='.',\n","                                  train=True,\n","                                  download=True,\n","                                  transform=train_transform\n","                                  )\n","\n","  train_loader = DataLoader(train_dataset,\n","                            batch_size=batch_size,\n","                            num_workers=4,\n","                            shuffle=True,\n","                            drop_last=True\n","                          )\n","  \n","  linear_dataset = datasets.CIFAR10(root='.',\n","                                 train=True,\n","                                 download=True,\n","                                 transform=transforms.ToTensor()\n","                                )\n","\n","  linear_loader = DataLoader(linear_dataset,\n","                            batch_size=batch_size,\n","                            num_workers=4,\n","                            shuffle=True,\n","                            drop_last=True\n","                          )\n","\n","  test_dataset = datasets.CIFAR10(root='.',\n","                                  train=False,\n","                                  download=True,\n","                                  transform=transforms.ToTensor()\n","                                  )\n","\n","  test_loader = DataLoader(test_dataset,\n","                            batch_size=batch_size,\n","                            num_workers=4,\n","                            shuffle=True,\n","                            drop_last=True\n","                          )\n","  \n","  return train_loader, linear_loader, test_loader"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"ak1IDc6gtpqW","executionInfo":{"status":"ok","timestamp":1601137183569,"user_tz":-540,"elapsed":601,"user":{"displayName":"서준원","photoUrl":"","userId":"09602630791249428793"}}},"source":["def train_param(net, loader, batch_size = 256, temperature = 0.07, n_epoch = 200):\n","    \n","\n","    loss_fn = NTXentLoss(batch_size=batch_size, temperature=temperature, use_cosine_similarity=True)\n","    \n","\n","    optimizer = SGD_with_lars(net.parameters(), lr = 0.1 * batch_size / 256, momentum=0.9, weight_decay=1e-6)\n","    \n","    from warmup_scheduler import GradualWarmupScheduler\n","    ### 2. Use GradualWarmupScheduler with\n","    ### multiplier = 1\n","    ### total_epoch = 1/10 of total epochs\n","    ### after_scheduler = optim.lr_scheduler.CosineAnnealingLR\n","    scheduler = GradualWarmupScheduler(optimizer, multiplier=1, total_epoch= n_epoch // 10, \n","                                       after_scheduler=optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max= n_epoch - n_epoch // 10))\n","    \n","    train_start = time.time()\n","    \n","    loss_hist = []\n","    import os.path\n","    checkpoint = range(0, 200, 40)\n","    x = 0\n","    for ckpt in checkpoint :\n","      if os.path.isfile(\"/content/gdrive/My Drive/simclr_{}_{}_epoch{}in{}.pt\".format(batch_size, temperature, ckpt, n_epoch)) :\n","        load_model(net, \"simclr_{}_{}_epoch{}in{}.pt\".format(batch_size, temperature, ckpt, n_epoch))\n","        x = ckpt\n","        print(\"loaded\", ckpt)\n","\n","    for epoch in range(1 + x, n_epoch + 1):\n","        \n","        train_loss = 0\n","        net.train()\n","        epoch_start = time.time()\n","        for idx, (data, target) in enumerate(loader):\n","            optimizer.zero_grad()\n","            \n","            ### 3. data variable contains two augmented images\n","            ### -1. send them to your GPU by calling .cuda()\n","            ### -2. forward each of them to net\n","            ### -3. compute the InfoNCE loss\n","            \n","            # target : labels.\n","\n","            zi, zj = data\n","            feat_i, emb_i, logit_i = net(zi.cuda())\n","            feat_j, emb_j, logit_j = net(zj.cuda())\n","            \n","            loss = loss_fn(emb_i, emb_j)\n","\n","            ### IMPLEMENTATION ENDS HERE ###\n","            \n","            train_loss += loss.item()\n","            \n","            loss.backward()\n","            optimizer.step()\n","            \n","        train_loss /= (idx + 1)\n","        loss_hist.append(train_loss) # added by junwon\n","        scheduler.step()\n","        \n","        epoch_time = time.time() - epoch_start\n","        print(\"Epoch\\t\", epoch, \n","              \"\\tLoss\\t\", train_loss, \n","              \"\\tTime\\t\", epoch_time,\n","             )\n","        if epoch % 40 == 0 :\n","          save_model(net, \"simclr_{}_{}_epoch{}in{}.pt\".format(batch_size, temperature, epoch, n_epoch))\n","        \n","    elapsed_train_time = time.time() - train_start\n","    print('Finished training. Train time was:', elapsed_train_time)\n","\n","    return loss_hist"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"hPyb8818h3mJ","executionInfo":{"status":"ok","timestamp":1601075410449,"user_tz":-540,"elapsed":514,"user":{"displayName":"서준원","photoUrl":"","userId":"09602630791249428793"}},"outputId":"0ff29586-5de3-4c89-8132-d0f70ab2a033","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["net = SimCLRNet(26, 1, 10, 32)\n","net.cuda()\n","net.zero_grad()\n","load_model(net, \"simclr_256_0.01_epoch200in200.pt\")\n"],"execution_count":23,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aoIIbBf6h_OM","executionInfo":{"status":"ok","timestamp":1601076230611,"user_tz":-540,"elapsed":2356,"user":{"displayName":"서준원","photoUrl":"","userId":"09602630791249428793"}},"outputId":"bcadbb6d-9f81-480b-b373-b1081327d937","colab":{"base_uri":"https://localhost:8080/","height":70}},"source":["load_model(net, \"classifier\")\n","acc = test_classifier(net, testl)\n","save_model(net, \"simclr_tmp_{}.pt\".format(0.01))"],"execution_count":26,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Accuracy : 65 %\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cVW2dg4wcplr","executionInfo":{"status":"error","timestamp":1601076954737,"user_tz":-540,"elapsed":107090,"user":{"displayName":"서준원","photoUrl":"","userId":"09602630791249428793"}},"outputId":"40eb2964-d23b-4b48-b83d-57377e9f8e63","colab":{"base_uri":"https://localhost:8080/","height":460}},"source":["batch_size_list = [64, 128, 512, 1024]\n","temperature_list = [0.05, 0.1, 0.5, 1.0]\n","n_epoch_list = [100]\n","\n","batch_loss =[]\n","temperature_loss =[]\n","epoch_loss =[]\n","\n","batch_acc =[]\n","temperature_acc =[]\n","epoch_acc =[]\n","\n","# batch_size test\n","\n","\n","\n","for t in temperature_list :\n","  net = SimCLRNet(26, 1, 10, 32)\n","  net.cuda()\n","  net.zero_grad()\n","  tl, ll, testl = make_loader(256)\n","  loss = train_param(net, tl, temperature=t)\n","  temperature_loss.append(loss)\n","  train_classifier(net, ll)\n","  acc = test_classifier(net, testl)\n","  temperature_acc.append((t, acc))\n","  save_model(net, \"simclr_tmp_{}.pt\".format(t))\n","\n","for n in n_epoch_list :\n","  net = SimCLRNet(26, 1, 10, 32)\n","  net.cuda()\n","  net.zero_grad()\n","  tl, ll, testl = make_loader(256)\n","  loss = train_param(net, tl, n_epoch=n)\n","  epoch_loss.append(loss)\n","  train_classifier(net, ll)\n","  acc = test_classifier(net, testl)\n","  epoch_acc.append((n, acc))\n","  save_model(net, \"simclr_epoch_{}.pt\".format(n))"],"execution_count":29,"outputs":[{"output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Epoch\t 1 \tLoss\t 7.305410940219194 \tTime\t 64.55060696601868\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-48c2d57c5d2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0mtl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m   \u001b[0mtemperature_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0mtrain_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-21-3cd7075f5e06>\u001b[0m in \u001b[0;36mtrain_param\u001b[0;34m(net, loader, batch_size, temperature, n_epoch)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mepoch_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;31m### 3. data variable contains two augmented images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m                     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"bEYZb5UDj1TU","outputId":"4804841a-0411-41c0-c26c-7fbe4b55cadd","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["batch_size_list = [64, 128, 512, 1024]\n","temperature_list = [0.05, 0.1, 0.5, 1.0]\n","n_epoch_list = [100]\n","\n","batch_loss =[]\n","temperature_loss =[]\n","epoch_loss =[]\n","\n","batch_acc =[]\n","temperature_acc =[]\n","epoch_acc =[]\n","\n","\n","for t in temperature_list :\n","  net = SimCLRNet(26, 1, 10, 32)\n","  net.cuda()\n","  net.zero_grad()\n","  tl, ll, testl = make_loader(256)\n","  loss = train_param(net, tl, temperature=t)\n","  temperature_loss.append(loss)\n","  train_classifier(net, ll)\n","  acc = test_classifier(net, testl)\n","  temperature_acc.append((t, acc))\n","  save_model(net, \"simclr_tmp_{}.pt\".format(t))\n","\n","for n in n_epoch_list :\n","  net = SimCLRNet(26, 1, 10, 32)\n","  net.cuda()\n","  net.zero_grad()\n","  tl, ll, testl = make_loader(256)\n","  loss = train_param(net, tl, n_epoch=n)\n","  epoch_loss.append(loss)\n","  train_classifier(net, ll)\n","  acc = test_classifier(net, testl)\n","  epoch_acc.append((n, acc))\n","  save_model(net, \"simclr_epoch_{}.pt\".format(n))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n","Files already downloaded and verified\n"],"name":"stdout"},{"output_type":"stream","text":["Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f5794c86cf8>>\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f5794c86cf8>>\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f5794c86cf8>>\n","Traceback (most recent call last):\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f5794c86cf8>>\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1101, in __del__\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1101, in __del__\n","Traceback (most recent call last):\n","    self._shutdown_workers()\n","Traceback (most recent call last):\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1075, in _shutdown_workers\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1101, in __del__\n","    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1101, in __del__\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1075, in _shutdown_workers\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1075, in _shutdown_workers\n","    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n","    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n","    self._shutdown_workers()\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","AssertionError: can only join a child process\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1075, in _shutdown_workers\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n","AssertionError: can only join a child process\n","    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch\t 1 \tLoss\t 7.189136346181233 \tTime\t 63.38021945953369\n","Epoch\t 2 \tLoss\t 6.002117120302641 \tTime\t 63.17547059059143\n","Epoch\t 3 \tLoss\t 5.628581521449945 \tTime\t 63.55437922477722\n","Epoch\t 4 \tLoss\t 5.236422247764391 \tTime\t 63.704386472702026\n","Epoch\t 5 \tLoss\t 4.841975737840701 \tTime\t 63.25448656082153\n","Epoch\t 6 \tLoss\t 4.459486784079136 \tTime\t 63.41651201248169\n","Epoch\t 7 \tLoss\t 4.093474456591484 \tTime\t 63.71238660812378\n","Epoch\t 8 \tLoss\t 3.830213400033804 \tTime\t 63.789615631103516\n","Epoch\t 9 \tLoss\t 3.5753409997010843 \tTime\t 63.66919183731079\n","Epoch\t 10 \tLoss\t 3.428507354931954 \tTime\t 63.70458984375\n","Epoch\t 11 \tLoss\t 3.295331216469789 \tTime\t 64.58187675476074\n","Epoch\t 12 \tLoss\t 3.1458092408302503 \tTime\t 63.11143088340759\n","Epoch\t 13 \tLoss\t 3.058456293741862 \tTime\t 64.35694360733032\n","Epoch\t 14 \tLoss\t 2.998601201864389 \tTime\t 64.0052797794342\n","Epoch\t 15 \tLoss\t 2.897271336041964 \tTime\t 64.5133695602417\n","Epoch\t 16 \tLoss\t 2.8430903361393853 \tTime\t 63.76951789855957\n","Epoch\t 17 \tLoss\t 2.80997687119704 \tTime\t 63.245630979537964\n","Epoch\t 18 \tLoss\t 2.743338901568682 \tTime\t 63.19971990585327\n","Epoch\t 19 \tLoss\t 2.70577828456194 \tTime\t 64.1453287601471\n","Epoch\t 20 \tLoss\t 2.63676964808733 \tTime\t 63.53420829772949\n","Epoch\t 21 \tLoss\t 2.634338402136778 \tTime\t 62.7463743686676\n","Epoch\t 22 \tLoss\t 2.5658275946592672 \tTime\t 63.701924562454224\n","Epoch\t 23 \tLoss\t 2.5145671893388797 \tTime\t 63.508492946624756\n","Epoch\t 24 \tLoss\t 2.465314959257077 \tTime\t 63.74002170562744\n","Epoch\t 25 \tLoss\t 2.4208470430129614 \tTime\t 63.515251874923706\n","Epoch\t 26 \tLoss\t 2.416827073464027 \tTime\t 62.5235161781311\n","Epoch\t 27 \tLoss\t 2.3861211263216457 \tTime\t 64.18882942199707\n","Epoch\t 28 \tLoss\t 2.362894987448668 \tTime\t 63.53222370147705\n","Epoch\t 29 \tLoss\t 2.3350523673571075 \tTime\t 62.90287899971008\n","Epoch\t 30 \tLoss\t 2.3249015679726233 \tTime\t 63.44724416732788\n","Epoch\t 31 \tLoss\t 2.2836535417116606 \tTime\t 63.155985832214355\n","Epoch\t 32 \tLoss\t 2.2745820314456253 \tTime\t 63.24831438064575\n","Epoch\t 33 \tLoss\t 2.2350260147681604 \tTime\t 63.06330847740173\n","Epoch\t 34 \tLoss\t 2.2107092062632243 \tTime\t 62.78591752052307\n","Epoch\t 35 \tLoss\t 2.2056093368774805 \tTime\t 63.38243341445923\n","Epoch\t 36 \tLoss\t 2.178441763535524 \tTime\t 63.61489248275757\n","Epoch\t 37 \tLoss\t 2.167751920528901 \tTime\t 63.64454889297485\n","Epoch\t 38 \tLoss\t 2.17851414619348 \tTime\t 64.06815457344055\n","Epoch\t 39 \tLoss\t 2.150092324232444 \tTime\t 63.978919982910156\n","Epoch\t 40 \tLoss\t 2.118345541220445 \tTime\t 63.977996587753296\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 41 \tLoss\t 2.11665029220092 \tTime\t 63.58594822883606\n","Epoch\t 42 \tLoss\t 2.107463861734439 \tTime\t 64.08129978179932\n","Epoch\t 43 \tLoss\t 2.1124956589478714 \tTime\t 63.87013220787048\n","Epoch\t 44 \tLoss\t 2.091941508880028 \tTime\t 63.97372913360596\n","Epoch\t 45 \tLoss\t 2.075543212279295 \tTime\t 64.12200784683228\n","Epoch\t 46 \tLoss\t 2.0647974038735413 \tTime\t 63.890501499176025\n","Epoch\t 47 \tLoss\t 2.062352635921576 \tTime\t 63.8083918094635\n","Epoch\t 48 \tLoss\t 2.036480087500352 \tTime\t 64.24328064918518\n","Epoch\t 49 \tLoss\t 2.0449444972551785 \tTime\t 64.54058527946472\n","Epoch\t 50 \tLoss\t 2.024141421684852 \tTime\t 63.25904107093811\n","Epoch\t 51 \tLoss\t 1.9893019994099934 \tTime\t 64.08169174194336\n","Epoch\t 52 \tLoss\t 2.0007903068493573 \tTime\t 63.50523114204407\n","Epoch\t 53 \tLoss\t 1.9745745976765952 \tTime\t 63.08320188522339\n","Epoch\t 54 \tLoss\t 1.9831030099819869 \tTime\t 63.18338489532471\n","Epoch\t 55 \tLoss\t 1.9655163165850518 \tTime\t 63.59653615951538\n","Epoch\t 56 \tLoss\t 1.9623993097207486 \tTime\t 63.74978303909302\n","Epoch\t 57 \tLoss\t 1.9672707820549988 \tTime\t 64.53323721885681\n","Epoch\t 58 \tLoss\t 1.9443902639242319 \tTime\t 63.53420281410217\n","Epoch\t 59 \tLoss\t 1.9525971858929365 \tTime\t 63.57099628448486\n","Epoch\t 60 \tLoss\t 1.9504731422815567 \tTime\t 64.03755450248718\n","Epoch\t 61 \tLoss\t 1.9485639914488182 \tTime\t 63.691707134246826\n","Epoch\t 62 \tLoss\t 1.9341564912062426 \tTime\t 64.70267009735107\n","Epoch\t 63 \tLoss\t 1.9131399943278387 \tTime\t 63.085745334625244\n","Epoch\t 64 \tLoss\t 1.914726349635002 \tTime\t 63.84032440185547\n","Epoch\t 65 \tLoss\t 1.9123503966209217 \tTime\t 63.698408126831055\n","Epoch\t 66 \tLoss\t 1.9221138819670065 \tTime\t 63.97363877296448\n","Epoch\t 67 \tLoss\t 1.9025355400183261 \tTime\t 64.15363311767578\n","Epoch\t 68 \tLoss\t 1.8969031132184542 \tTime\t 63.966599464416504\n","Epoch\t 69 \tLoss\t 1.8730856663141495 \tTime\t 62.997424364089966\n","Epoch\t 70 \tLoss\t 1.894952191450657 \tTime\t 63.639970779418945\n","Epoch\t 71 \tLoss\t 1.8787059239852122 \tTime\t 64.29042482376099\n","Epoch\t 72 \tLoss\t 1.8654224487451407 \tTime\t 64.18954944610596\n","Epoch\t 73 \tLoss\t 1.863854325734652 \tTime\t 64.33965110778809\n","Epoch\t 74 \tLoss\t 1.84140938550998 \tTime\t 64.13669013977051\n","Epoch\t 75 \tLoss\t 1.8646136675125513 \tTime\t 63.80938506126404\n","Epoch\t 76 \tLoss\t 1.8680549334257077 \tTime\t 63.90649652481079\n","Epoch\t 77 \tLoss\t 1.8576156188280155 \tTime\t 63.751327991485596\n","Epoch\t 78 \tLoss\t 1.8458795792017229 \tTime\t 63.85575032234192\n","Epoch\t 79 \tLoss\t 1.8490336681023622 \tTime\t 63.08734202384949\n","Epoch\t 80 \tLoss\t 1.8461720912884443 \tTime\t 63.93196153640747\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 81 \tLoss\t 1.8185661322031266 \tTime\t 63.66729998588562\n","Epoch\t 82 \tLoss\t 1.8224515896577103 \tTime\t 65.14960050582886\n","Epoch\t 83 \tLoss\t 1.8221440779857145 \tTime\t 63.94028377532959\n","Epoch\t 84 \tLoss\t 1.8342742155759761 \tTime\t 64.45501899719238\n","Epoch\t 85 \tLoss\t 1.8072746160702828 \tTime\t 63.8545982837677\n","Epoch\t 86 \tLoss\t 1.800465202331543 \tTime\t 63.98879909515381\n","Epoch\t 87 \tLoss\t 1.8101671836315056 \tTime\t 63.94135546684265\n","Epoch\t 88 \tLoss\t 1.7947565959050105 \tTime\t 63.880154848098755\n","Epoch\t 89 \tLoss\t 1.7895715120511178 \tTime\t 63.68267774581909\n","Epoch\t 90 \tLoss\t 1.7889426054098667 \tTime\t 64.25084805488586\n","Epoch\t 91 \tLoss\t 1.7834216472430107 \tTime\t 63.34745454788208\n","Epoch\t 92 \tLoss\t 1.7809810100457608 \tTime\t 63.77936863899231\n","Epoch\t 93 \tLoss\t 1.7881596112862612 \tTime\t 63.36652898788452\n","Epoch\t 94 \tLoss\t 1.7775107059723292 \tTime\t 63.189510107040405\n","Epoch\t 95 \tLoss\t 1.7610512605080237 \tTime\t 63.51588797569275\n","Epoch\t 96 \tLoss\t 1.7666247355632292 \tTime\t 64.41383695602417\n","Epoch\t 97 \tLoss\t 1.765118370912014 \tTime\t 64.08159041404724\n","Epoch\t 98 \tLoss\t 1.7528894925728822 \tTime\t 62.853808879852295\n","Epoch\t 99 \tLoss\t 1.7479428731478177 \tTime\t 64.25748991966248\n","Epoch\t 100 \tLoss\t 1.7467988185393504 \tTime\t 63.805323362350464\n","Epoch\t 101 \tLoss\t 1.7304786896094297 \tTime\t 63.43064069747925\n","Epoch\t 102 \tLoss\t 1.73890236891233 \tTime\t 64.40378713607788\n","Epoch\t 103 \tLoss\t 1.7313142525844085 \tTime\t 63.52308630943298\n","Epoch\t 104 \tLoss\t 1.7140335847169925 \tTime\t 63.283113956451416\n","Epoch\t 105 \tLoss\t 1.7146041674491688 \tTime\t 63.95498466491699\n","Epoch\t 106 \tLoss\t 1.7040032570178691 \tTime\t 64.27045965194702\n","Epoch\t 107 \tLoss\t 1.7255332867304485 \tTime\t 63.61910057067871\n","Epoch\t 108 \tLoss\t 1.7179547157043065 \tTime\t 63.4743013381958\n","Epoch\t 109 \tLoss\t 1.7252520261666713 \tTime\t 63.446796894073486\n","Epoch\t 110 \tLoss\t 1.703352845632113 \tTime\t 63.391019105911255\n","Epoch\t 111 \tLoss\t 1.695763850823427 \tTime\t 64.45743751525879\n","Epoch\t 112 \tLoss\t 1.7041843701631596 \tTime\t 64.1527009010315\n","Epoch\t 113 \tLoss\t 1.7050939168685522 \tTime\t 63.28516149520874\n","Epoch\t 114 \tLoss\t 1.6882930553876436 \tTime\t 63.194119691848755\n","Epoch\t 115 \tLoss\t 1.700034363453205 \tTime\t 63.69172263145447\n","Epoch\t 116 \tLoss\t 1.6819884471404247 \tTime\t 63.65391683578491\n","Epoch\t 117 \tLoss\t 1.6838400473961463 \tTime\t 64.69303274154663\n","Epoch\t 118 \tLoss\t 1.7059474223699325 \tTime\t 64.38876438140869\n","Epoch\t 119 \tLoss\t 1.6851432457948343 \tTime\t 64.15349245071411\n","Epoch\t 120 \tLoss\t 1.6829794199038774 \tTime\t 63.53369069099426\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 121 \tLoss\t 1.669088225486951 \tTime\t 64.44212198257446\n","Epoch\t 122 \tLoss\t 1.6827671350576938 \tTime\t 63.618752002716064\n","Epoch\t 123 \tLoss\t 1.6943070912972475 \tTime\t 63.18734550476074\n","Epoch\t 124 \tLoss\t 1.6637128866635835 \tTime\t 63.630905628204346\n","Epoch\t 125 \tLoss\t 1.6649484946177555 \tTime\t 64.27445101737976\n","Epoch\t 126 \tLoss\t 1.6536263429201565 \tTime\t 63.836371421813965\n","Epoch\t 127 \tLoss\t 1.658858307202657 \tTime\t 63.66014266014099\n","Epoch\t 128 \tLoss\t 1.6462472322659614 \tTime\t 64.42756700515747\n","Epoch\t 129 \tLoss\t 1.6530270197452643 \tTime\t 64.61596822738647\n","Epoch\t 130 \tLoss\t 1.6619491173670842 \tTime\t 63.7052743434906\n","Epoch\t 131 \tLoss\t 1.6618504059620394 \tTime\t 64.4921350479126\n","Epoch\t 132 \tLoss\t 1.6480444046167226 \tTime\t 63.51890444755554\n","Epoch\t 133 \tLoss\t 1.6382747320028452 \tTime\t 63.85265517234802\n","Epoch\t 134 \tLoss\t 1.621972287618197 \tTime\t 64.31536912918091\n","Epoch\t 135 \tLoss\t 1.6299166208658462 \tTime\t 63.96964621543884\n","Epoch\t 136 \tLoss\t 1.6166293669969607 \tTime\t 64.08307933807373\n","Epoch\t 137 \tLoss\t 1.6185196546407847 \tTime\t 63.83634948730469\n","Epoch\t 138 \tLoss\t 1.627243482760894 \tTime\t 64.26594042778015\n","Epoch\t 139 \tLoss\t 1.6401282811776186 \tTime\t 63.953967571258545\n","Epoch\t 140 \tLoss\t 1.6386917132597703 \tTime\t 64.35884141921997\n","Epoch\t 141 \tLoss\t 1.6146139419995822 \tTime\t 63.31014156341553\n","Epoch\t 142 \tLoss\t 1.6139069752815443 \tTime\t 63.41663980484009\n","Epoch\t 143 \tLoss\t 1.6194767646300487 \tTime\t 65.15248966217041\n","Epoch\t 144 \tLoss\t 1.6231731787706032 \tTime\t 64.14841961860657\n","Epoch\t 145 \tLoss\t 1.622492788388179 \tTime\t 62.94516134262085\n","Epoch\t 146 \tLoss\t 1.5931579638750126 \tTime\t 64.09203624725342\n","Epoch\t 147 \tLoss\t 1.6057150975251808 \tTime\t 64.86398148536682\n","Epoch\t 148 \tLoss\t 1.5952399981327545 \tTime\t 64.15151619911194\n","Epoch\t 149 \tLoss\t 1.6111205082673292 \tTime\t 64.04332065582275\n","Epoch\t 150 \tLoss\t 1.6143041415092272 \tTime\t 64.04772186279297\n","Epoch\t 151 \tLoss\t 1.5981088448793461 \tTime\t 64.63727331161499\n","Epoch\t 152 \tLoss\t 1.595061431786953 \tTime\t 63.308873891830444\n","Epoch\t 153 \tLoss\t 1.599761729974013 \tTime\t 64.21482682228088\n","Epoch\t 154 \tLoss\t 1.6067918863051978 \tTime\t 63.76849722862244\n","Epoch\t 155 \tLoss\t 1.5872754622728398 \tTime\t 64.34617829322815\n","Epoch\t 156 \tLoss\t 1.5870551738983545 \tTime\t 64.42809200286865\n","Epoch\t 157 \tLoss\t 1.5821379508727635 \tTime\t 63.67453718185425\n","Epoch\t 158 \tLoss\t 1.6006245729250785 \tTime\t 63.63871479034424\n","Epoch\t 159 \tLoss\t 1.5786961506574582 \tTime\t 63.348055362701416\n","Epoch\t 160 \tLoss\t 1.5851168993191842 \tTime\t 63.52002143859863\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 161 \tLoss\t 1.5700620259994116 \tTime\t 63.840537548065186\n","Epoch\t 162 \tLoss\t 1.5673708872917371 \tTime\t 64.05294060707092\n","Epoch\t 163 \tLoss\t 1.5767072494213397 \tTime\t 64.09890651702881\n","Epoch\t 164 \tLoss\t 1.5827037083796966 \tTime\t 64.33376717567444\n","Epoch\t 165 \tLoss\t 1.5562141766914954 \tTime\t 64.49663972854614\n","Epoch\t 166 \tLoss\t 1.5560838143030802 \tTime\t 63.87290549278259\n","Epoch\t 167 \tLoss\t 1.573487505545983 \tTime\t 64.50841045379639\n","Epoch\t 168 \tLoss\t 1.5733489287205231 \tTime\t 63.797670125961304\n","Epoch\t 169 \tLoss\t 1.563081606840476 \tTime\t 64.70704364776611\n","Epoch\t 170 \tLoss\t 1.55988509227068 \tTime\t 63.530110359191895\n","Epoch\t 171 \tLoss\t 1.5646925840622339 \tTime\t 63.9211049079895\n","Epoch\t 172 \tLoss\t 1.5642929767950988 \tTime\t 63.8215913772583\n","Epoch\t 173 \tLoss\t 1.5587476663100415 \tTime\t 63.66441583633423\n","Epoch\t 174 \tLoss\t 1.5525467444688845 \tTime\t 63.71360778808594\n","Epoch\t 175 \tLoss\t 1.5682681481043497 \tTime\t 64.55919361114502\n","Epoch\t 176 \tLoss\t 1.57881107880519 \tTime\t 63.9425950050354\n","Epoch\t 177 \tLoss\t 1.5764344686116927 \tTime\t 64.19555377960205\n","Epoch\t 178 \tLoss\t 1.5597496228340344 \tTime\t 63.734994888305664\n","Epoch\t 179 \tLoss\t 1.570108178945688 \tTime\t 64.24196195602417\n","Epoch\t 180 \tLoss\t 1.5576194066267748 \tTime\t 63.96605682373047\n","Epoch\t 181 \tLoss\t 1.5399324967310979 \tTime\t 64.49848484992981\n","Epoch\t 182 \tLoss\t 1.559557153017093 \tTime\t 63.97473764419556\n","Epoch\t 183 \tLoss\t 1.5523714664654853 \tTime\t 63.97998929023743\n","Epoch\t 184 \tLoss\t 1.5598741726997571 \tTime\t 64.18908643722534\n","Epoch\t 185 \tLoss\t 1.563012730769622 \tTime\t 63.8329644203186\n","Epoch\t 186 \tLoss\t 1.5407161413094936 \tTime\t 64.13095831871033\n","Epoch\t 187 \tLoss\t 1.5520761392055413 \tTime\t 65.63671278953552\n","Epoch\t 188 \tLoss\t 1.555868519269503 \tTime\t 64.09914422035217\n","Epoch\t 189 \tLoss\t 1.5372936688936674 \tTime\t 64.31707239151001\n","Epoch\t 190 \tLoss\t 1.5558269537412204 \tTime\t 64.32708263397217\n","Epoch\t 191 \tLoss\t 1.5424167957061377 \tTime\t 64.6140205860138\n","Epoch\t 192 \tLoss\t 1.5360436751292301 \tTime\t 63.897193908691406\n","Epoch\t 193 \tLoss\t 1.53612733009534 \tTime\t 63.73673391342163\n","Epoch\t 194 \tLoss\t 1.5314358637883112 \tTime\t 64.91165161132812\n","Epoch\t 195 \tLoss\t 1.5412776494637515 \tTime\t 64.58090925216675\n","Epoch\t 196 \tLoss\t 1.5482465267181396 \tTime\t 63.983463764190674\n","Epoch\t 197 \tLoss\t 1.5384092337045914 \tTime\t 63.684988021850586\n","Epoch\t 198 \tLoss\t 1.5389098821542202 \tTime\t 63.528027057647705\n","Epoch\t 199 \tLoss\t 1.5469485680262247 \tTime\t 63.52142643928528\n","Epoch\t 200 \tLoss\t 1.5515420901469694 \tTime\t 63.24096465110779\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Finished training. Train time was: 12772.359286785126\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 1 \tLoss\t 2.001463795930911 \tTime\t 16.825273513793945\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 2 \tLoss\t 1.6197971784151517 \tTime\t 16.792943239212036\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 3 \tLoss\t 1.4270895089858617 \tTime\t 16.77254343032837\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 4 \tLoss\t 1.3180175909629235 \tTime\t 16.763625144958496\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 5 \tLoss\t 1.248317821820577 \tTime\t 16.80354404449463\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 6 \tLoss\t 1.200075872739156 \tTime\t 16.833122730255127\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 7 \tLoss\t 1.1649135980850611 \tTime\t 16.79099678993225\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 8 \tLoss\t 1.1384548841378628 \tTime\t 16.797152996063232\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 9 \tLoss\t 1.1171735913325578 \tTime\t 16.805969715118408\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 10 \tLoss\t 1.1001262010672153 \tTime\t 16.84303641319275\n","Finished training. Train time was: 168.04420018196106\n","Accuracy : 62 %\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Epoch\t 1 \tLoss\t 6.341306243798671 \tTime\t 64.13438177108765\n","Epoch\t 2 \tLoss\t 5.973468095828325 \tTime\t 63.59566259384155\n","Epoch\t 3 \tLoss\t 5.597007159697704 \tTime\t 63.43206262588501\n","Epoch\t 4 \tLoss\t 5.15594510053977 \tTime\t 64.77828478813171\n","Epoch\t 5 \tLoss\t 4.658267270601713 \tTime\t 64.48962450027466\n","Epoch\t 6 \tLoss\t 4.2534118114373625 \tTime\t 64.31353068351746\n","Epoch\t 7 \tLoss\t 3.957806873321533 \tTime\t 63.10468888282776\n","Epoch\t 8 \tLoss\t 3.7569048478053166 \tTime\t 64.67569351196289\n","Epoch\t 9 \tLoss\t 3.6119164234552628 \tTime\t 64.19301223754883\n","Epoch\t 10 \tLoss\t 3.5098998950077935 \tTime\t 64.8723714351654\n","Epoch\t 11 \tLoss\t 3.3790538983467298 \tTime\t 64.11220979690552\n","Epoch\t 12 \tLoss\t 3.3052910768068755 \tTime\t 63.61104488372803\n","Epoch\t 13 \tLoss\t 3.20765745700934 \tTime\t 63.19343447685242\n","Epoch\t 14 \tLoss\t 3.1211383844033267 \tTime\t 63.41350269317627\n","Epoch\t 15 \tLoss\t 3.0660817488645895 \tTime\t 62.915401458740234\n","Epoch\t 16 \tLoss\t 3.0329517340048766 \tTime\t 64.15328931808472\n","Epoch\t 17 \tLoss\t 2.9659765769273805 \tTime\t 63.52668046951294\n","Epoch\t 18 \tLoss\t 2.9360579979725374 \tTime\t 63.335230350494385\n","Epoch\t 19 \tLoss\t 2.8961282399984505 \tTime\t 63.79762935638428\n","Epoch\t 20 \tLoss\t 2.867798942174667 \tTime\t 64.3540575504303\n","Epoch\t 21 \tLoss\t 2.8364056220421423 \tTime\t 62.99528455734253\n","Epoch\t 22 \tLoss\t 2.768197027842204 \tTime\t 63.42018365859985\n","Epoch\t 23 \tLoss\t 2.7269719221653084 \tTime\t 63.71562194824219\n","Epoch\t 24 \tLoss\t 2.719959062185043 \tTime\t 63.856693744659424\n","Epoch\t 25 \tLoss\t 2.6835272177671774 \tTime\t 64.15636873245239\n","Epoch\t 26 \tLoss\t 2.643130115362314 \tTime\t 64.22685241699219\n","Epoch\t 27 \tLoss\t 2.60196547019176 \tTime\t 63.26179003715515\n","Epoch\t 28 \tLoss\t 2.594453168526674 \tTime\t 64.2793390750885\n","Epoch\t 29 \tLoss\t 2.5669334289355157 \tTime\t 62.93740153312683\n","Epoch\t 30 \tLoss\t 2.551110035333878 \tTime\t 62.992661237716675\n","Epoch\t 31 \tLoss\t 2.53212231978392 \tTime\t 64.12017226219177\n","Epoch\t 32 \tLoss\t 2.533926016245133 \tTime\t 63.18554902076721\n","Epoch\t 33 \tLoss\t 2.4978615320645847 \tTime\t 64.58841252326965\n","Epoch\t 34 \tLoss\t 2.474193866436298 \tTime\t 64.32345390319824\n","Epoch\t 35 \tLoss\t 2.4731533160576453 \tTime\t 64.62796330451965\n","Epoch\t 36 \tLoss\t 2.453781601098868 \tTime\t 64.01773929595947\n","Epoch\t 37 \tLoss\t 2.437344655012473 \tTime\t 62.58398461341858\n","Epoch\t 38 \tLoss\t 2.430796576768924 \tTime\t 63.503475189208984\n","Epoch\t 39 \tLoss\t 2.41114361958626 \tTime\t 64.7509560585022\n","Epoch\t 40 \tLoss\t 2.4036467723357373 \tTime\t 63.23285412788391\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 41 \tLoss\t 2.4034975320864946 \tTime\t 63.28593945503235\n","Epoch\t 42 \tLoss\t 2.359349984389085 \tTime\t 63.26135015487671\n","Epoch\t 43 \tLoss\t 2.360639756765121 \tTime\t 62.66153335571289\n","Epoch\t 44 \tLoss\t 2.3643425648029033 \tTime\t 63.29718613624573\n","Epoch\t 45 \tLoss\t 2.3547023993272047 \tTime\t 63.65616226196289\n","Epoch\t 46 \tLoss\t 2.344625005355248 \tTime\t 63.06201887130737\n","Epoch\t 47 \tLoss\t 2.34498707147745 \tTime\t 63.477590560913086\n","Epoch\t 48 \tLoss\t 2.343995578472431 \tTime\t 63.35493993759155\n","Epoch\t 49 \tLoss\t 2.315882849693298 \tTime\t 64.09211826324463\n","Epoch\t 50 \tLoss\t 2.318972761203081 \tTime\t 64.07200193405151\n","Epoch\t 51 \tLoss\t 2.3102066743068206 \tTime\t 64.43996953964233\n","Epoch\t 52 \tLoss\t 2.292042675996438 \tTime\t 64.09117388725281\n","Epoch\t 53 \tLoss\t 2.295051643787286 \tTime\t 63.66645812988281\n","Epoch\t 54 \tLoss\t 2.2867501912972865 \tTime\t 63.64081931114197\n","Epoch\t 55 \tLoss\t 2.2718224928929254 \tTime\t 63.66342282295227\n","Epoch\t 56 \tLoss\t 2.2630830177894006 \tTime\t 63.88864493370056\n","Epoch\t 57 \tLoss\t 2.2748235409076396 \tTime\t 63.76660442352295\n","Epoch\t 58 \tLoss\t 2.265563544860253 \tTime\t 63.63306736946106\n","Epoch\t 59 \tLoss\t 2.244238900526976 \tTime\t 63.57522201538086\n","Epoch\t 60 \tLoss\t 2.2582060758884137 \tTime\t 63.572497606277466\n","Epoch\t 61 \tLoss\t 2.242702558101752 \tTime\t 64.35579895973206\n","Epoch\t 62 \tLoss\t 2.232309287022322 \tTime\t 63.84602952003479\n","Epoch\t 63 \tLoss\t 2.240054848255255 \tTime\t 63.772050619125366\n","Epoch\t 64 \tLoss\t 2.236614040228037 \tTime\t 65.286203622818\n","Epoch\t 65 \tLoss\t 2.21612836825542 \tTime\t 64.36981534957886\n","Epoch\t 66 \tLoss\t 2.2084957856398364 \tTime\t 64.33287310600281\n","Epoch\t 67 \tLoss\t 2.18836529866243 \tTime\t 63.732911109924316\n","Epoch\t 68 \tLoss\t 2.18831262527368 \tTime\t 64.34275221824646\n","Epoch\t 69 \tLoss\t 2.2079415730940988 \tTime\t 63.862099409103394\n","Epoch\t 70 \tLoss\t 2.2011190090423973 \tTime\t 63.56932091712952\n","Epoch\t 71 \tLoss\t 2.1845395216575034 \tTime\t 62.76372528076172\n","Epoch\t 72 \tLoss\t 2.1899431986686513 \tTime\t 63.813629150390625\n","Epoch\t 73 \tLoss\t 2.186216613573906 \tTime\t 63.83820867538452\n","Epoch\t 74 \tLoss\t 2.1802949862602428 \tTime\t 63.21827673912048\n","Epoch\t 75 \tLoss\t 2.182690475537227 \tTime\t 63.80711317062378\n","Epoch\t 76 \tLoss\t 2.176079679146791 \tTime\t 63.81359148025513\n","Epoch\t 77 \tLoss\t 2.142798176789895 \tTime\t 63.27051043510437\n","Epoch\t 78 \tLoss\t 2.1534472074264137 \tTime\t 63.948054790496826\n","Epoch\t 79 \tLoss\t 2.142918570836385 \tTime\t 64.05002188682556\n","Epoch\t 80 \tLoss\t 2.152457890755091 \tTime\t 63.45334243774414\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 81 \tLoss\t 2.1327877558194674 \tTime\t 63.96573281288147\n","Epoch\t 82 \tLoss\t 2.15203900826283 \tTime\t 64.23272848129272\n","Epoch\t 83 \tLoss\t 2.1313041521952703 \tTime\t 62.180354595184326\n","Epoch\t 84 \tLoss\t 2.1455231880530334 \tTime\t 63.57674431800842\n","Epoch\t 85 \tLoss\t 2.127262470049736 \tTime\t 63.976210594177246\n","Epoch\t 86 \tLoss\t 2.1400272882901703 \tTime\t 63.538113594055176\n","Epoch\t 87 \tLoss\t 2.1194270647489106 \tTime\t 63.357250928878784\n","Epoch\t 88 \tLoss\t 2.096137960140522 \tTime\t 65.17847323417664\n","Epoch\t 89 \tLoss\t 2.1186703284581503 \tTime\t 65.72602558135986\n","Epoch\t 90 \tLoss\t 2.1019927562811436 \tTime\t 64.9207456111908\n","Epoch\t 91 \tLoss\t 2.1113396039375893 \tTime\t 63.9456627368927\n","Epoch\t 92 \tLoss\t 2.116530065658765 \tTime\t 64.16085743904114\n","Epoch\t 93 \tLoss\t 2.0842543155719073 \tTime\t 65.02184796333313\n","Epoch\t 94 \tLoss\t 2.1113255262374877 \tTime\t 64.04808759689331\n","Epoch\t 95 \tLoss\t 2.091821521367782 \tTime\t 63.605713844299316\n","Epoch\t 96 \tLoss\t 2.0901374688515295 \tTime\t 63.22811436653137\n","Epoch\t 97 \tLoss\t 2.0804363684776503 \tTime\t 62.90089535713196\n","Epoch\t 98 \tLoss\t 2.084397152142647 \tTime\t 64.10167026519775\n","Epoch\t 99 \tLoss\t 2.079777194903447 \tTime\t 63.86792206764221\n","Epoch\t 100 \tLoss\t 2.076416263824854 \tTime\t 63.82775115966797\n","Epoch\t 101 \tLoss\t 2.072623387361184 \tTime\t 63.52314066886902\n","Epoch\t 102 \tLoss\t 2.0730730001743023 \tTime\t 64.44374775886536\n","Epoch\t 103 \tLoss\t 2.0681495201893343 \tTime\t 63.837154388427734\n","Epoch\t 104 \tLoss\t 2.0536465687629506 \tTime\t 63.25102877616882\n","Epoch\t 105 \tLoss\t 2.0725818652373094 \tTime\t 64.31875872612\n","Epoch\t 106 \tLoss\t 2.0512323691294743 \tTime\t 64.42122840881348\n","Epoch\t 107 \tLoss\t 2.048749209061647 \tTime\t 64.23633909225464\n","Epoch\t 108 \tLoss\t 2.0564580941811585 \tTime\t 62.744213581085205\n","Epoch\t 109 \tLoss\t 2.0410228717021455 \tTime\t 63.91467213630676\n","Epoch\t 110 \tLoss\t 2.0341587390655125 \tTime\t 63.76590895652771\n","Epoch\t 111 \tLoss\t 2.043169093743349 \tTime\t 62.78590965270996\n","Epoch\t 112 \tLoss\t 2.042292188986754 \tTime\t 63.21607542037964\n","Epoch\t 113 \tLoss\t 2.044475714976971 \tTime\t 63.64384746551514\n","Epoch\t 114 \tLoss\t 2.034710549085568 \tTime\t 62.647170066833496\n","Epoch\t 115 \tLoss\t 2.037847366699806 \tTime\t 62.91179847717285\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"x3d0z9bo9ypA","executionInfo":{"status":"error","timestamp":1601107759059,"user_tz":-540,"elapsed":7980442,"user":{"displayName":"서준원","photoUrl":"","userId":"09602630791249428793"}},"outputId":"b5ae2155-a776-4a2a-a2fa-48481b10917d","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["batch_size_list = [64, 128, 512, 1024]\n","temperature_list = [0.1, 0.5, 1.0]\n","n_epoch_list = [100]\n","\n","batch_loss =[]\n","temperature_loss =[]\n","epoch_loss =[]\n","\n","batch_acc =[]\n","temperature_acc =[]\n","epoch_acc =[]\n","\n","\n","for t in temperature_list :\n","  net = SimCLRNet(26, 1, 10, 32)\n","  net.cuda()\n","  net.zero_grad()\n","  tl, ll, testl = make_loader(256)\n","  loss = train_param(net, tl, temperature=t)\n","  temperature_loss.append(loss)\n","  train_classifier(net, ll)\n","  acc = test_classifier(net, testl)\n","  temperature_acc.append((t, acc))\n","  save_model(net, \"simclr_tmp_{}.pt\".format(t))\n","\n","for n in n_epoch_list :\n","  net = SimCLRNet(26, 1, 10, 32)\n","  net.cuda()\n","  net.zero_grad()\n","  tl, ll, testl = make_loader(256)\n","  loss = train_param(net, tl, n_epoch=n)\n","  epoch_loss.append(loss)\n","  train_classifier(net, ll)\n","  acc = test_classifier(net, testl)\n","  epoch_acc.append((n, acc))\n","  save_model(net, \"simclr_epoch_{}.pt\".format(n))"],"execution_count":23,"outputs":[{"output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","loaded 40\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","loaded 80\n","Epoch\t 81 \tLoss\t 2.178968620300293 \tTime\t 62.29389500617981\n","Epoch\t 82 \tLoss\t 2.082356288494208 \tTime\t 62.88844108581543\n","Epoch\t 83 \tLoss\t 2.060207517330463 \tTime\t 63.251840114593506\n","Epoch\t 84 \tLoss\t 2.0695359609065913 \tTime\t 63.27787804603577\n","Epoch\t 85 \tLoss\t 2.0567153062575905 \tTime\t 63.53095364570618\n","Epoch\t 86 \tLoss\t 2.0828705677619346 \tTime\t 63.81547236442566\n","Epoch\t 87 \tLoss\t 2.0742332696914674 \tTime\t 64.66506147384644\n","Epoch\t 88 \tLoss\t 2.084212780610109 \tTime\t 65.44819641113281\n","Epoch\t 89 \tLoss\t 2.0976177172783093 \tTime\t 64.92107367515564\n","Epoch\t 90 \tLoss\t 2.091535634872241 \tTime\t 65.26742792129517\n","Epoch\t 91 \tLoss\t 2.0919578839571047 \tTime\t 66.12830400466919\n","Epoch\t 92 \tLoss\t 2.1053925110743594 \tTime\t 66.64318370819092\n","Epoch\t 93 \tLoss\t 2.100393770902585 \tTime\t 66.6978030204773\n","Epoch\t 94 \tLoss\t 2.1310845735745554 \tTime\t 67.11984848976135\n","Epoch\t 95 \tLoss\t 2.1166466541779347 \tTime\t 66.90096592903137\n","Epoch\t 96 \tLoss\t 2.113542616673005 \tTime\t 67.39053916931152\n","Epoch\t 97 \tLoss\t 2.132626604422545 \tTime\t 67.26896381378174\n","Epoch\t 98 \tLoss\t 2.1288250061181877 \tTime\t 67.1700439453125\n","Epoch\t 99 \tLoss\t 2.1507767555041193 \tTime\t 64.60316514968872\n","Epoch\t 100 \tLoss\t 2.135061149719434 \tTime\t 63.769022703170776\n","Epoch\t 101 \tLoss\t 2.140837457852486 \tTime\t 63.19363737106323\n","Epoch\t 102 \tLoss\t 2.1429423656219093 \tTime\t 63.23184394836426\n","Epoch\t 103 \tLoss\t 2.1384938704661836 \tTime\t 63.151546478271484\n","Epoch\t 104 \tLoss\t 2.124306000196017 \tTime\t 62.32203912734985\n","Epoch\t 105 \tLoss\t 2.1382121067780715 \tTime\t 63.051714181900024\n","Epoch\t 106 \tLoss\t 2.1305894246468178 \tTime\t 62.74639105796814\n","Epoch\t 107 \tLoss\t 2.108778732862228 \tTime\t 63.55216145515442\n","Epoch\t 108 \tLoss\t 2.11607965811705 \tTime\t 63.78844475746155\n","Epoch\t 109 \tLoss\t 2.1249536049671662 \tTime\t 64.06362581253052\n","Epoch\t 110 \tLoss\t 2.125180287238879 \tTime\t 64.79309606552124\n","Epoch\t 111 \tLoss\t 2.1118272549066788 \tTime\t 64.4955108165741\n","Epoch\t 112 \tLoss\t 2.1092278376603737 \tTime\t 65.54993534088135\n","Epoch\t 113 \tLoss\t 2.1039476957076637 \tTime\t 66.15623807907104\n","Epoch\t 114 \tLoss\t 2.115820227525173 \tTime\t 66.60842776298523\n","Epoch\t 115 \tLoss\t 2.0906074242714126 \tTime\t 67.14069437980652\n","Epoch\t 116 \tLoss\t 2.0725439322300447 \tTime\t 66.9214940071106\n","Epoch\t 117 \tLoss\t 2.097091399706327 \tTime\t 66.44339919090271\n","Epoch\t 118 \tLoss\t 2.08755038579305 \tTime\t 66.96096229553223\n","Epoch\t 119 \tLoss\t 2.0982956849611725 \tTime\t 67.47369170188904\n","Epoch\t 120 \tLoss\t 2.0702781365467953 \tTime\t 67.20946478843689\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 121 \tLoss\t 2.093996963134179 \tTime\t 68.16304230690002\n","Epoch\t 122 \tLoss\t 2.080740092962216 \tTime\t 68.97336268424988\n","Epoch\t 123 \tLoss\t 2.0916080462626923 \tTime\t 68.44797492027283\n","Epoch\t 124 \tLoss\t 2.0927633297749053 \tTime\t 66.64707565307617\n","Epoch\t 125 \tLoss\t 2.0641457789983506 \tTime\t 64.43538284301758\n","Epoch\t 126 \tLoss\t 2.0729048845095512 \tTime\t 64.31140780448914\n","Epoch\t 127 \tLoss\t 2.0629914234846067 \tTime\t 64.88215637207031\n","Epoch\t 128 \tLoss\t 2.061450051038693 \tTime\t 65.15360617637634\n","Epoch\t 129 \tLoss\t 2.0431922380740826 \tTime\t 64.52837085723877\n","Epoch\t 130 \tLoss\t 2.0594277950433586 \tTime\t 63.53483605384827\n","Epoch\t 131 \tLoss\t 2.0356379417272716 \tTime\t 63.544188022613525\n","Epoch\t 132 \tLoss\t 2.050757228411161 \tTime\t 63.132447957992554\n","Epoch\t 133 \tLoss\t 2.045255048458393 \tTime\t 63.18402147293091\n","Epoch\t 134 \tLoss\t 2.0279432027767865 \tTime\t 63.52095603942871\n","Epoch\t 135 \tLoss\t 2.0337081261170216 \tTime\t 63.72516989707947\n","Epoch\t 136 \tLoss\t 2.0394795986322256 \tTime\t 64.87118101119995\n","Epoch\t 137 \tLoss\t 2.0363484853353255 \tTime\t 64.8786461353302\n","Epoch\t 138 \tLoss\t 2.05314426727784 \tTime\t 64.47175526618958\n","Epoch\t 139 \tLoss\t 2.0211602168205456 \tTime\t 65.04777073860168\n","Epoch\t 140 \tLoss\t 2.0176465321809816 \tTime\t 65.70844626426697\n","Epoch\t 141 \tLoss\t 2.024945249924293 \tTime\t 66.72589182853699\n","Epoch\t 142 \tLoss\t 2.0333966022882706 \tTime\t 66.3244776725769\n","Epoch\t 143 \tLoss\t 2.030195923340626 \tTime\t 66.5762300491333\n","Epoch\t 144 \tLoss\t 2.0264441300661136 \tTime\t 67.40001964569092\n","Epoch\t 145 \tLoss\t 2.004581494209094 \tTime\t 67.67677307128906\n","Epoch\t 146 \tLoss\t 2.006215166434264 \tTime\t 67.67881727218628\n","Epoch\t 147 \tLoss\t 2.0043577946149385 \tTime\t 68.60014629364014\n","Epoch\t 148 \tLoss\t 2.0060321838427813 \tTime\t 68.41445064544678\n","Epoch\t 149 \tLoss\t 2.0054766373756605 \tTime\t 68.54278421401978\n","Epoch\t 150 \tLoss\t 2.0158401128573296 \tTime\t 68.94708013534546\n","Epoch\t 151 \tLoss\t 2.0041321986760847 \tTime\t 68.35235786437988\n","Epoch\t 152 \tLoss\t 2.019984734364045 \tTime\t 68.6632125377655\n","Epoch\t 153 \tLoss\t 2.00410814957741 \tTime\t 68.48820877075195\n","Epoch\t 154 \tLoss\t 1.982378219335507 \tTime\t 68.71432614326477\n","Epoch\t 155 \tLoss\t 1.9935170039152488 \tTime\t 66.36590075492859\n","Epoch\t 156 \tLoss\t 1.9844158178720719 \tTime\t 64.94013500213623\n","Epoch\t 157 \tLoss\t 1.9936943610509237 \tTime\t 64.95542454719543\n","Epoch\t 158 \tLoss\t 1.9999816827284984 \tTime\t 64.8128354549408\n","Epoch\t 159 \tLoss\t 1.9713140585483648 \tTime\t 64.9020516872406\n","Epoch\t 160 \tLoss\t 1.9583051779331304 \tTime\t 64.88891673088074\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 161 \tLoss\t 1.9899483589025644 \tTime\t 64.39931559562683\n","Epoch\t 162 \tLoss\t 1.9829180369010337 \tTime\t 63.99777436256409\n","Epoch\t 163 \tLoss\t 1.975595436952053 \tTime\t 64.15015721321106\n","Epoch\t 164 \tLoss\t 1.9807445960167127 \tTime\t 64.57794713973999\n","Epoch\t 165 \tLoss\t 1.9650451061053154 \tTime\t 64.86471581459045\n","Epoch\t 166 \tLoss\t 1.9816240127270037 \tTime\t 64.80099844932556\n","Epoch\t 167 \tLoss\t 1.9732438833285602 \tTime\t 64.22671365737915\n","Epoch\t 168 \tLoss\t 1.9657743979723026 \tTime\t 64.1900646686554\n","Epoch\t 169 \tLoss\t 1.965262332940713 \tTime\t 64.505047082901\n","Epoch\t 170 \tLoss\t 1.951732950944167 \tTime\t 64.50572419166565\n","Epoch\t 171 \tLoss\t 1.9692279644501516 \tTime\t 64.35019111633301\n","Epoch\t 172 \tLoss\t 1.9452374091515174 \tTime\t 64.35910058021545\n","Epoch\t 173 \tLoss\t 1.9689082922079624 \tTime\t 64.37501573562622\n","Epoch\t 174 \tLoss\t 1.9595964431762696 \tTime\t 64.86566543579102\n","Epoch\t 175 \tLoss\t 1.9448895448293442 \tTime\t 64.42207884788513\n","Epoch\t 176 \tLoss\t 1.9579661326530653 \tTime\t 64.1908905506134\n","Epoch\t 177 \tLoss\t 1.948644106204693 \tTime\t 63.9625768661499\n","Epoch\t 178 \tLoss\t 1.9430815623356745 \tTime\t 64.49852323532104\n","Epoch\t 179 \tLoss\t 1.9337024236336733 \tTime\t 64.19885563850403\n","Epoch\t 180 \tLoss\t 1.9373068895095433 \tTime\t 63.885704040527344\n","Epoch\t 181 \tLoss\t 1.9454807275380843 \tTime\t 63.762022733688354\n","Epoch\t 182 \tLoss\t 1.9482088724772135 \tTime\t 63.28245520591736\n","Epoch\t 183 \tLoss\t 1.9386621579145773 \tTime\t 64.24939703941345\n","Epoch\t 184 \tLoss\t 1.930284656622471 \tTime\t 63.47437334060669\n","Epoch\t 185 \tLoss\t 1.9383653530707725 \tTime\t 63.67717218399048\n","Epoch\t 186 \tLoss\t 1.9321132287001 \tTime\t 63.537373065948486\n","Epoch\t 187 \tLoss\t 1.9390450520393177 \tTime\t 63.25726628303528\n","Epoch\t 188 \tLoss\t 1.923945330350827 \tTime\t 64.03968334197998\n","Epoch\t 189 \tLoss\t 1.9488012460561899 \tTime\t 63.65854239463806\n","Epoch\t 190 \tLoss\t 1.9480256257913051 \tTime\t 63.674166440963745\n","Epoch\t 191 \tLoss\t 1.9233791063993404 \tTime\t 63.84453010559082\n","Epoch\t 192 \tLoss\t 1.9193022654606746 \tTime\t 63.600783586502075\n","Epoch\t 193 \tLoss\t 1.914494969905951 \tTime\t 63.933117151260376\n","Epoch\t 194 \tLoss\t 1.9170460474796784 \tTime\t 63.486143827438354\n","Epoch\t 195 \tLoss\t 1.9162637955103166 \tTime\t 63.29292321205139\n","Epoch\t 196 \tLoss\t 1.9046126225055793 \tTime\t 63.26140999794006\n","Epoch\t 197 \tLoss\t 1.9193814418254755 \tTime\t 63.971489667892456\n","Epoch\t 198 \tLoss\t 1.9277279150791657 \tTime\t 63.83590245246887\n","Epoch\t 199 \tLoss\t 1.9167229310060159 \tTime\t 63.67928910255432\n","Epoch\t 200 \tLoss\t 1.9189849089353512 \tTime\t 63.7672438621521\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Finished training. Train time was: 7805.888730049133\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 1 \tLoss\t 1.9507682861425937 \tTime\t 16.93717098236084\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 2 \tLoss\t 1.5096183911348 \tTime\t 16.64607810974121\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 3 \tLoss\t 1.3165377849187607 \tTime\t 16.594112634658813\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 4 \tLoss\t 1.2126181859236498 \tTime\t 16.583900928497314\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 5 \tLoss\t 1.1486623843510946 \tTime\t 16.590698719024658\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 6 \tLoss\t 1.106095722394112 \tTime\t 16.622675895690918\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 7 \tLoss\t 1.0737746183688823 \tTime\t 16.634438037872314\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 8 \tLoss\t 1.0513960712995285 \tTime\t 16.624972820281982\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 9 \tLoss\t 1.032992812914726 \tTime\t 16.66338086128235\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 10 \tLoss\t 1.0181874113205152 \tTime\t 16.65352153778076\n","Finished training. Train time was: 166.56597995758057\n","Accuracy : 65 %\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Files already downloaded and verified\n"],"name":"stdout"},{"output_type":"error","ename":"UnboundLocalError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-bf5a42ae4fee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0mtl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m   \u001b[0mtemperature_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0mtrain_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-22-d17228c61f56>\u001b[0m in \u001b[0;36mtrain_param\u001b[0;34m(net, loader, batch_size, temperature, n_epoch)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loaded\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'x' referenced before assignment"]}]},{"cell_type":"code","metadata":{"id":"gZcz_3y5jhTZ","executionInfo":{"status":"ok","timestamp":1601165216613,"user_tz":-540,"elapsed":28012127,"user":{"displayName":"서준원","photoUrl":"","userId":"09602630791249428793"}},"outputId":"fe70a32f-f051-4036-d135-735352641dad","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["batch_size_list = [64, 128, 512, 1024]\n","temperature_list = [0.5, 1.0]\n","n_epoch_list = [100]\n","\n","batch_loss =[]\n","temperature_loss =[]\n","epoch_loss =[]\n","\n","batch_acc =[]\n","temperature_acc =[]\n","epoch_acc =[]\n","\n","\n","for t in temperature_list :\n","  net = SimCLRNet(26, 1, 10, 32)\n","  net.cuda()\n","  net.zero_grad()\n","  tl, ll, testl = make_loader(256)\n","  loss = train_param(net, tl, temperature=t)\n","  temperature_loss.append(loss)\n","  train_classifier(net, ll)\n","  acc = test_classifier(net, testl)\n","  temperature_acc.append((t, acc))\n","  save_model(net, \"simclr_tmp_{}.pt\".format(t))\n","\n","for n in n_epoch_list :\n","  net = SimCLRNet(26, 1, 10, 32)\n","  net.cuda()\n","  net.zero_grad()\n","  tl, ll, testl = make_loader(256)\n","  loss = train_param(net, tl, n_epoch=n)\n","  epoch_loss.append(loss)\n","  train_classifier(net, ll)\n","  acc = test_classifier(net, testl)\n","  epoch_acc.append((n, acc))\n","  save_model(net, \"simclr_epoch_{}.pt\".format(n))"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","loaded 40\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","loaded 80\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:46: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch\t 81 \tLoss\t 4.941969340886825 \tTime\t 64.51828837394714\n","Epoch\t 82 \tLoss\t 4.93207420202402 \tTime\t 64.71930456161499\n","Epoch\t 83 \tLoss\t 4.926310676183456 \tTime\t 64.86624598503113\n","Epoch\t 84 \tLoss\t 4.925696483025184 \tTime\t 65.27939987182617\n","Epoch\t 85 \tLoss\t 4.927167601463122 \tTime\t 64.45504570007324\n","Epoch\t 86 \tLoss\t 4.926168950398763 \tTime\t 64.0670120716095\n","Epoch\t 87 \tLoss\t 4.9273407838283445 \tTime\t 65.56034660339355\n","Epoch\t 88 \tLoss\t 4.931159488971417 \tTime\t 65.62299418449402\n","Epoch\t 89 \tLoss\t 4.932139687660413 \tTime\t 64.4908537864685\n","Epoch\t 90 \tLoss\t 4.933412109277187 \tTime\t 64.20229530334473\n","Epoch\t 91 \tLoss\t 4.930461343129476 \tTime\t 64.45590233802795\n","Epoch\t 92 \tLoss\t 4.931400162134415 \tTime\t 64.93443393707275\n","Epoch\t 93 \tLoss\t 4.935975607847556 \tTime\t 64.6029748916626\n","Epoch\t 94 \tLoss\t 4.9353855817745895 \tTime\t 64.23508906364441\n","Epoch\t 95 \tLoss\t 4.937181291824732 \tTime\t 64.87706089019775\n","Epoch\t 96 \tLoss\t 4.93645325196095 \tTime\t 64.37103390693665\n","Epoch\t 97 \tLoss\t 4.941366943946251 \tTime\t 64.83371067047119\n","Epoch\t 98 \tLoss\t 4.9413639777745955 \tTime\t 64.31758570671082\n","Epoch\t 99 \tLoss\t 4.941401701707107 \tTime\t 63.66099190711975\n","Epoch\t 100 \tLoss\t 4.940532718560634 \tTime\t 64.30436587333679\n","Epoch\t 101 \tLoss\t 4.943647401760786 \tTime\t 64.85635089874268\n","Epoch\t 102 \tLoss\t 4.941836697016007 \tTime\t 64.56135272979736\n","Epoch\t 103 \tLoss\t 4.943064721425374 \tTime\t 64.94230365753174\n","Epoch\t 104 \tLoss\t 4.943058287791716 \tTime\t 65.50526738166809\n","Epoch\t 105 \tLoss\t 4.939786094274276 \tTime\t 64.41813683509827\n","Epoch\t 106 \tLoss\t 4.943338369711851 \tTime\t 64.18212175369263\n","Epoch\t 107 \tLoss\t 4.94333010942508 \tTime\t 64.94816946983337\n","Epoch\t 108 \tLoss\t 4.940068905170147 \tTime\t 64.23552203178406\n","Epoch\t 109 \tLoss\t 4.940179922641852 \tTime\t 65.38045501708984\n","Epoch\t 110 \tLoss\t 4.93827815422645 \tTime\t 65.25809073448181\n","Epoch\t 111 \tLoss\t 4.938184288220528 \tTime\t 64.47715282440186\n","Epoch\t 112 \tLoss\t 4.938441000229273 \tTime\t 65.52691864967346\n","Epoch\t 113 \tLoss\t 4.934852265088987 \tTime\t 64.8269579410553\n","Epoch\t 114 \tLoss\t 4.9374681961842075 \tTime\t 65.1388430595398\n","Epoch\t 115 \tLoss\t 4.938585420755239 \tTime\t 65.5632688999176\n","Epoch\t 116 \tLoss\t 4.935343554081061 \tTime\t 65.16850137710571\n","Epoch\t 117 \tLoss\t 4.930970150385147 \tTime\t 65.0024025440216\n","Epoch\t 118 \tLoss\t 4.931826197795379 \tTime\t 65.96429824829102\n","Epoch\t 119 \tLoss\t 4.930255840986203 \tTime\t 64.66776013374329\n","Epoch\t 120 \tLoss\t 4.93155180368668 \tTime\t 64.40596842765808\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 121 \tLoss\t 4.931347492413643 \tTime\t 63.4717230796814\n","Epoch\t 122 \tLoss\t 4.927108033498128 \tTime\t 63.73965787887573\n","Epoch\t 123 \tLoss\t 4.9299757761833 \tTime\t 64.83683443069458\n","Epoch\t 124 \tLoss\t 4.930701011266464 \tTime\t 64.94246339797974\n","Epoch\t 125 \tLoss\t 4.931987006847675 \tTime\t 65.84378433227539\n","Epoch\t 126 \tLoss\t 4.92512759184226 \tTime\t 65.5116617679596\n","Epoch\t 127 \tLoss\t 4.929942087026743 \tTime\t 64.59065985679626\n","Epoch\t 128 \tLoss\t 4.928873018118051 \tTime\t 65.6144630908966\n","Epoch\t 129 \tLoss\t 4.925796543023525 \tTime\t 64.83882761001587\n","Epoch\t 130 \tLoss\t 4.927191159664056 \tTime\t 65.12170338630676\n","Epoch\t 131 \tLoss\t 4.92599091896644 \tTime\t 65.05239677429199\n","Epoch\t 132 \tLoss\t 4.921868312053191 \tTime\t 65.09977912902832\n","Epoch\t 133 \tLoss\t 4.924328826023982 \tTime\t 65.47206926345825\n","Epoch\t 134 \tLoss\t 4.919657416221423 \tTime\t 64.93592381477356\n","Epoch\t 135 \tLoss\t 4.925064568641858 \tTime\t 64.20741271972656\n","Epoch\t 136 \tLoss\t 4.923705812600943 \tTime\t 64.88146376609802\n","Epoch\t 137 \tLoss\t 4.9216891142038195 \tTime\t 63.897947549819946\n","Epoch\t 138 \tLoss\t 4.918602197598188 \tTime\t 65.62403845787048\n","Epoch\t 139 \tLoss\t 4.919628561460055 \tTime\t 64.60545706748962\n","Epoch\t 140 \tLoss\t 4.921269900982196 \tTime\t 65.91750884056091\n","Epoch\t 141 \tLoss\t 4.920854118542794 \tTime\t 64.30815124511719\n","Epoch\t 142 \tLoss\t 4.917471000475762 \tTime\t 65.21004676818848\n","Epoch\t 143 \tLoss\t 4.91660037896572 \tTime\t 64.9749128818512\n","Epoch\t 144 \tLoss\t 4.919658047113663 \tTime\t 63.76404285430908\n","Epoch\t 145 \tLoss\t 4.913154988411145 \tTime\t 64.51474213600159\n","Epoch\t 146 \tLoss\t 4.914134409488776 \tTime\t 64.33461427688599\n","Epoch\t 147 \tLoss\t 4.914322090148926 \tTime\t 64.73578667640686\n","Epoch\t 148 \tLoss\t 4.911975207695594 \tTime\t 64.8582558631897\n","Epoch\t 149 \tLoss\t 4.914187262608455 \tTime\t 63.59927415847778\n","Epoch\t 150 \tLoss\t 4.9129059693752195 \tTime\t 64.34554433822632\n","Epoch\t 151 \tLoss\t 4.916711795024383 \tTime\t 64.93846702575684\n","Epoch\t 152 \tLoss\t 4.909422801091121 \tTime\t 64.09235119819641\n","Epoch\t 153 \tLoss\t 4.913428482642541 \tTime\t 64.07409858703613\n","Epoch\t 154 \tLoss\t 4.9118863570384494 \tTime\t 64.26659774780273\n","Epoch\t 155 \tLoss\t 4.910605785174248 \tTime\t 64.34454846382141\n","Epoch\t 156 \tLoss\t 4.907257999517979 \tTime\t 64.21580219268799\n","Epoch\t 157 \tLoss\t 4.9104502213306915 \tTime\t 64.07832288742065\n","Epoch\t 158 \tLoss\t 4.907703597729022 \tTime\t 64.16349697113037\n","Epoch\t 159 \tLoss\t 4.910893895075872 \tTime\t 64.85307836532593\n","Epoch\t 160 \tLoss\t 4.908172079233023 \tTime\t 64.5110137462616\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 161 \tLoss\t 4.908924036759597 \tTime\t 65.57781791687012\n","Epoch\t 162 \tLoss\t 4.90571310825837 \tTime\t 64.49388813972473\n","Epoch\t 163 \tLoss\t 4.908631026439178 \tTime\t 64.3357424736023\n","Epoch\t 164 \tLoss\t 4.907815622672056 \tTime\t 64.8463339805603\n","Epoch\t 165 \tLoss\t 4.905785995874649 \tTime\t 64.48577094078064\n","Epoch\t 166 \tLoss\t 4.903410914005377 \tTime\t 64.73892712593079\n","Epoch\t 167 \tLoss\t 4.903271161592924 \tTime\t 65.34999966621399\n","Epoch\t 168 \tLoss\t 4.901961395068047 \tTime\t 64.70180630683899\n","Epoch\t 169 \tLoss\t 4.907839535444211 \tTime\t 65.82238674163818\n","Epoch\t 170 \tLoss\t 4.9010894702031065 \tTime\t 64.83774948120117\n","Epoch\t 171 \tLoss\t 4.905350804940248 \tTime\t 64.61313009262085\n","Epoch\t 172 \tLoss\t 4.904671852405254 \tTime\t 63.84889626502991\n","Epoch\t 173 \tLoss\t 4.902498602255797 \tTime\t 64.1761646270752\n","Epoch\t 174 \tLoss\t 4.90102420709072 \tTime\t 65.083505153656\n","Epoch\t 175 \tLoss\t 4.900147540752704 \tTime\t 64.3279013633728\n","Epoch\t 176 \tLoss\t 4.89563051370474 \tTime\t 64.94603776931763\n","Epoch\t 177 \tLoss\t 4.903504180908203 \tTime\t 64.64304494857788\n","Epoch\t 178 \tLoss\t 4.901494461450821 \tTime\t 65.11961770057678\n","Epoch\t 179 \tLoss\t 4.899286333719889 \tTime\t 64.27951383590698\n","Epoch\t 180 \tLoss\t 4.902116716825045 \tTime\t 63.95602583885193\n","Epoch\t 181 \tLoss\t 4.897902857951629 \tTime\t 64.75595164299011\n","Epoch\t 182 \tLoss\t 4.896694166232378 \tTime\t 64.61307072639465\n","Epoch\t 183 \tLoss\t 4.898650364998059 \tTime\t 64.76560878753662\n","Epoch\t 184 \tLoss\t 4.896582197531676 \tTime\t 64.80469942092896\n","Epoch\t 185 \tLoss\t 4.896722720219539 \tTime\t 64.79718446731567\n","Epoch\t 186 \tLoss\t 4.899708126752804 \tTime\t 65.0166425704956\n","Epoch\t 187 \tLoss\t 4.89588311513265 \tTime\t 64.66093730926514\n","Epoch\t 188 \tLoss\t 4.896758776444655 \tTime\t 64.57982635498047\n","Epoch\t 189 \tLoss\t 4.898488621833997 \tTime\t 64.9678065776825\n","Epoch\t 190 \tLoss\t 4.891971671275604 \tTime\t 65.51775979995728\n","Epoch\t 191 \tLoss\t 4.894104754619109 \tTime\t 64.90429210662842\n","Epoch\t 192 \tLoss\t 4.89188043154203 \tTime\t 64.65565991401672\n","Epoch\t 193 \tLoss\t 4.893746197529328 \tTime\t 64.65731501579285\n","Epoch\t 194 \tLoss\t 4.893155410962227 \tTime\t 66.0824043750763\n","Epoch\t 195 \tLoss\t 4.891004234705216 \tTime\t 65.47100400924683\n","Epoch\t 196 \tLoss\t 4.892727795625344 \tTime\t 64.76583194732666\n","Epoch\t 197 \tLoss\t 4.892323139386299 \tTime\t 64.99769568443298\n","Epoch\t 198 \tLoss\t 4.889343535594451 \tTime\t 64.96552801132202\n","Epoch\t 199 \tLoss\t 4.890986276284242 \tTime\t 65.13431143760681\n","Epoch\t 200 \tLoss\t 4.892476881467379 \tTime\t 65.72683072090149\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Finished training. Train time was: 7772.460624933243\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 1 \tLoss\t 1.7907592993516188 \tTime\t 15.009110927581787\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 2 \tLoss\t 1.288052258736048 \tTime\t 14.825519323348999\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 3 \tLoss\t 1.1286504507064818 \tTime\t 14.785435676574707\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 4 \tLoss\t 1.0567128719427648 \tTime\t 14.836619138717651\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 5 \tLoss\t 1.0156860409638822 \tTime\t 14.813337564468384\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 6 \tLoss\t 0.9882473826408387 \tTime\t 14.74638295173645\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 7 \tLoss\t 0.9695941442098372 \tTime\t 14.780014276504517\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 8 \tLoss\t 0.9554762684381926 \tTime\t 14.792882204055786\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 9 \tLoss\t 0.9465719812955612 \tTime\t 14.812642574310303\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 10 \tLoss\t 0.9359458156121083 \tTime\t 14.815054655075073\n","Finished training. Train time was: 148.23494267463684\n","Accuracy : 66 %\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Epoch\t 1 \tLoss\t 6.229382356007894 \tTime\t 65.75105667114258\n","Epoch\t 2 \tLoss\t 6.08891656826704 \tTime\t 65.45989179611206\n","Epoch\t 3 \tLoss\t 5.926305602147029 \tTime\t 64.58233094215393\n","Epoch\t 4 \tLoss\t 5.840911310147017 \tTime\t 64.97148942947388\n","Epoch\t 5 \tLoss\t 5.796943979996901 \tTime\t 65.74198508262634\n","Epoch\t 6 \tLoss\t 5.773563211392133 \tTime\t 65.12185215950012\n","Epoch\t 7 \tLoss\t 5.746905453999838 \tTime\t 65.83721470832825\n","Epoch\t 8 \tLoss\t 5.726790080926357 \tTime\t 65.1152994632721\n","Epoch\t 9 \tLoss\t 5.710009122506166 \tTime\t 64.94436931610107\n","Epoch\t 10 \tLoss\t 5.689929341047238 \tTime\t 65.00612354278564\n","Epoch\t 11 \tLoss\t 5.67764849051451 \tTime\t 65.78183150291443\n","Epoch\t 12 \tLoss\t 5.670782644320757 \tTime\t 66.126788854599\n","Epoch\t 13 \tLoss\t 5.664561364589593 \tTime\t 66.00912523269653\n","Epoch\t 14 \tLoss\t 5.655679008288261 \tTime\t 66.18795347213745\n","Epoch\t 15 \tLoss\t 5.6501989291264465 \tTime\t 66.44514393806458\n","Epoch\t 16 \tLoss\t 5.644546931829208 \tTime\t 65.58715629577637\n","Epoch\t 17 \tLoss\t 5.636213158338498 \tTime\t 65.26495504379272\n","Epoch\t 18 \tLoss\t 5.633021792387351 \tTime\t 64.90358757972717\n","Epoch\t 19 \tLoss\t 5.627953250591571 \tTime\t 64.57508540153503\n","Epoch\t 20 \tLoss\t 5.62465481146788 \tTime\t 64.29123950004578\n","Epoch\t 21 \tLoss\t 5.617901256756904 \tTime\t 64.66130566596985\n","Epoch\t 22 \tLoss\t 5.612420730101756 \tTime\t 65.5407485961914\n","Epoch\t 23 \tLoss\t 5.609059350918501 \tTime\t 64.6001226902008\n","Epoch\t 24 \tLoss\t 5.603062979380289 \tTime\t 64.94435930252075\n","Epoch\t 25 \tLoss\t 5.600310469896366 \tTime\t 65.70021510124207\n","Epoch\t 26 \tLoss\t 5.599753514314309 \tTime\t 64.7720193862915\n","Epoch\t 27 \tLoss\t 5.5966563078073355 \tTime\t 65.86831450462341\n","Epoch\t 28 \tLoss\t 5.594253554710975 \tTime\t 65.28074407577515\n","Epoch\t 29 \tLoss\t 5.592272186279297 \tTime\t 64.98177576065063\n","Epoch\t 30 \tLoss\t 5.587333446893936 \tTime\t 64.67283773422241\n","Epoch\t 31 \tLoss\t 5.585430372678316 \tTime\t 66.07111239433289\n","Epoch\t 32 \tLoss\t 5.5861229260762535 \tTime\t 63.55574059486389\n","Epoch\t 33 \tLoss\t 5.582632901118352 \tTime\t 64.54637551307678\n","Epoch\t 34 \tLoss\t 5.581302686838003 \tTime\t 65.03894448280334\n","Epoch\t 35 \tLoss\t 5.5805341793940615 \tTime\t 64.48764848709106\n","Epoch\t 36 \tLoss\t 5.576499809362949 \tTime\t 64.8035888671875\n","Epoch\t 37 \tLoss\t 5.577422413459191 \tTime\t 64.46935272216797\n","Epoch\t 38 \tLoss\t 5.575444744794797 \tTime\t 64.79706931114197\n","Epoch\t 39 \tLoss\t 5.573144105764536 \tTime\t 64.0622169971466\n","Epoch\t 40 \tLoss\t 5.573678696461213 \tTime\t 65.26093006134033\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 41 \tLoss\t 5.571172882960393 \tTime\t 63.488837003707886\n","Epoch\t 42 \tLoss\t 5.567106907184307 \tTime\t 64.27795028686523\n","Epoch\t 43 \tLoss\t 5.568351146502373 \tTime\t 63.66506552696228\n","Epoch\t 44 \tLoss\t 5.567169372852032 \tTime\t 64.10929989814758\n","Epoch\t 45 \tLoss\t 5.567692145323142 \tTime\t 64.80677533149719\n","Epoch\t 46 \tLoss\t 5.563019710932022 \tTime\t 65.34468507766724\n","Epoch\t 47 \tLoss\t 5.563633229182317 \tTime\t 65.55295085906982\n","Epoch\t 48 \tLoss\t 5.561463223970853 \tTime\t 64.83043670654297\n","Epoch\t 49 \tLoss\t 5.562928419846755 \tTime\t 64.39490032196045\n","Epoch\t 50 \tLoss\t 5.560428509345422 \tTime\t 65.55076718330383\n","Epoch\t 51 \tLoss\t 5.559072597210224 \tTime\t 64.55500912666321\n","Epoch\t 52 \tLoss\t 5.558295188805996 \tTime\t 65.631427526474\n","Epoch\t 53 \tLoss\t 5.558781474675888 \tTime\t 65.72941422462463\n","Epoch\t 54 \tLoss\t 5.558283121157915 \tTime\t 65.40658926963806\n","Epoch\t 55 \tLoss\t 5.554399045308431 \tTime\t 65.62302422523499\n","Epoch\t 56 \tLoss\t 5.55717377051329 \tTime\t 66.96033811569214\n","Epoch\t 57 \tLoss\t 5.554500655638866 \tTime\t 65.19417405128479\n","Epoch\t 58 \tLoss\t 5.555471926469069 \tTime\t 65.55526685714722\n","Epoch\t 59 \tLoss\t 5.554905360784286 \tTime\t 64.79230904579163\n","Epoch\t 60 \tLoss\t 5.554866008269481 \tTime\t 65.2224223613739\n","Epoch\t 61 \tLoss\t 5.552979907011374 \tTime\t 65.32758593559265\n","Epoch\t 62 \tLoss\t 5.550930526929024 \tTime\t 66.01949429512024\n","Epoch\t 63 \tLoss\t 5.5526791523664425 \tTime\t 66.03944969177246\n","Epoch\t 64 \tLoss\t 5.551713703840207 \tTime\t 65.28901052474976\n","Epoch\t 65 \tLoss\t 5.550681378291204 \tTime\t 65.35206365585327\n","Epoch\t 66 \tLoss\t 5.550089256580059 \tTime\t 65.29752683639526\n","Epoch\t 67 \tLoss\t 5.548112037854317 \tTime\t 64.88793992996216\n","Epoch\t 68 \tLoss\t 5.549931751153408 \tTime\t 65.88626194000244\n","Epoch\t 69 \tLoss\t 5.547962078681359 \tTime\t 65.38387775421143\n","Epoch\t 70 \tLoss\t 5.548359638605362 \tTime\t 64.79305243492126\n","Epoch\t 71 \tLoss\t 5.547994630764692 \tTime\t 65.32611632347107\n","Epoch\t 72 \tLoss\t 5.546567689455473 \tTime\t 64.90207481384277\n","Epoch\t 73 \tLoss\t 5.545366678482447 \tTime\t 65.2524082660675\n","Epoch\t 74 \tLoss\t 5.544346591753837 \tTime\t 66.02001523971558\n","Epoch\t 75 \tLoss\t 5.544861226204114 \tTime\t 65.24029874801636\n","Epoch\t 76 \tLoss\t 5.543509727869278 \tTime\t 65.7108154296875\n","Epoch\t 77 \tLoss\t 5.545013552445631 \tTime\t 65.78897762298584\n","Epoch\t 78 \tLoss\t 5.5453945404443985 \tTime\t 65.36523509025574\n","Epoch\t 79 \tLoss\t 5.542140041253505 \tTime\t 65.29997849464417\n","Epoch\t 80 \tLoss\t 5.543528329409086 \tTime\t 64.84780406951904\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 81 \tLoss\t 5.542604456192408 \tTime\t 64.5816810131073\n","Epoch\t 82 \tLoss\t 5.542073648403853 \tTime\t 64.74671721458435\n","Epoch\t 83 \tLoss\t 5.5412222495445835 \tTime\t 65.48406839370728\n","Epoch\t 84 \tLoss\t 5.54022587507199 \tTime\t 66.44203329086304\n","Epoch\t 85 \tLoss\t 5.539589539552346 \tTime\t 66.41779589653015\n","Epoch\t 86 \tLoss\t 5.539557063273895 \tTime\t 65.88136029243469\n","Epoch\t 87 \tLoss\t 5.539963480142447 \tTime\t 67.0496335029602\n","Epoch\t 88 \tLoss\t 5.5379419522407725 \tTime\t 67.29408264160156\n","Epoch\t 89 \tLoss\t 5.540567373618101 \tTime\t 66.20052003860474\n","Epoch\t 90 \tLoss\t 5.539887254666059 \tTime\t 66.97170543670654\n","Epoch\t 91 \tLoss\t 5.538365373855982 \tTime\t 66.59100723266602\n","Epoch\t 92 \tLoss\t 5.537552324930827 \tTime\t 67.68807554244995\n","Epoch\t 93 \tLoss\t 5.539694798298371 \tTime\t 67.32593941688538\n","Epoch\t 94 \tLoss\t 5.538356431325277 \tTime\t 67.21135759353638\n","Epoch\t 95 \tLoss\t 5.5372346682426254 \tTime\t 67.43929290771484\n","Epoch\t 96 \tLoss\t 5.537290287017822 \tTime\t 68.76127243041992\n","Epoch\t 97 \tLoss\t 5.535898893307417 \tTime\t 67.57084918022156\n","Epoch\t 98 \tLoss\t 5.536594716096536 \tTime\t 68.16965889930725\n","Epoch\t 99 \tLoss\t 5.534963820530818 \tTime\t 67.23251032829285\n","Epoch\t 100 \tLoss\t 5.535503715123886 \tTime\t 68.1523027420044\n","Epoch\t 101 \tLoss\t 5.535551105401455 \tTime\t 67.5755889415741\n","Epoch\t 102 \tLoss\t 5.535303785862067 \tTime\t 68.54553437232971\n","Epoch\t 103 \tLoss\t 5.5338611896221455 \tTime\t 67.44195604324341\n","Epoch\t 104 \tLoss\t 5.533388208731627 \tTime\t 68.47612166404724\n","Epoch\t 105 \tLoss\t 5.5327255518008505 \tTime\t 69.44073748588562\n","Epoch\t 106 \tLoss\t 5.534019269698706 \tTime\t 69.54016089439392\n","Epoch\t 107 \tLoss\t 5.534106357281025 \tTime\t 68.01344752311707\n","Epoch\t 108 \tLoss\t 5.532092187343499 \tTime\t 67.23482322692871\n","Epoch\t 109 \tLoss\t 5.532569195674016 \tTime\t 68.58442974090576\n","Epoch\t 110 \tLoss\t 5.5308472535549065 \tTime\t 67.81345343589783\n","Epoch\t 111 \tLoss\t 5.531789953280718 \tTime\t 67.75742506980896\n","Epoch\t 112 \tLoss\t 5.531979856735621 \tTime\t 67.49130249023438\n","Epoch\t 113 \tLoss\t 5.530394182449732 \tTime\t 68.76861381530762\n","Epoch\t 114 \tLoss\t 5.528767030667036 \tTime\t 67.85098648071289\n","Epoch\t 115 \tLoss\t 5.528600010505089 \tTime\t 67.64343476295471\n","Epoch\t 116 \tLoss\t 5.5309678102150945 \tTime\t 69.26847410202026\n","Epoch\t 117 \tLoss\t 5.528349839724028 \tTime\t 68.39908003807068\n","Epoch\t 118 \tLoss\t 5.529330664414626 \tTime\t 68.53969359397888\n","Epoch\t 119 \tLoss\t 5.529036475450565 \tTime\t 67.8059139251709\n","Epoch\t 120 \tLoss\t 5.527864842537122 \tTime\t 67.65428614616394\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 121 \tLoss\t 5.526379778446295 \tTime\t 67.8732271194458\n","Epoch\t 122 \tLoss\t 5.528842989603678 \tTime\t 67.37957835197449\n","Epoch\t 123 \tLoss\t 5.530192653949444 \tTime\t 68.26138138771057\n","Epoch\t 124 \tLoss\t 5.527472674540984 \tTime\t 67.29825329780579\n","Epoch\t 125 \tLoss\t 5.5260257843213205 \tTime\t 67.14985299110413\n","Epoch\t 126 \tLoss\t 5.526497312692496 \tTime\t 68.56958365440369\n","Epoch\t 127 \tLoss\t 5.526827479631473 \tTime\t 67.68364572525024\n","Epoch\t 128 \tLoss\t 5.527458479465582 \tTime\t 68.2862958908081\n","Epoch\t 129 \tLoss\t 5.526102122282371 \tTime\t 68.65572357177734\n","Epoch\t 130 \tLoss\t 5.526516388624143 \tTime\t 68.63552784919739\n","Epoch\t 131 \tLoss\t 5.525913622440436 \tTime\t 68.67068910598755\n","Epoch\t 132 \tLoss\t 5.523800265483367 \tTime\t 68.68639516830444\n","Epoch\t 133 \tLoss\t 5.525930451124142 \tTime\t 69.25251483917236\n","Epoch\t 134 \tLoss\t 5.526049303397154 \tTime\t 69.08769607543945\n","Epoch\t 135 \tLoss\t 5.525074046697372 \tTime\t 69.02241849899292\n","Epoch\t 136 \tLoss\t 5.523011603722206 \tTime\t 68.45305800437927\n","Epoch\t 137 \tLoss\t 5.5237470724643805 \tTime\t 67.54024600982666\n","Epoch\t 138 \tLoss\t 5.523291734548716 \tTime\t 67.54387140274048\n","Epoch\t 139 \tLoss\t 5.524679682804988 \tTime\t 68.6357855796814\n","Epoch\t 140 \tLoss\t 5.52254354770367 \tTime\t 68.20535778999329\n","Epoch\t 141 \tLoss\t 5.5207382495586685 \tTime\t 68.2759063243866\n","Epoch\t 142 \tLoss\t 5.523238443716979 \tTime\t 67.4058768749237\n","Epoch\t 143 \tLoss\t 5.522637648460193 \tTime\t 68.67463684082031\n","Epoch\t 144 \tLoss\t 5.524602396060259 \tTime\t 67.68104124069214\n","Epoch\t 145 \tLoss\t 5.522771375607221 \tTime\t 68.2035300731659\n","Epoch\t 146 \tLoss\t 5.5208567741589665 \tTime\t 68.23228645324707\n","Epoch\t 147 \tLoss\t 5.521876763074826 \tTime\t 68.62537670135498\n","Epoch\t 148 \tLoss\t 5.521169755397699 \tTime\t 69.2056474685669\n","Epoch\t 149 \tLoss\t 5.520934774936774 \tTime\t 67.88067150115967\n","Epoch\t 150 \tLoss\t 5.521726062970283 \tTime\t 69.46508288383484\n","Epoch\t 151 \tLoss\t 5.519955363640419 \tTime\t 69.7606451511383\n","Epoch\t 152 \tLoss\t 5.520647501334166 \tTime\t 68.77900791168213\n","Epoch\t 153 \tLoss\t 5.519824135609162 \tTime\t 68.74274730682373\n","Epoch\t 154 \tLoss\t 5.520611772781764 \tTime\t 67.7244975566864\n","Epoch\t 155 \tLoss\t 5.5205979273869445 \tTime\t 68.07296872138977\n","Epoch\t 156 \tLoss\t 5.5186958166269156 \tTime\t 68.1439380645752\n","Epoch\t 157 \tLoss\t 5.52066273811536 \tTime\t 68.24010419845581\n","Epoch\t 158 \tLoss\t 5.520256421504876 \tTime\t 67.77769041061401\n","Epoch\t 159 \tLoss\t 5.5209616514352655 \tTime\t 65.76410055160522\n","Epoch\t 160 \tLoss\t 5.51912273749327 \tTime\t 65.67157816886902\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 161 \tLoss\t 5.518913992857322 \tTime\t 66.48653388023376\n","Epoch\t 162 \tLoss\t 5.51977292574369 \tTime\t 64.99918746948242\n","Epoch\t 163 \tLoss\t 5.5186630322383 \tTime\t 66.0629198551178\n","Epoch\t 164 \tLoss\t 5.518592624175243 \tTime\t 65.89983797073364\n","Epoch\t 165 \tLoss\t 5.520718342218643 \tTime\t 66.36880469322205\n","Epoch\t 166 \tLoss\t 5.519557798825778 \tTime\t 66.119961977005\n","Epoch\t 167 \tLoss\t 5.519449016375419 \tTime\t 65.92958283424377\n","Epoch\t 168 \tLoss\t 5.517570723020113 \tTime\t 66.15818524360657\n","Epoch\t 169 \tLoss\t 5.518370779966697 \tTime\t 65.50576901435852\n","Epoch\t 170 \tLoss\t 5.517327154599704 \tTime\t 65.13793301582336\n","Epoch\t 171 \tLoss\t 5.517097294636262 \tTime\t 66.60979890823364\n","Epoch\t 172 \tLoss\t 5.51835909867898 \tTime\t 66.2783215045929\n","Epoch\t 173 \tLoss\t 5.516634153708433 \tTime\t 65.22485303878784\n","Epoch\t 174 \tLoss\t 5.519268314655011 \tTime\t 66.66195774078369\n","Epoch\t 175 \tLoss\t 5.516619298396966 \tTime\t 65.34131789207458\n","Epoch\t 176 \tLoss\t 5.517708389575665 \tTime\t 65.03399229049683\n","Epoch\t 177 \tLoss\t 5.517804008875138 \tTime\t 64.48675036430359\n","Epoch\t 178 \tLoss\t 5.518825438083747 \tTime\t 66.52624368667603\n","Epoch\t 179 \tLoss\t 5.516365584349021 \tTime\t 64.39074897766113\n","Epoch\t 180 \tLoss\t 5.517749162820669 \tTime\t 65.36028742790222\n","Epoch\t 181 \tLoss\t 5.518193812248034 \tTime\t 65.84810495376587\n","Epoch\t 182 \tLoss\t 5.514983749389648 \tTime\t 65.32039785385132\n","Epoch\t 183 \tLoss\t 5.517736417819292 \tTime\t 66.11796021461487\n","Epoch\t 184 \tLoss\t 5.516872530717116 \tTime\t 65.81745481491089\n","Epoch\t 185 \tLoss\t 5.516469258528489 \tTime\t 65.68725800514221\n","Epoch\t 186 \tLoss\t 5.515653079595321 \tTime\t 66.48325681686401\n","Epoch\t 187 \tLoss\t 5.516719140761938 \tTime\t 66.10263347625732\n","Epoch\t 188 \tLoss\t 5.515153361589481 \tTime\t 64.73058247566223\n","Epoch\t 189 \tLoss\t 5.515676958133013 \tTime\t 64.95660853385925\n","Epoch\t 190 \tLoss\t 5.514665043659699 \tTime\t 65.48035025596619\n","Epoch\t 191 \tLoss\t 5.516008399083064 \tTime\t 64.91579031944275\n","Epoch\t 192 \tLoss\t 5.5173301623417785 \tTime\t 65.28444051742554\n","Epoch\t 193 \tLoss\t 5.5163925537696255 \tTime\t 65.2442536354065\n","Epoch\t 194 \tLoss\t 5.515456356146397 \tTime\t 65.70566725730896\n","Epoch\t 195 \tLoss\t 5.516230055002066 \tTime\t 65.41097784042358\n","Epoch\t 196 \tLoss\t 5.515158572563759 \tTime\t 65.1976797580719\n","Epoch\t 197 \tLoss\t 5.515477745349591 \tTime\t 65.15567898750305\n","Epoch\t 198 \tLoss\t 5.517150380061223 \tTime\t 65.04393815994263\n","Epoch\t 199 \tLoss\t 5.516463751670642 \tTime\t 65.0337905883789\n","Epoch\t 200 \tLoss\t 5.517735082675249 \tTime\t 64.07865881919861\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Finished training. Train time was: 13265.319501876831\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 1 \tLoss\t 1.8131278362029637 \tTime\t 14.790858268737793\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 2 \tLoss\t 1.343527321326427 \tTime\t 14.7749183177948\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 3 \tLoss\t 1.1939433213991997 \tTime\t 14.796476364135742\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 4 \tLoss\t 1.1246785298371926 \tTime\t 14.777867555618286\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 5 \tLoss\t 1.085494656746204 \tTime\t 14.773404121398926\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 6 \tLoss\t 1.059125986466041 \tTime\t 14.747579336166382\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 7 \tLoss\t 1.0393433252970377 \tTime\t 14.783440589904785\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 8 \tLoss\t 1.02458144059548 \tTime\t 14.766719579696655\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 9 \tLoss\t 1.0142295161883037 \tTime\t 14.761525630950928\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 10 \tLoss\t 1.004074622117556 \tTime\t 14.764786005020142\n","Finished training. Train time was: 147.75176334381104\n","Accuracy : 64 %\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Epoch\t 1 \tLoss\t 6.522559811518742 \tTime\t 64.98084592819214\n","Epoch\t 2 \tLoss\t 5.988173983647273 \tTime\t 65.56842350959778\n","Epoch\t 3 \tLoss\t 5.616054400419578 \tTime\t 64.85836696624756\n","Epoch\t 4 \tLoss\t 5.102292168446076 \tTime\t 64.6776225566864\n","Epoch\t 5 \tLoss\t 4.591304512512989 \tTime\t 64.08556771278381\n","Epoch\t 6 \tLoss\t 4.1650579733726305 \tTime\t 65.02865171432495\n","Epoch\t 7 \tLoss\t 3.865292992958656 \tTime\t 66.31809091567993\n","Epoch\t 8 \tLoss\t 3.692963126989511 \tTime\t 64.8036036491394\n","Epoch\t 9 \tLoss\t 3.5075562049181035 \tTime\t 65.4873538017273\n","Epoch\t 10 \tLoss\t 3.371617654653696 \tTime\t 65.48368668556213\n","Epoch\t 11 \tLoss\t 3.315061461619842 \tTime\t 65.78106236457825\n","Epoch\t 12 \tLoss\t 3.195803397741073 \tTime\t 65.02875566482544\n","Epoch\t 13 \tLoss\t 3.1078892316573707 \tTime\t 64.41079497337341\n","Epoch\t 14 \tLoss\t 3.0145465349539733 \tTime\t 63.950286865234375\n","Epoch\t 15 \tLoss\t 2.9062931672120706 \tTime\t 65.21765398979187\n","Epoch\t 16 \tLoss\t 2.8633301857190254 \tTime\t 66.73289108276367\n","Epoch\t 17 \tLoss\t 2.794875151071793 \tTime\t 64.76117372512817\n","Epoch\t 18 \tLoss\t 2.7523354823772723 \tTime\t 65.32536005973816\n","Epoch\t 19 \tLoss\t 2.69735568853525 \tTime\t 65.31637978553772\n","Epoch\t 20 \tLoss\t 2.6789900828630495 \tTime\t 65.89147615432739\n","Epoch\t 21 \tLoss\t 2.6109188556671143 \tTime\t 66.23671913146973\n","Epoch\t 22 \tLoss\t 2.5871319526281114 \tTime\t 65.52947950363159\n","Epoch\t 23 \tLoss\t 2.585142498749953 \tTime\t 65.57569789886475\n","Epoch\t 24 \tLoss\t 2.5160999628213734 \tTime\t 65.90055680274963\n","Epoch\t 25 \tLoss\t 2.518453665268727 \tTime\t 64.86523604393005\n","Epoch\t 26 \tLoss\t 2.4866180065350654 \tTime\t 65.54860615730286\n","Epoch\t 27 \tLoss\t 2.496942126445281 \tTime\t 65.17735433578491\n","Epoch\t 28 \tLoss\t 2.4740410217872033 \tTime\t 65.77751469612122\n","Epoch\t 29 \tLoss\t 2.443014728105985 \tTime\t 66.76203417778015\n","Epoch\t 30 \tLoss\t 2.4150022555620243 \tTime\t 66.07265281677246\n","Epoch\t 31 \tLoss\t 2.393934696759933 \tTime\t 65.39108538627625\n","Epoch\t 32 \tLoss\t 2.384594710667928 \tTime\t 64.93050146102905\n","Epoch\t 33 \tLoss\t 2.338610684566009 \tTime\t 66.15500903129578\n","Epoch\t 34 \tLoss\t 2.2998465311832916 \tTime\t 65.15097260475159\n","Epoch\t 35 \tLoss\t 2.299884255727132 \tTime\t 65.65560150146484\n","Epoch\t 36 \tLoss\t 2.3062425821255417 \tTime\t 64.83749389648438\n","Epoch\t 37 \tLoss\t 2.276347591326787 \tTime\t 65.17922163009644\n","Epoch\t 38 \tLoss\t 2.275792650687389 \tTime\t 64.88449430465698\n","Epoch\t 39 \tLoss\t 2.2885296020752346 \tTime\t 64.71725058555603\n","Epoch\t 40 \tLoss\t 2.2569108730707415 \tTime\t 64.71212673187256\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 41 \tLoss\t 2.2246374166928806 \tTime\t 64.85854649543762\n","Epoch\t 42 \tLoss\t 2.2171203766113674 \tTime\t 64.05815029144287\n","Epoch\t 43 \tLoss\t 2.220430627847329 \tTime\t 65.77921772003174\n","Epoch\t 44 \tLoss\t 2.1762359576347547 \tTime\t 64.8687207698822\n","Epoch\t 45 \tLoss\t 2.2032648771237104 \tTime\t 64.44294881820679\n","Epoch\t 46 \tLoss\t 2.1775225400924683 \tTime\t 65.03440427780151\n","Epoch\t 47 \tLoss\t 2.1392034224974803 \tTime\t 64.38195633888245\n","Epoch\t 48 \tLoss\t 2.1561903329995964 \tTime\t 64.36750388145447\n","Epoch\t 49 \tLoss\t 2.1159658334194087 \tTime\t 65.65668368339539\n","Epoch\t 50 \tLoss\t 2.112518085577549 \tTime\t 64.75199055671692\n","Epoch\t 51 \tLoss\t 2.1076022429343984 \tTime\t 65.03110098838806\n","Epoch\t 52 \tLoss\t 2.087676638823289 \tTime\t 65.48379921913147\n","Epoch\t 53 \tLoss\t 2.097656844212459 \tTime\t 66.66227054595947\n","Epoch\t 54 \tLoss\t 2.053205068906148 \tTime\t 64.42164301872253\n","Epoch\t 55 \tLoss\t 2.062930880448757 \tTime\t 64.17590403556824\n","Epoch\t 56 \tLoss\t 2.066192285831158 \tTime\t 63.75486087799072\n","Epoch\t 57 \tLoss\t 2.031685759471013 \tTime\t 64.51456189155579\n","Epoch\t 58 \tLoss\t 2.05029350060683 \tTime\t 64.45446848869324\n","Epoch\t 59 \tLoss\t 2.0334040654011263 \tTime\t 64.7725555896759\n","Epoch\t 60 \tLoss\t 2.04015691280365 \tTime\t 64.61093950271606\n","Epoch\t 61 \tLoss\t 2.0115911551010917 \tTime\t 65.02709436416626\n","Epoch\t 62 \tLoss\t 2.0008394754849945 \tTime\t 63.872779846191406\n","Epoch\t 63 \tLoss\t 2.010058483099326 \tTime\t 65.02082562446594\n","Epoch\t 64 \tLoss\t 2.00042708837069 \tTime\t 64.52379417419434\n","Epoch\t 65 \tLoss\t 1.978810944312658 \tTime\t 65.12378883361816\n","Epoch\t 66 \tLoss\t 1.9903561268097316 \tTime\t 65.27301263809204\n","Epoch\t 67 \tLoss\t 1.9641981234917274 \tTime\t 65.09408736228943\n","Epoch\t 68 \tLoss\t 1.9645127663245567 \tTime\t 65.93925523757935\n","Epoch\t 69 \tLoss\t 1.9309715008124326 \tTime\t 65.44315195083618\n","Epoch\t 70 \tLoss\t 1.9402906387280194 \tTime\t 65.38035678863525\n","Epoch\t 71 \tLoss\t 1.9606597985976781 \tTime\t 66.01520657539368\n","Epoch\t 72 \tLoss\t 1.9392200873448298 \tTime\t 65.36818408966064\n","Epoch\t 73 \tLoss\t 1.9354902291909242 \tTime\t 65.09999060630798\n","Epoch\t 74 \tLoss\t 1.9187181362738976 \tTime\t 65.22643446922302\n","Epoch\t 75 \tLoss\t 1.922125117595379 \tTime\t 65.09421730041504\n","Epoch\t 76 \tLoss\t 1.896995987647619 \tTime\t 65.80369997024536\n","Epoch\t 77 \tLoss\t 1.9027699403273755 \tTime\t 65.28970265388489\n","Epoch\t 78 \tLoss\t 1.908889434276483 \tTime\t 64.68080687522888\n","Epoch\t 79 \tLoss\t 1.9059879180712578 \tTime\t 65.48102378845215\n","Epoch\t 80 \tLoss\t 1.8842295127037243 \tTime\t 64.6951322555542\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 81 \tLoss\t 1.8879783012928106 \tTime\t 64.79149007797241\n","Epoch\t 82 \tLoss\t 1.8848580641624255 \tTime\t 64.63901257514954\n","Epoch\t 83 \tLoss\t 1.8904370870345677 \tTime\t 65.06830048561096\n","Epoch\t 84 \tLoss\t 1.8863601458378327 \tTime\t 65.02285957336426\n","Epoch\t 85 \tLoss\t 1.8734894789182224 \tTime\t 64.5535089969635\n","Epoch\t 86 \tLoss\t 1.8836725436724149 \tTime\t 66.24891686439514\n","Epoch\t 87 \tLoss\t 1.879221299978403 \tTime\t 66.15975737571716\n","Epoch\t 88 \tLoss\t 1.8649549056322148 \tTime\t 65.12567663192749\n","Epoch\t 89 \tLoss\t 1.852806325447865 \tTime\t 65.54884171485901\n","Epoch\t 90 \tLoss\t 1.8670904202338976 \tTime\t 65.72351217269897\n","Epoch\t 91 \tLoss\t 1.83527297606835 \tTime\t 65.14170932769775\n","Epoch\t 92 \tLoss\t 1.858147102747208 \tTime\t 65.23181247711182\n","Epoch\t 93 \tLoss\t 1.8531041713861318 \tTime\t 65.83819842338562\n","Epoch\t 94 \tLoss\t 1.8483556019954193 \tTime\t 65.60428309440613\n","Epoch\t 95 \tLoss\t 1.8544089225622324 \tTime\t 65.1272280216217\n","Epoch\t 96 \tLoss\t 1.8701495091120401 \tTime\t 65.2548816204071\n","Epoch\t 97 \tLoss\t 1.8336136512267285 \tTime\t 65.14418482780457\n","Epoch\t 98 \tLoss\t 1.8477933859213804 \tTime\t 66.03325581550598\n","Epoch\t 99 \tLoss\t 1.848686221929697 \tTime\t 64.46770405769348\n","Epoch\t 100 \tLoss\t 1.8434484066107335 \tTime\t 64.60945558547974\n","Finished training. Train time was: 6517.775332689285\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 1 \tLoss\t 2.0157518992057213 \tTime\t 14.74507999420166\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 2 \tLoss\t 1.6410105662468153 \tTime\t 14.772881984710693\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 3 \tLoss\t 1.4459103333644379 \tTime\t 14.779217720031738\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 4 \tLoss\t 1.3343830726085566 \tTime\t 14.786324262619019\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 5 \tLoss\t 1.2617874182187594 \tTime\t 14.745169401168823\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 6 \tLoss\t 1.2118657625638523 \tTime\t 14.7665114402771\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 7 \tLoss\t 1.1782082624924488 \tTime\t 14.821084976196289\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 8 \tLoss\t 1.1512615968019535 \tTime\t 14.750359296798706\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 9 \tLoss\t 1.1308831398303691 \tTime\t 14.73403000831604\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Epoch\t 10 \tLoss\t 1.113923988586817 \tTime\t 14.70854115486145\n","Finished training. Train time was: 147.62520170211792\n","Accuracy : 62 %\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]}]}