{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "Emb_SimCLR_Practice.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIbUUOphvNRR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install git+https://github.com/ildoonet/pytorch-gradual-warmup-lr.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYtKgry8uqb3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZ4WM8_Guqb6",
        "colab_type": "text"
      },
      "source": [
        "### Step 1. Construct a CNN model\n",
        "\n",
        "#### Implementation 1-1. Design SimCLRHead class\n",
        "\n",
        "#### Implementation 1-2. Design SimCLRNet class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvfdvwkQuqb6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "from torchvision.models.resnet import conv3x3\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, inplanes, planes, norm_layer, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "        \n",
        "        self.bn1 = norm_layer(inplanes)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        \n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x \n",
        "        residual = self.bn1(residual)\n",
        "        residual = self.relu1(residual)\n",
        "        residual = self.conv1(residual)\n",
        "\n",
        "        residual = self.bn2(residual)\n",
        "        residual = self.relu2(residual)\n",
        "        residual = self.conv2(residual)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            x = self.downsample(x)\n",
        "        return x + residual\n",
        "\n",
        "class Downsample(nn.Module):\n",
        "    def __init__(self, nIn, nOut, stride):\n",
        "        super(Downsample, self).__init__()\n",
        "        self.avg = nn.AvgPool2d(stride)\n",
        "        assert nOut % nIn == 0\n",
        "        self.expand_ratio = nOut // nIn\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.avg(x)\n",
        "        return torch.cat([x] + [x.mul(0)] * (self.expand_ratio - 1), 1)\n",
        "\n",
        "class ResNetCifar(nn.Module):\n",
        "    def __init__(self, depth, width=1, classes=10, channels=3, norm_layer=nn.BatchNorm2d):\n",
        "        assert (depth - 2) % 6 == 0         # depth is 6N+2\n",
        "        self.N = (depth - 2) // 6\n",
        "        super(ResNetCifar, self).__init__()\n",
        "\n",
        "        # Following the Wide ResNet convention, we fix the very first convolution\n",
        "        self.conv1 = nn.Conv2d(channels, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.inplanes = 16\n",
        "        self.layer1 = self._make_layer(norm_layer, 16 * width)\n",
        "        self.layer2 = self._make_layer(norm_layer, 32 * width, stride=2)\n",
        "        self.layer3 = self._make_layer(norm_layer, 64 * width, stride=2)\n",
        "        self.bn = norm_layer(64 * width)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.avgpool = nn.AvgPool2d(8)\n",
        "\n",
        "        # Initialization\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "                \n",
        "    def _make_layer(self, norm_layer, planes, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes:\n",
        "            downsample = Downsample(self.inplanes, planes, stride)\n",
        "        layers = [BasicBlock(self.inplanes, planes, norm_layer, stride, downsample)]\n",
        "        self.inplanes = planes\n",
        "        for i in range(self.N - 1):\n",
        "            layers.append(BasicBlock(self.inplanes, planes, norm_layer))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x\n",
        "\n",
        "    \n",
        "class Normalize(nn.Module):\n",
        "\n",
        "    def __init__(self, power=2):\n",
        "        super(Normalize, self).__init__()\n",
        "        self.power = power\n",
        "\n",
        "    def forward(self, x):\n",
        "        norm = x.pow(self.power).sum(1, keepdim=True).pow(1. / self.power)\n",
        "        out = x.div(norm)\n",
        "        return out\n",
        "    \n",
        "\n",
        "class SimCLRHead(nn.Module):\n",
        "    def __init__(self, width, emb_dim):\n",
        "        super(SimCLRHead, self).__init__()\n",
        "        \n",
        "        ### IMPLEMENTATION 1-1 ###\n",
        "        ### 1. Linear layer (64 * width -> 64 * width)\n",
        "        ### 2. ReLU\n",
        "        ### 3. Linear layer (64 * width -> emb_dim)\n",
        "        ### 4. Normalization layer (Normalize module above)\n",
        "        self.fc1 = \n",
        "        self.relu = \n",
        "        self.fc2 = \n",
        "        self.norm = \n",
        "        ### IMPLEMENTATION ENDS HERE ###\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        ### IMPLEMENTATION 1-1 ###\n",
        "        ### Design a proper forward function\n",
        "        \n",
        "        ### IMPLEMENTATION ENDS HERE ###\n",
        "        return x\n",
        "    \n",
        "\n",
        "class SimCLRNet(nn.Module):\n",
        "    def __init__(self, depth, width=1, num_classes=10, emb_dim=32):\n",
        "        super(SimCLRNet, self).__init__()\n",
        "        \n",
        "        self.num_classes = num_classes\n",
        "        \n",
        "        self.feat = ResNetCifar(depth=depth, width=width, classes=num_classes)\n",
        "        \n",
        "        ### IMPLEMENTATION 1-2 ###\n",
        "        ### 1. A projection head (SimCLRHead module above)\n",
        "        self.head = \n",
        "        \n",
        "        ### 2. A linear classifier (64 * width -> num_classes)\n",
        "        self.classifier = \n",
        "        \n",
        "        ### 3. Normalization layer for conv feature normalization (Normalize module above)\n",
        "        self.norm = \n",
        "        \n",
        "        ### IMPLEMENTATION ENDS HERE ###\n",
        "    \n",
        "    def forward(self, x, norm_feat=False):\n",
        "        \n",
        "        ### IMPLEMENTATION 1-2 ###\n",
        "        ### Your module must return\n",
        "        ### 1. Conv feature (feat) - when norm_feat is true, apply L2 normalization\n",
        "        ### 2. Projected embedding (emb)\n",
        "        ### 3. Logit vector by the linear classifier (logit)\n",
        "        \n",
        "        ### IMPLEMENTATION ENDS HERE ###\n",
        "        return feat, emb, logit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvxUSFm-uqb9",
        "colab_type": "text"
      },
      "source": [
        "### Step 2. Prepare datasets & data augmentations\n",
        "\n",
        "For contrastaive learning, a set of random augmentation functions is first defined.\n",
        "\n",
        "Then, the set is applied twice to each image, which is implemented as provided DuplicatedCompose module\n",
        "\n",
        "#### Implementation 2-1. Design a train transform (train_transform)\n",
        "\n",
        "Follow the instruction inside the train_transform\n",
        "\n",
        "https://pytorch.org/docs/stable/torchvision/transforms.html\n",
        "\n",
        "Refer to the torchvision.transforms documentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S53vsB4Iuqb9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DuplicatedCompose(object):\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, img):\n",
        "        img1 = img.copy()\n",
        "        img2 = img.copy()\n",
        "        for t in self.transforms:\n",
        "            img1 = t(img1)\n",
        "            img2 = t(img2)\n",
        "        return img1, img2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ex83YAnMuqcA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "cv2.setNumThreads(0)\n",
        "\n",
        "class GaussianBlur(object):\n",
        "    # Implements Gaussian blur as described in the SimCLR paper\n",
        "    def __init__(self, kernel_size, min=0.1, max=2.0):\n",
        "        self.min = min\n",
        "        self.max = max\n",
        "        # kernel size is set to be 10% of the image height/width\n",
        "        self.kernel_size = kernel_size\n",
        "        \n",
        "        if self.kernel_size % 2 == 0:\n",
        "            self.kernel_size += 1\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        sample = np.array(sample)\n",
        "\n",
        "        # blur the image with a 50% chance\n",
        "        prob = np.random.random_sample()\n",
        "\n",
        "        if prob < 0.5:\n",
        "            sigma = (self.max - self.min) * np.random.random_sample() + self.min\n",
        "            sample = cv2.GaussianBlur(sample, (self.kernel_size, self.kernel_size), sigma)\n",
        "\n",
        "        return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xHmdyuguqcC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision.transforms as transforms\n",
        "\n",
        "img_size = (32, 32)\n",
        "\n",
        "color_jitter = transforms.ColorJitter(0.8, 0.8, 0.8, 0.2)\n",
        "\n",
        "train_transform = DuplicatedCompose([\n",
        "    ### IMPLEMENTATION 2-1 ###\n",
        "    ### 1. Random resized crop w/ final size of (32, 32)\n",
        "    ### 2. Random horizontal flip w/ p=0.5\n",
        "    ### 3. Randomly apply the pre-defined color jittering w/ p=0.8\n",
        "    ### 4. Random gray scale w/ p=0.2\n",
        "    ### 5. Gaussian blur w/ kernel size of 1/10 of the image width or height (32)\n",
        "    \n",
        "    ### IMPLEMENTATION ENDS HERE ###\n",
        "    transforms.ToTensor(),\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zo9Xj3KruqcE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='.',\n",
        "                                 train=True,\n",
        "                                 download=True,\n",
        "                                 transform=train_transform\n",
        "                                )\n",
        "\n",
        "train_loader = DataLoader(train_dataset,\n",
        "                          batch_size=256,\n",
        "                          num_workers=4,\n",
        "                          shuffle=True,\n",
        "                          drop_last=True\n",
        "                         )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xwkINzBuqcH",
        "colab_type": "text"
      },
      "source": [
        "### Step 3. Implement InfoNCE loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUjxzqf6uqcI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NTXentLoss(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, batch_size, temperature, use_cosine_similarity):\n",
        "        super(NTXentLoss, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.temperature = temperature\n",
        "        self.softmax = torch.nn.Softmax(dim=-1)\n",
        "        self.mask_samples_from_same_repr = self._get_correlated_mask().type(torch.bool)\n",
        "        self.similarity_function = self._get_similarity_function(use_cosine_similarity)\n",
        "        self.criterion = torch.nn.CrossEntropyLoss(reduction=\"sum\")\n",
        "\n",
        "    def _get_similarity_function(self, use_cosine_similarity):\n",
        "        if use_cosine_similarity:\n",
        "            self._cosine_similarity = torch.nn.CosineSimilarity(dim=-1)\n",
        "            return self._cosine_simililarity\n",
        "        else:\n",
        "            return self._dot_simililarity\n",
        "\n",
        "    def _get_correlated_mask(self):\n",
        "        diag = np.eye(2 * self.batch_size)\n",
        "        l1 = np.eye((2 * self.batch_size), 2 * self.batch_size, k=-self.batch_size)\n",
        "        l2 = np.eye((2 * self.batch_size), 2 * self.batch_size, k=self.batch_size)\n",
        "        mask = torch.from_numpy((diag + l1 + l2))\n",
        "        mask = (1 - mask).type(torch.bool)\n",
        "        return mask.cuda()\n",
        "\n",
        "    @staticmethod\n",
        "    def _dot_simililarity(x, y):\n",
        "        v = torch.tensordot(x.unsqueeze(1), y.T.unsqueeze(0), dims=2)\n",
        "        # x shape: (N, 1, C)\n",
        "        # y shape: (1, C, 2N)\n",
        "        # v shape: (N, 2N)\n",
        "        return v\n",
        "\n",
        "    def _cosine_simililarity(self, x, y):\n",
        "        # x shape: (N, 1, C)\n",
        "        # y shape: (1, 2N, C)\n",
        "        # v shape: (N, 2N)\n",
        "        v = self._cosine_similarity(x.unsqueeze(1), y.unsqueeze(0))\n",
        "        return v\n",
        "\n",
        "    def forward(self, zis, zjs):\n",
        "        representations = torch.cat([zjs, zis], dim=0)\n",
        "\n",
        "        similarity_matrix = self.similarity_function(representations, representations)\n",
        "\n",
        "        # filter out the scores from the positive samples\n",
        "        l_pos = torch.diag(similarity_matrix, self.batch_size)\n",
        "        r_pos = torch.diag(similarity_matrix, -self.batch_size)\n",
        "        positives = torch.cat([l_pos, r_pos]).view(2 * self.batch_size, 1)\n",
        "\n",
        "        negatives = similarity_matrix[self.mask_samples_from_same_repr].view(2 * self.batch_size, -1)\n",
        "\n",
        "        logits = torch.cat((positives, negatives), dim=1)\n",
        "        logits = logits / self.temperature\n",
        "\n",
        "        labels = torch.zeros(2 * self.batch_size).cuda().long()\n",
        "        loss = self.criterion(logits, labels)\n",
        "\n",
        "        return loss / (2 * self.batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-SGunpIuqcK",
        "colab_type": "text"
      },
      "source": [
        "### Step 4. Run pre-training step\n",
        "\n",
        "#### Implementation 4-1. Complete a basic SimCLR training loop\n",
        "\n",
        "https://github.com/ildoonet/pytorch-gradual-warmup-lr\n",
        "\n",
        "The linear warmup scheduler implementation is from a github in the link above\n",
        "\n",
        "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
        "\n",
        "Refer to this documentation to use lr schedulers integrated in PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlL2Yl5AuqcL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.optim.optimizer import Optimizer, required\n",
        "\n",
        "class SGD_with_lars(Optimizer):\n",
        "    r\"\"\"Implements stochastic gradient descent (optionally with momentum).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, lr=required, momentum=0, weight_decay=0, trust_coef=1.): # need to add trust coef\n",
        "        if lr is not required and lr < 0.0:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if momentum < 0.0:\n",
        "            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n",
        "        if weight_decay < 0.0:\n",
        "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
        "        if trust_coef < 0.0:\n",
        "            raise ValueError(\"Invalid trust_coef value: {}\".format(trust_coef))\n",
        "\n",
        "        defaults = dict(lr=lr, momentum=momentum, weight_decay=weight_decay, trust_coef=trust_coef)\n",
        "\n",
        "        super(SGD_with_lars, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(SGD_with_lars, self).__setstate__(state)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "        Arguments:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            weight_decay = group['weight_decay']\n",
        "            momentum = group['momentum']\n",
        "            trust_coef = group['trust_coef']\n",
        "            global_lr = group['lr']\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                d_p = p.grad.data\n",
        "\n",
        "                p_norm = torch.norm(p.data, p=2)\n",
        "                d_p_norm = torch.norm(d_p, p=2).add_(momentum, p_norm)\n",
        "                lr = torch.div(p_norm, d_p_norm).mul_(trust_coef)\n",
        "\n",
        "                lr.mul_(global_lr)\n",
        "\n",
        "                if weight_decay != 0:\n",
        "                    d_p.add_(weight_decay, p.data)\n",
        "\n",
        "                d_p.mul_(lr)\n",
        "\n",
        "                if momentum != 0:\n",
        "                    param_state = self.state[p]\n",
        "                    if 'momentum_buffer' not in param_state:\n",
        "                        buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
        "                    else:\n",
        "                        buf = param_state['momentum_buffer']\n",
        "                        buf.mul_(momentum).add_(d_p)\n",
        "                    d_p = buf\n",
        "\n",
        "                p.data.add_(-1, d_p)\n",
        "\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fjc3wkcuqcN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(net, loader):\n",
        "    \n",
        "    loss_fn = NTXentLoss(batch_size=256, temperature=0.07, use_cosine_similarity=True)\n",
        "    \n",
        "    ### IMPLEMENTATION 4-2 ###\n",
        "    ### 1. Use SGD_with_lars with\n",
        "    ### lr = 0.1 * batch_size / 256\n",
        "    ### momentum = 0.9\n",
        "    ### weight_decay = 1e-6\n",
        "    optimizer = \n",
        "    \n",
        "    from warmup_scheduler import GradualWarmupScheduler\n",
        "    ### 2. Use GradualWarmupScheduler with\n",
        "    ### multiplier = 1\n",
        "    ### total_epoch = 1/10 of total epochs\n",
        "    ### after_scheduler = optim.lr_scheduler.CosineAnnealingLR\n",
        "    scheduler = \n",
        "    \n",
        "    train_start = time.time()\n",
        "    \n",
        "    for epoch in range(1, 200 + 1):\n",
        "        \n",
        "        train_loss = 0\n",
        "        net.train()\n",
        "        \n",
        "        epoch_start = time.time()\n",
        "        for idx, (data, target) in enumerate(loader):\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            ### 3. data variable contains two augmented images\n",
        "            ### -1. send them to your GPU by calling .cuda()\n",
        "            ### -2. forward each of them to net\n",
        "            ### -3. compute the InfoNCE loss\n",
        "            \n",
        "            ### IMPLEMENTATION ENDS HERE ###\n",
        "            \n",
        "            train_loss += loss.item()\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "        train_loss /= (idx + 1)\n",
        "        scheduler.step()\n",
        "        \n",
        "        epoch_time = time.time() - epoch_start\n",
        "        print(\"Epoch\\t\", epoch, \n",
        "              \"\\tLoss\\t\", train_loss, \n",
        "              \"\\tTime\\t\", epoch_time,\n",
        "             )\n",
        "        \n",
        "    elapsed_train_time = time.time() - train_start\n",
        "    print('Finished training. Train time was:', elapsed_train_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2Bqj--ZuqcR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GPU_NUM = '0'\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = GPU_NUM\n",
        "\n",
        "net = SimCLRNet(26, 1, 10, 32)\n",
        "\n",
        "net.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43yf5jCluqcT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train(net, train_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CPurz9SuqcV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}